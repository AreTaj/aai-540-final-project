{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69584688-1464-4b46-be61-06a594249ce9",
   "metadata": {},
   "source": [
    "# MLOps Pipeline Workflow & Team Notes\n",
    "\n",
    "## Project Overview\n",
    "This notebook implements an end-to-end MLOps data pipeline using the **Olist Brazilian E-Commerce dataset**. The goal was to demonstrate a production-style workflow that covers data ingestion, cataloging, exploratory analysis, feature engineering, feature storage, and dataset splitting \u2014 all using AWS services in a cost-efficient way.\n",
    "\n",
    "The pipeline was intentionally built step-by-step to mirror MLOps practices rather than a one-off modeling notebook.\n",
    "\n",
    "---\n",
    "\n",
    "## High-Level Workflow\n",
    "\n",
    "### 1. Raw Data Ingestion (S3 Data Lake)\n",
    "- Created an S3 bucket to act as a data lake.\n",
    "- Uploaded all **9 raw CSV files** from the Kaggle Olist dataset.\n",
    "- Organized raw data under: s3:///raw/olist/ingest_date=YYYY-MM-DD/\n",
    "- Each dataset was placed into its own subfolder to support Athena\u2019s directory-based table requirements.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Data Cataloging & Querying (Athena)\n",
    "- Created an Athena database (`olist_datalake`).\n",
    "- Defined **external tables** for each dataset directly from JupyterLab (no Glue crawler required).\n",
    "- Verified schemas and row counts using Athena queries.\n",
    "- This step enabled SQL-based access to the data and served as the cataloging layer for downstream analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Exploratory Data Analysis (SageMaker + Pandas)\n",
    "- Loaded Athena tables into Pandas using `awswrangler`.\n",
    "- Performed sanity checks on row counts and joins.\n",
    "- Built an **order-level analytical view** by aggregating:\n",
    "- order items\n",
    "- payments\n",
    "- customer attributes\n",
    "- Engineered a target variable (`is_late`) based on delivery vs. estimated delivery dates.\n",
    "- Observed class imbalance (~8% late deliveries), motivating careful splitting and evaluation later.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Feature Engineering\n",
    "- Created leakage-safe, order-level features using only information available at purchase time:\n",
    "- pricing, freight, number of items/sellers\n",
    "- payment information\n",
    "- time-based features (day of week, hour of day)\n",
    "- customer state\n",
    "- Maintained a **canonical feature dataset** for analysis and splitting.\n",
    "- Created a **Feature Store\u2013compatible version** with strict data types.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. SageMaker Feature Store (Offline Store)\n",
    "- Created a **SageMaker Feature Group** (offline store only to control cost).\n",
    "- Used `order_id` as the record identifier.\n",
    "- Used a strictly formatted ISO-8601 `event_time` with UTC (`Z`) as the event time feature.\n",
    "- Successfully ingested ~99k feature records into Feature Store.\n",
    "- Offline store data is persisted in S3 for training and future reuse.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Dataset Splitting (Time-Based)\n",
    "- Performed a **time-based split** using `event_time` to avoid temporal leakage:\n",
    "- Train: ~40%\n",
    "- Validation: ~10%\n",
    "- Test: ~10%\n",
    "- Production reserve: ~40%\n",
    "- Persisted each split as Parquet files to: s3:///splits/olist/features/version=v1/\n",
    "- This mirrors a real production setup where recent data is reserved for inference.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Engineering Decisions & Lessons Learned\n",
    "\n",
    "- **Athena LOCATION must point to directories**, not individual files.\n",
    "- **Feature Store requires strict ISO-8601 timestamps with timezone** \u2014 missing the `Z` suffix causes ingestion failures.\n",
    "- Maintaining separate:\n",
    "- canonical feature data (analysis-friendly)\n",
    "- Feature Store\u2013safe data (schema-restricted)\n",
    "is a best practice in real MLOps systems.\n",
    "- Time-based splitting is critical to avoid data leakage in temporal datasets.\n",
    "- Offline Feature Store provides the required functionality while minimizing cost.\n",
    "\n",
    "---\n",
    "\n",
    "## Cost Management Notes\n",
    "- SageMaker compute was stopped immediately after completion.\n",
    "- Feature Store **online store was intentionally disabled** to avoid ongoing charges.\n",
    "- S3 storage costs are minimal and safe to keep until final submission.\n",
    "- Cleanup (Feature Group deletion, S3 cleanup) should only be done **after submission**.\n",
    "\n",
    "---\n",
    "\n",
    "## For Teammates\n",
    "If you need to re-run or extend this work:\n",
    "1. Start at the Athena read step (no need to re-upload raw data).\n",
    "2. Do **not** re-run ingestion unless changing the feature schema.\n",
    "3. Always stop SageMaker compute when finished.\n",
    "\n",
    "This notebook represents a complete, MLOps data pipeline.\n",
    "\n",
    "## Model Benchmark and Evaluation (Module 4)\n",
    "\n",
    "### Baseline Model\n",
    "As a benchmark, we implemented a simple heuristic model that always predicts an order will be delivered on time. This reflects the majority class in the dataset and establishes a lower bound for model performance.\n",
    "\n",
    "Due to class imbalance (~86% of orders are not late), the baseline achieves high accuracy but fails to identify late deliveries, resulting in zero precision, recall, and F1-score.\n",
    "\n",
    "### First Iteration Model (XGBoost v1)\n",
    "We trained a first-pass XGBoost binary classifier in Amazon SageMaker using a limited set of engineered features related to order size, payment behavior, and purchase timing.\n",
    "\n",
    "The model was evaluated using SageMaker Batch Transform on the held-out test dataset. Batch Transform was selected over a real-time endpoint to minimize cost and ensure automatic resource cleanup.\n",
    "\n",
    "### Results Summary\n",
    "- The XGBoost model achieved an AUC of approximately **0.56**, indicating it learned some discriminative signal beyond random chance.\n",
    "- Overall accuracy matched the baseline model due to class imbalance and use of a default classification threshold.\n",
    "- Precision, recall, and F1-score remained low, highlighting the need for future improvements such as class weighting, threshold tuning, and feature expansion.\n",
    "\n",
    "### Key Takeaways\n",
    "This iteration establishes a complete, cost-aware MLOps workflow including data ingestion, feature engineering, model training, evaluation, and deployment via batch inference. While performance improvements are needed, this version serves as a strong baseline for future model iterations and CI/CD integration in later modules.\n",
    "\n",
    "## Monitoring, Evaluation, and Reporting Summary (Module 5)\n",
    "\n",
    "### Model Benchmarking and Evaluation\n",
    "We established a simple benchmark model that always predicts orders as on-time. While this baseline achieved high accuracy due to class imbalance, it had zero recall for late deliveries. We then trained an XGBoost classifier using a small, carefully selected feature set. Model performance was evaluated using batch inference, and metrics such as accuracy and AUC were compared against the benchmark to establish a minimum viable improvement baseline.\n",
    "\n",
    "### Data Monitoring\n",
    "Data monitoring was implemented using SageMaker Batch Transform with batch data capture enabled. Production inference inputs and corresponding model outputs were automatically captured to S3. Baseline statistics and constraints were generated using SageMaker Model Monitor and stored in S3. These artifacts enable offline drift detection and future scheduled monitoring without requiring a persistent real-time endpoint, aligning with cost-efficient MLOps best practices.\n",
    "\n",
    "### Infrastructure Monitoring\n",
    "Infrastructure-level monitoring was implemented using Amazon CloudWatch. Metrics for SageMaker training jobs, batch transform jobs, and processing jobs were collected automatically. A centralized CloudWatch dashboard was created to visualize job durations and failure counts, providing operational visibility into the ML system.\n",
    "\n",
    "### Reports and Artifacts\n",
    "The system produces the following monitoring artifacts:\n",
    "- Model training metrics in CloudWatch Logs\n",
    "- Baseline statistics and constraints in S3\n",
    "- Captured production input and output data in S3\n",
    "- CloudWatch dashboard for infrastructure monitoring\n",
    "\n",
    "This architecture supports scalable monitoring, auditability, and future CI/CD integration while remaining within budget constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "d7992abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidated Imports\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import awswrangler as wr\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "from botocore.exceptions import ClientError\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "0e10ac1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing data found. Using new prefix: raw/olist/ingest_date=2026-02-18/\n",
      "Target Bucket: sagemaker-us-east-1-587322031938\n",
      "Target Prefix: raw/olist/ingest_date=2026-02-18/\n"
     ]
    }
   ],
   "source": [
    "# Global Setup\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "region = boto3.session.Session().region_name\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Dynamic Prefix Detection\n",
    "# Look for existing ingestion folders under raw/olist/\n",
    "resp = s3.list_objects_v2(Bucket=bucket, Prefix='raw/olist/', Delimiter='/')\n",
    "prefixes = [p['Prefix'] for p in resp.get('CommonPrefixes', [])]\n",
    "\n",
    "if prefixes:\n",
    "    # Sort ensuring YYYY-MM-DD format sorts correctly\n",
    "    # Format expected: raw/olist/ingest_date=YYYY-MM-DD/\n",
    "    prefix = sorted(prefixes)[-1]\n",
    "    print(f'Found existing data: {prefix}')\n",
    "else:\n",
    "    # Fallback to today if no data found\n",
    "    today = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "    prefix = f'raw/olist/ingest_date={today}/'\n",
    "    print(f'No existing data found. Using new prefix: {prefix}')\n",
    "\n",
    "print(f'Target Bucket: {bucket}')\n",
    "print(f'Target Prefix: {prefix}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "87ff4373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidated Imports\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "from botocore.exceptions import ClientError\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "62117dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing data found. Using new prefix: raw/olist/ingest_date=2026-02-18/\n",
      "Target Bucket: sagemaker-us-east-1-587322031938\n",
      "Target Prefix: raw/olist/ingest_date=2026-02-18/\n"
     ]
    }
   ],
   "source": [
    "# Global Setup\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "region = boto3.session.Session().region_name\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Dynamic Prefix Detection\n",
    "# Look for existing ingestion folders under raw/olist/\n",
    "resp = s3.list_objects_v2(Bucket=bucket, Prefix='raw/olist/', Delimiter='/')\n",
    "prefixes = [p['Prefix'] for p in resp.get('CommonPrefixes', [])]\n",
    "\n",
    "if prefixes:\n",
    "    # Sort ensuring YYYY-MM-DD format sorts correctly\n",
    "    # Format expected: raw/olist/ingest_date=YYYY-MM-DD/\n",
    "    prefix = sorted(prefixes)[-1]\n",
    "    print(f'Found existing data: {prefix}')\n",
    "else:\n",
    "    # Fallback to today if no data found\n",
    "    today = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "    print(f'No existing data found. Using new prefix: {prefix}')\n",
    "\n",
    "print(f'Target Bucket: {bucket}')\n",
    "print(f'Target Prefix: {prefix}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "3a684ab4-5185-4603-9f30-358b125dcba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found files: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "resp = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "\n",
    "keys = [obj[\"Key\"] for obj in resp.get(\"Contents\", [])]\n",
    "print(\"Found files:\", len(keys))\n",
    "for k in keys:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "9273a5f2-6a04-48e3-91fc-89dd28b3cd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Athena results prefix exists: s3://sagemaker-us-east-1-587322031938/athena-results/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results_prefix = \"athena-results/\"\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "resp = s3.list_objects_v2(Bucket=bucket, Prefix=results_prefix, MaxKeys=1)\n",
    "\n",
    "if \"Contents\" in resp:\n",
    "    print(\"Athena results prefix exists:\", f\"s3://{bucket}/{results_prefix}\")\n",
    "else:\n",
    "    # create a zero-byte object so the prefix exists\n",
    "    s3.put_object(Bucket=bucket, Key=results_prefix)\n",
    "    print(\"Created Athena results prefix:\", f\"s3://{bucket}/{results_prefix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "9ca03c11-54ff-441e-bf5e-fd9c05665d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database ready: olist_datalake\n"
     ]
    }
   ],
   "source": [
    "\n",
    "REGION = boto3.session.Session().region_name\n",
    "athena = boto3.client(\"athena\", region_name=REGION)\n",
    "\n",
    "ATHENA_OUTPUT = f\"s3://{bucket}/athena-results/\"\n",
    "DB = \"olist_datalake\"\n",
    "\n",
    "def run_athena(sql: str, database: str = \"default\"):\n",
    "    res = athena.start_query_execution(\n",
    "        QueryString=sql,\n",
    "        QueryExecutionContext={\"Database\": database},\n",
    "        ResultConfiguration={\"OutputLocation\": ATHENA_OUTPUT},\n",
    "    )\n",
    "    qid = res[\"QueryExecutionId\"]\n",
    "    while True:\n",
    "        q = athena.get_query_execution(QueryExecutionId=qid)\n",
    "        state = q[\"QueryExecution\"][\"Status\"][\"State\"]\n",
    "        if state in (\"SUCCEEDED\", \"FAILED\", \"CANCELLED\"):\n",
    "            break\n",
    "        time.sleep(1)\n",
    "    if state != \"SUCCEEDED\":\n",
    "        reason = q[\"QueryExecution\"][\"Status\"].get(\"StateChangeReason\", \"Unknown\")\n",
    "        raise RuntimeError(f\"Athena query failed: {state} - {reason}\\nSQL:\\n{sql}\")\n",
    "    return qid\n",
    "\n",
    "run_athena(f\"CREATE DATABASE IF NOT EXISTS {DB};\", database=\"default\")\n",
    "print(\"Database ready:\", DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "e977a0a3-98b4-4b4a-90b6-d6b93a85f392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying to: raw/olist/ingest_date=2026-02-18/olist_customers_dataset/olist_customers_dataset.csv\n",
      "Warning: Could not copy olist_customers_dataset.csv. Is it uploaded to raw/olist/ingest_date=2026-02-18/olist_customers_dataset.csv?\n",
      "Copying to: raw/olist/ingest_date=2026-02-18/olist_geolocation_dataset/olist_geolocation_dataset.csv\n",
      "Warning: Could not copy olist_geolocation_dataset.csv. Is it uploaded to raw/olist/ingest_date=2026-02-18/olist_geolocation_dataset.csv?\n",
      "Copying to: raw/olist/ingest_date=2026-02-18/olist_order_items_dataset/olist_order_items_dataset.csv\n",
      "Warning: Could not copy olist_order_items_dataset.csv. Is it uploaded to raw/olist/ingest_date=2026-02-18/olist_order_items_dataset.csv?\n",
      "Copying to: raw/olist/ingest_date=2026-02-18/olist_order_payments_dataset/olist_order_payments_dataset.csv\n",
      "Warning: Could not copy olist_order_payments_dataset.csv. Is it uploaded to raw/olist/ingest_date=2026-02-18/olist_order_payments_dataset.csv?\n",
      "Copying to: raw/olist/ingest_date=2026-02-18/olist_order_reviews_dataset/olist_order_reviews_dataset.csv\n",
      "Warning: Could not copy olist_order_reviews_dataset.csv. Is it uploaded to raw/olist/ingest_date=2026-02-18/olist_order_reviews_dataset.csv?\n",
      "Copying to: raw/olist/ingest_date=2026-02-18/olist_orders_dataset/olist_orders_dataset.csv\n",
      "Warning: Could not copy olist_orders_dataset.csv. Is it uploaded to raw/olist/ingest_date=2026-02-18/olist_orders_dataset.csv?\n",
      "Copying to: raw/olist/ingest_date=2026-02-18/olist_products_dataset/olist_products_dataset.csv\n",
      "Warning: Could not copy olist_products_dataset.csv. Is it uploaded to raw/olist/ingest_date=2026-02-18/olist_products_dataset.csv?\n",
      "Copying to: raw/olist/ingest_date=2026-02-18/olist_sellers_dataset/olist_sellers_dataset.csv\n",
      "Warning: Could not copy olist_sellers_dataset.csv. Is it uploaded to raw/olist/ingest_date=2026-02-18/olist_sellers_dataset.csv?\n",
      "Copying to: raw/olist/ingest_date=2026-02-18/product_category_name_translation/product_category_name_translation.csv\n",
      "Warning: Could not copy product_category_name_translation.csv. Is it uploaded to raw/olist/ingest_date=2026-02-18/product_category_name_translation.csv?\n",
      "\n",
      "Data preparation check complete.\n"
     ]
    }
   ],
   "source": [
    "# Copy raw files into subfolders (Only if they do not exist)\n",
    "files = [\n",
    "    'olist_customers_dataset.csv',\n",
    "    'olist_geolocation_dataset.csv',\n",
    "    'olist_order_items_dataset.csv',\n",
    "    'olist_order_payments_dataset.csv',\n",
    "    'olist_order_reviews_dataset.csv',\n",
    "    'olist_orders_dataset.csv',\n",
    "    'olist_products_dataset.csv',\n",
    "    'olist_sellers_dataset.csv',\n",
    "    'product_category_name_translation.csv',\n",
    "]\n",
    "\n",
    "for f in files:\n",
    "    src_key = prefix + f\n",
    "    folder = f.replace('.csv', '')\n",
    "    dst_key = f'{prefix}{folder}/{f}'\n",
    "    \n",
    "    try:\n",
    "        s3.head_object(Bucket=bucket, Key=dst_key)\n",
    "        print(f'Skipping copy, file already exists: {dst_key}')\n",
    "    except:\n",
    "        print(f'Copying to: {dst_key}')\n",
    "        try:\n",
    "            s3.copy_object(\n",
    "                Bucket=bucket,\n",
    "                CopySource={'Bucket': bucket, 'Key': src_key},\n",
    "                Key=dst_key\n",
    "            )\n",
    "        except Exception as e:\n",
    "             print(f'Warning: Could not copy {f}. Is it uploaded to {src_key}?')\n",
    "\n",
    "print('\\nData preparation check complete.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "858ee701-16e0-4030-bd4c-c966fe47a717",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = s3.list_objects_v2(Bucket=bucket, Prefix=prefix, MaxKeys=50)\n",
    "for obj in resp.get(\"Contents\", []):\n",
    "    print(obj[\"Key\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "c29e34fd-478d-486e-ab8e-bf2105b59b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: olist_datalake.olist_customers_dataset\n",
      "Created: olist_datalake.olist_geolocation_dataset\n",
      "Created: olist_datalake.olist_order_items_dataset\n",
      "Created: olist_datalake.olist_order_payments_dataset\n",
      "Created: olist_datalake.olist_order_reviews_dataset\n",
      "Created: olist_datalake.olist_orders_dataset\n",
      "Created: olist_datalake.olist_products_dataset\n",
      "Created: olist_datalake.olist_sellers_dataset\n",
      "Created: olist_datalake.product_category_name_translation\n"
     ]
    }
   ],
   "source": [
    "RAW_BASE = f\"s3://{bucket}/{prefix}\"\n",
    "\n",
    "def create_csv_table(table_name: str, columns_ddl: str, folder_name: str):\n",
    "    sql = f\"\"\"\n",
    "    CREATE EXTERNAL TABLE IF NOT EXISTS {DB}.{table_name} (\n",
    "      {columns_ddl}\n",
    "    )\n",
    "    ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
    "    WITH SERDEPROPERTIES (\n",
    "      'separatorChar' = ',',\n",
    "      'quoteChar'     = '\\\"',\n",
    "      'escapeChar'    = '\\\\\\\\'\n",
    "    )\n",
    "    STORED AS TEXTFILE\n",
    "    LOCATION '{RAW_BASE}{folder_name}/'\n",
    "    TBLPROPERTIES ('skip.header.line.count'='1');\n",
    "    \"\"\"\n",
    "    run_athena(sql, database=DB)\n",
    "    print(f\"Created: {DB}.{table_name}\")\n",
    "\n",
    "create_csv_table(\n",
    "    \"olist_customers_dataset\",\n",
    "    \"\"\"\n",
    "    customer_id string,\n",
    "    customer_unique_id string,\n",
    "    customer_zip_code_prefix int,\n",
    "    customer_city string,\n",
    "    customer_state string\n",
    "    \"\"\",\n",
    "    \"olist_customers_dataset\"\n",
    ")\n",
    "\n",
    "create_csv_table(\n",
    "    \"olist_geolocation_dataset\",\n",
    "    \"\"\"\n",
    "    geolocation_zip_code_prefix int,\n",
    "    geolocation_lat double,\n",
    "    geolocation_lng double,\n",
    "    geolocation_city string,\n",
    "    geolocation_state string\n",
    "    \"\"\",\n",
    "    \"olist_geolocation_dataset\"\n",
    ")\n",
    "\n",
    "create_csv_table(\n",
    "    \"olist_order_items_dataset\",\n",
    "    \"\"\"\n",
    "    order_id string,\n",
    "    order_item_id int,\n",
    "    product_id string,\n",
    "    seller_id string,\n",
    "    shipping_limit_date string,\n",
    "    price double,\n",
    "    freight_value double\n",
    "    \"\"\",\n",
    "    \"olist_order_items_dataset\"\n",
    ")\n",
    "\n",
    "create_csv_table(\n",
    "    \"olist_order_payments_dataset\",\n",
    "    \"\"\"\n",
    "    order_id string,\n",
    "    payment_sequential int,\n",
    "    payment_type string,\n",
    "    payment_installments int,\n",
    "    payment_value double\n",
    "    \"\"\",\n",
    "    \"olist_order_payments_dataset\"\n",
    ")\n",
    "\n",
    "create_csv_table(\n",
    "    \"olist_order_reviews_dataset\",\n",
    "    \"\"\"\n",
    "    review_id string,\n",
    "    order_id string,\n",
    "    review_score int,\n",
    "    review_comment_title string,\n",
    "    review_comment_message string,\n",
    "    review_creation_date string,\n",
    "    review_answer_timestamp string\n",
    "    \"\"\",\n",
    "    \"olist_order_reviews_dataset\"\n",
    ")\n",
    "\n",
    "create_csv_table(\n",
    "    \"olist_orders_dataset\",\n",
    "    \"\"\"\n",
    "    order_id string,\n",
    "    customer_id string,\n",
    "    order_status string,\n",
    "    order_purchase_timestamp string,\n",
    "    order_approved_at string,\n",
    "    order_delivered_carrier_date string,\n",
    "    order_delivered_customer_date string,\n",
    "    order_estimated_delivery_date string\n",
    "    \"\"\",\n",
    "    \"olist_orders_dataset\"\n",
    ")\n",
    "\n",
    "create_csv_table(\n",
    "    \"olist_products_dataset\",\n",
    "    \"\"\"\n",
    "    product_id string,\n",
    "    product_category_name string,\n",
    "    product_name_lenght int,\n",
    "    product_description_lenght int,\n",
    "    product_photos_qty int,\n",
    "    product_weight_g int,\n",
    "    product_length_cm int,\n",
    "    product_height_cm int,\n",
    "    product_width_cm int\n",
    "    \"\"\",\n",
    "    \"olist_products_dataset\"\n",
    ")\n",
    "\n",
    "create_csv_table(\n",
    "    \"olist_sellers_dataset\",\n",
    "    \"\"\"\n",
    "    seller_id string,\n",
    "    seller_zip_code_prefix int,\n",
    "    seller_city string,\n",
    "    seller_state string\n",
    "    \"\"\",\n",
    "    \"olist_sellers_dataset\"\n",
    ")\n",
    "\n",
    "create_csv_table(\n",
    "    \"product_category_name_translation\",\n",
    "    \"\"\"\n",
    "    product_category_name string,\n",
    "    product_category_name_english string\n",
    "    \"\"\",\n",
    "    \"product_category_name_translation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "393bfe94-4366-42c4-b48c-8a556f415750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHOW TABLES succeeded\n",
      "COUNT orders succeeded\n",
      "GROUP BY order_status succeeded\n"
     ]
    }
   ],
   "source": [
    "run_athena(f\"SHOW TABLES IN {DB};\", database=DB)\n",
    "print(\"SHOW TABLES succeeded\")\n",
    "\n",
    "run_athena(f\"SELECT COUNT(*) FROM {DB}.olist_orders_dataset;\", database=DB)\n",
    "print(\"COUNT orders succeeded\")\n",
    "\n",
    "run_athena(f\"SELECT order_status, COUNT(*) c FROM {DB}.olist_orders_dataset GROUP BY 1 ORDER BY c DESC;\", database=DB)\n",
    "print(\"GROUP BY order_status succeeded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "5d48e162-12df-4319-ba71-f3cf7f1412a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orders: (99441, 8)\n",
      "order_items: (112650, 7)\n",
      "payments: (103886, 5)\n",
      "customers: (99441, 5)\n"
     ]
    }
   ],
   "source": [
    "#6.1\n",
    "\n",
    "DB = \"olist_datalake\"\n",
    "\n",
    "orders = wr.athena.read_sql_query(\n",
    "    sql=f\"SELECT * FROM {DB}.olist_orders_dataset\",\n",
    "    database=DB,\n",
    "    ctas_approach=False\n",
    ")\n",
    "\n",
    "order_items = wr.athena.read_sql_query(\n",
    "    sql=f\"SELECT * FROM {DB}.olist_order_items_dataset\",\n",
    "    database=DB,\n",
    "    ctas_approach=False\n",
    ")\n",
    "\n",
    "payments = wr.athena.read_sql_query(\n",
    "    sql=f\"SELECT * FROM {DB}.olist_order_payments_dataset\",\n",
    "    database=DB,\n",
    "    ctas_approach=False\n",
    ")\n",
    "\n",
    "customers = wr.athena.read_sql_query(\n",
    "    sql=f\"SELECT * FROM {DB}.olist_customers_dataset\",\n",
    "    database=DB,\n",
    "    ctas_approach=False\n",
    ")\n",
    "\n",
    "print(\"orders:\", orders.shape)\n",
    "print(\"order_items:\", order_items.shape)\n",
    "print(\"payments:\", payments.shape)\n",
    "print(\"customers:\", customers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "2cfa6170-a17c-4f5c-8cf7-8770b8b46166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orders rows: 99441\n",
      "EDA rows: 99441\n",
      "Row loss: 0\n",
      "Late rate:\n",
      " is_late\n",
      "0    0.92129\n",
      "1    0.07871\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#6.2\n",
    "# Parse timestamps\n",
    "timestamp_cols = [\n",
    "    \"order_purchase_timestamp\",\n",
    "    \"order_approved_at\",\n",
    "    \"order_delivered_carrier_date\",\n",
    "    \"order_delivered_customer_date\",\n",
    "    \"order_estimated_delivery_date\",\n",
    "]\n",
    "for col in timestamp_cols:\n",
    "    orders[col] = pd.to_datetime(orders[col], errors=\"coerce\")\n",
    "\n",
    "# Aggregations\n",
    "items_agg = (\n",
    "    order_items.groupby(\"order_id\")\n",
    "    .agg(\n",
    "        num_items=(\"order_item_id\", \"count\"),\n",
    "        total_price=(\"price\", \"sum\"),\n",
    "        total_freight_value=(\"freight_value\", \"sum\"),\n",
    "        num_sellers=(\"seller_id\", \"nunique\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "payments_agg = (\n",
    "    payments.groupby(\"order_id\")\n",
    "    .agg(\n",
    "        payment_value=(\"payment_value\", \"sum\"),\n",
    "        payment_installments=(\"payment_installments\", \"max\"),\n",
    "        payment_type=(\"payment_type\", lambda x: x.value_counts().index[0]),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "eda_df = (\n",
    "    orders\n",
    "    .merge(items_agg, on=\"order_id\", how=\"left\")\n",
    "    .merge(payments_agg, on=\"order_id\", how=\"left\")\n",
    "    .merge(customers[[\"customer_id\", \"customer_state\"]], on=\"customer_id\", how=\"left\")\n",
    ")\n",
    "\n",
    "# Time features\n",
    "eda_df[\"purchase_dow\"] = eda_df[\"order_purchase_timestamp\"].dt.dayofweek\n",
    "eda_df[\"purchase_hour\"] = eda_df[\"order_purchase_timestamp\"].dt.hour\n",
    "\n",
    "# Label: late delivery\n",
    "eda_df[\"is_late\"] = (\n",
    "    (eda_df[\"order_delivered_customer_date\"].notna()) &\n",
    "    (eda_df[\"order_estimated_delivery_date\"].notna()) &\n",
    "    (eda_df[\"order_delivered_customer_date\"] > eda_df[\"order_estimated_delivery_date\"])\n",
    ").astype(int)\n",
    "\n",
    "print(\"Orders rows:\", len(orders))\n",
    "print(\"EDA rows:\", len(eda_df))\n",
    "print(\"Row loss:\", len(orders) - len(eda_df))\n",
    "print(\"Late rate:\\n\", eda_df[\"is_late\"].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "f6c44e50-a27a-4052-b192-82f344558f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat shape: (99441, 14)\n",
      "feat_fs shape: (99441, 13)\n",
      "event_time sample: ['2017-10-02T10:56:33Z', '2018-07-24T20:41:37Z', '2018-08-08T08:38:49Z', '2017-11-18T19:28:06Z', '2018-02-13T21:18:39Z']\n"
     ]
    }
   ],
   "source": [
    "#7.0 + 7B\n",
    "# Canonical features (with purchase timestamp)\n",
    "feat = eda_df[[\n",
    "    \"order_id\",\n",
    "    \"order_purchase_timestamp\",\n",
    "    \"customer_state\",\n",
    "    \"num_items\",\n",
    "    \"total_price\",\n",
    "    \"total_freight_value\",\n",
    "    \"num_sellers\",\n",
    "    \"payment_value\",\n",
    "    \"payment_installments\",\n",
    "    \"payment_type\",\n",
    "    \"purchase_dow\",\n",
    "    \"purchase_hour\",\n",
    "    \"is_late\"\n",
    "]].copy()\n",
    "\n",
    "feat[\"customer_state\"] = feat[\"customer_state\"].fillna(\"unknown\").astype(str)\n",
    "feat[\"payment_type\"] = feat[\"payment_type\"].fillna(\"unknown\").astype(str)\n",
    "\n",
    "for c in [\"num_items\", \"num_sellers\", \"payment_installments\", \"purchase_dow\", \"purchase_hour\", \"is_late\"]:\n",
    "    feat[c] = feat[c].fillna(0).astype(int)\n",
    "\n",
    "for c in [\"total_price\", \"total_freight_value\", \"payment_value\"]:\n",
    "    feat[c] = feat[c].fillna(0.0).astype(float)\n",
    "\n",
    "# Create strict ISO-8601 event time WITH timezone \"Z\"\n",
    "feat[\"event_time\"] = (\n",
    "    pd.to_datetime(feat[\"order_purchase_timestamp\"], errors=\"coerce\")\n",
    "    .dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    ")\n",
    "\n",
    "feat = feat.dropna(subset=[\"order_id\", \"event_time\"]).reset_index(drop=True)\n",
    "\n",
    "# FeatureStore-safe version (remove datetime64 column)\n",
    "feat_fs = feat.drop(columns=[\"order_purchase_timestamp\"]).copy()\n",
    "feat_fs[\"event_time\"] = feat_fs[\"event_time\"].astype(str)\n",
    "\n",
    "print(\"feat shape:\", feat.shape)\n",
    "print(\"feat_fs shape:\", feat_fs.shape)\n",
    "print(\"event_time sample:\", feat_fs[\"event_time\"].head().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "8f15a681-f32e-4103-be35-fab9b0646be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region: us-east-1\n",
      "Role: arn:aws:iam::587322031938:role/LabRole\n",
      "Offline store URI: s3://sagemaker-us-east-1-587322031938/feature-store/olist-order-features-v1/\n"
     ]
    }
   ],
   "source": [
    "#7.1\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "feature_group_name = \"olist-order-features-v1\"\n",
    "offline_store_s3_uri = f\"s3://{bucket}/feature-store/{feature_group_name}/\"\n",
    "\n",
    "fg = FeatureGroup(name=feature_group_name, sagemaker_session=sess)\n",
    "\n",
    "print(\"Region:\", region)\n",
    "print(\"Role:\", role)\n",
    "print(\"Offline store URI:\", offline_store_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "b652859d-e72d-4a6b-8af1-72af458ed97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Group already exists: olist-order-features-v1\n",
      "Status=Created, OfflineStoreStatus=Active\n",
      "Feature Group ready\n"
     ]
    }
   ],
   "source": [
    "#7.2\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "sm = boto3.client(\"sagemaker\", region_name=region)\n",
    "\n",
    "def feature_group_exists(name: str) -> bool:\n",
    "    try:\n",
    "        sm.describe_feature_group(FeatureGroupName=name)\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        if \"ResourceNotFound\" in str(e):\n",
    "            return False\n",
    "        raise\n",
    "\n",
    "def wait_for_fg_created(name: str, timeout_sec: int = 600, poll_sec: int = 10):\n",
    "    start = time.time()\n",
    "    while True:\n",
    "        desc = sm.describe_feature_group(FeatureGroupName=name)\n",
    "        status = desc.get(\"FeatureGroupStatus\")\n",
    "        offline_status = desc.get(\"OfflineStoreStatus\", {}).get(\"Status\", \"UNKNOWN\")\n",
    "        print(f\"Status={status}, OfflineStoreStatus={offline_status}\")\n",
    "        if status == \"Created\" and offline_status in (\"Active\", \"UNKNOWN\"):\n",
    "            return desc\n",
    "        if status in (\"CreateFailed\", \"DeleteFailed\"):\n",
    "            raise RuntimeError(f\"Feature Group failed with status={status}. Details: {desc}\")\n",
    "        if time.time() - start > timeout_sec:\n",
    "            raise TimeoutError(f\"Timed out waiting for Feature Group to be Created: {name}\")\n",
    "        time.sleep(poll_sec)\n",
    "\n",
    "if feature_group_exists(feature_group_name):\n",
    "    print(f\"Feature Group already exists: {feature_group_name}\")\n",
    "else:\n",
    "    fg.load_feature_definitions(data_frame=feat_fs)\n",
    "    fg.create(\n",
    "        s3_uri=offline_store_s3_uri,\n",
    "        record_identifier_name=\"order_id\",\n",
    "        event_time_feature_name=\"event_time\",\n",
    "        role_arn=role,\n",
    "        enable_online_store=False,\n",
    "    )\n",
    "    print(\"Create request submitted\")\n",
    "\n",
    "wait_for_fg_created(feature_group_name)\n",
    "print(\"Feature Group ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "2460fd33-5601-4e33-a636-da6a6ce54fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.feature_store.feature_group:Started ingesting index 0 to 49721\n",
      "INFO:sagemaker.feature_store.feature_group:Started ingesting index 49721 to 99441\n",
      "INFO:sagemaker.feature_store.feature_group:Successfully ingested row 49721 to 99441\n",
      "INFO:sagemaker.feature_store.feature_group:Successfully ingested row 0 to 49721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingest complete\n"
     ]
    }
   ],
   "source": [
    "#7.3\n",
    "ingest_response = fg.ingest(data_frame=feat_fs, max_workers=2, wait=True)\n",
    "print(\"Ingest complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "e1afae01-1c16-48a2-ab19-d0115ae85d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureGroupStatus: Created\n",
      "OfflineStoreStatus: Active\n",
      "S3 Offline Store Uri: s3://sagemaker-us-east-1-587322031938/feature-store/olist-order-features-v1/\n"
     ]
    }
   ],
   "source": [
    "#7.4\n",
    "\n",
    "sm = boto3.client(\"sagemaker\", region_name=region)\n",
    "desc = sm.describe_feature_group(FeatureGroupName=feature_group_name)\n",
    "\n",
    "print(\"FeatureGroupStatus:\", desc[\"FeatureGroupStatus\"])\n",
    "print(\"OfflineStoreStatus:\", desc[\"OfflineStoreStatus\"][\"Status\"])\n",
    "print(\"S3 Offline Store Uri:\", desc[\"OfflineStoreConfig\"][\"S3StorageConfig\"][\"S3Uri\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "68adb030-ec45-4e9a-bd5a-abbbce5622ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes\n",
      "train: 39776\n",
      "val:   9944\n",
      "test:  9944\n",
      "prod:  39777\n",
      "Wrote splits to: s3://sagemaker-us-east-1-587322031938/splits/olist/features/version=v1/\n"
     ]
    }
   ],
   "source": [
    "#8.0\n",
    "\n",
    "\n",
    "# Use feat_fs for splits (Feature Store compatible)\n",
    "feat_sorted = feat_fs.sort_values(\"event_time\").reset_index(drop=True)\n",
    "n = len(feat_sorted)\n",
    "\n",
    "train_end = int(n * 0.40)\n",
    "val_end   = int(n * 0.50)\n",
    "test_end  = int(n * 0.60)\n",
    "\n",
    "train_df = feat_sorted.iloc[:train_end]\n",
    "val_df   = feat_sorted.iloc[train_end:val_end]\n",
    "test_df  = feat_sorted.iloc[val_end:test_end]\n",
    "prod_df  = feat_sorted.iloc[test_end:]\n",
    "\n",
    "print(\"Split sizes\")\n",
    "print(\"train:\", len(train_df))\n",
    "print(\"val:  \", len(val_df))\n",
    "print(\"test: \", len(test_df))\n",
    "print(\"prod: \", len(prod_df))\n",
    "\n",
    "split_base = f\"s3://{bucket}/splits/olist/features/version=v1/\"\n",
    "wr.s3.to_parquet(train_df, f\"{split_base}train/\", dataset=True, mode=\"overwrite\")\n",
    "wr.s3.to_parquet(val_df,   f\"{split_base}val/\",   dataset=True, mode=\"overwrite\")\n",
    "wr.s3.to_parquet(test_df,  f\"{split_base}test/\",  dataset=True, mode=\"overwrite\")\n",
    "wr.s3.to_parquet(prod_df,  f\"{split_base}prod/\",  dataset=True, mode=\"overwrite\")\n",
    "\n",
    "print(\"Wrote splits to:\", split_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "e8ab440e-a4ea-477e-bad5-fc372615b00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "5822e971-6f0e-4ddb-acb7-41898a947a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objects found: 4\n",
      "splits/olist/features/version=v1/prod/bc93f1fe93db48b0a9f52ace4a5057a0.snappy.parquet\n",
      "splits/olist/features/version=v1/test/32444716227a46a985813316a5ecd552.snappy.parquet\n",
      "splits/olist/features/version=v1/train/99e0cf35aa0d4c48b2a25d73fa7b9c09.snappy.parquet\n",
      "splits/olist/features/version=v1/val/67380724b7ed46a7bda7ccd2fe105022.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "prefix = \"splits/olist/features/version=v1/\"\n",
    "\n",
    "resp = s3.list_objects_v2(Bucket=bucket, Prefix=prefix, MaxKeys=50)\n",
    "print(\"Objects found:\", resp.get(\"KeyCount\", 0))\n",
    "for obj in resp.get(\"Contents\", [])[:20]:\n",
    "    print(obj[\"Key\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "bab04fb0-989f-4744-bbda-b33cb55b73ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39776\n"
     ]
    }
   ],
   "source": [
    "#M4 Start\n",
    "try:\n",
    "    print(len(train))\n",
    "except NameError:\n",
    "    print(\"Kernel is fresh \u2014 variables not loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "c0034099-aa6f-4045-a5b2-4375191fdacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "split_base = f\"s3://{bucket}/splits/olist/features/version=v1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "ae9faaf5-2b66-4cc1-b3e1-7d0e8e541b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39776, 13) (9944, 13) (9944, 13)\n"
     ]
    }
   ],
   "source": [
    "train = wr.s3.read_parquet(f\"{split_base}train/\")\n",
    "val   = wr.s3.read_parquet(f\"{split_base}val/\")\n",
    "test  = wr.s3.read_parquet(f\"{split_base}test/\")\n",
    "\n",
    "print(train.shape, val.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "8d4945ad-cf8d-4724-a7a2-9a62e20ad344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8621279163314561, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0}"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#M4-1 Benchmark Model Baseline\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Ground truth\n",
    "y_test = test[\"is_late\"].astype(int)\n",
    "\n",
    "# Baseline prediction: always predict NOT late\n",
    "y_pred_baseline = pd.Series(0, index=test.index)\n",
    "\n",
    "baseline_metrics = {\n",
    "    \"accuracy\": accuracy_score(y_test, y_pred_baseline),\n",
    "    \"precision\": precision_score(y_test, y_pred_baseline, zero_division=0),\n",
    "    \"recall\": recall_score(y_test, y_pred_baseline, zero_division=0),\n",
    "    \"f1\": f1_score(y_test, y_pred_baseline, zero_division=0),\n",
    "}\n",
    "\n",
    "baseline_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "30e4bb0b-3070-47b1-9036-21f7f01bbbb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_late</th>\n",
       "      <th>num_items</th>\n",
       "      <th>total_price</th>\n",
       "      <th>total_freight_value</th>\n",
       "      <th>num_sellers</th>\n",
       "      <th>payment_value</th>\n",
       "      <th>payment_installments</th>\n",
       "      <th>purchase_dow</th>\n",
       "      <th>purchase_hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>72.89</td>\n",
       "      <td>63.34</td>\n",
       "      <td>1</td>\n",
       "      <td>136.23</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>59.50</td>\n",
       "      <td>15.56</td>\n",
       "      <td>1</td>\n",
       "      <td>75.06</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>40.95</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>134.97</td>\n",
       "      <td>8.49</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100.00</td>\n",
       "      <td>9.34</td>\n",
       "      <td>1</td>\n",
       "      <td>109.34</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_late  num_items  total_price  total_freight_value  num_sellers  \\\n",
       "0        0          2        72.89                63.34            1   \n",
       "1        0          1        59.50                15.56            1   \n",
       "2        0          0         0.00                 0.00            0   \n",
       "3        1          3       134.97                 8.49            1   \n",
       "4        0          1       100.00                 9.34            1   \n",
       "\n",
       "   payment_value  payment_installments  purchase_dow  purchase_hour  \n",
       "0         136.23                     1             6             21  \n",
       "1          75.06                     3             0              0  \n",
       "2          40.95                     2             1             15  \n",
       "3           0.00                     0             3             12  \n",
       "4         109.34                     1             6             22  "
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#M4-2 1st Model Sage Maker\n",
    "\n",
    "FEATURES = [\n",
    "    \"num_items\",\n",
    "    \"total_price\",\n",
    "    \"total_freight_value\",\n",
    "    \"num_sellers\",\n",
    "    \"payment_value\",\n",
    "    \"payment_installments\",\n",
    "    \"purchase_dow\",\n",
    "    \"purchase_hour\",\n",
    "]\n",
    "\n",
    "def to_xgb_matrix(df):\n",
    "    out = df[[\"is_late\"] + FEATURES].copy()\n",
    "    for c in FEATURES:\n",
    "        out[c] = pd.to_numeric(out[c], errors=\"coerce\").fillna(0.0)\n",
    "    out[\"is_late\"] = out[\"is_late\"].astype(int)\n",
    "    return out\n",
    "\n",
    "train_xgb = to_xgb_matrix(train)\n",
    "val_xgb   = to_xgb_matrix(val)\n",
    "test_xgb  = to_xgb_matrix(test)\n",
    "\n",
    "train_xgb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "a07fbf40-aeb6-4aa5-ab61-cdb2068dba2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: s3://sagemaker-us-east-1-587322031938/modeling/xgb_v1/train/train.csv\n",
      "Val:   s3://sagemaker-us-east-1-587322031938/modeling/xgb_v1/val/val.csv\n",
      "Test:  s3://sagemaker-us-east-1-587322031938/modeling/xgb_v1/test/test.csv\n"
     ]
    }
   ],
   "source": [
    "#M4-2-2\n",
    "## NOTE: CSVs already written prior to training \u2014 do not rerun\n",
    "xgb_prefix = f\"s3://{bucket}/modeling/xgb_v1/\"\n",
    "\n",
    "train_csv_uri = f\"{xgb_prefix}train/train.csv\"\n",
    "val_csv_uri   = f\"{xgb_prefix}val/val.csv\"\n",
    "test_csv_uri  = f\"{xgb_prefix}test/test.csv\"\n",
    "\n",
    "wr.s3.to_csv(train_xgb, train_csv_uri, index=False, header=False)\n",
    "wr.s3.to_csv(val_xgb,   val_csv_uri,   index=False, header=False)\n",
    "wr.s3.to_csv(test_xgb,  test_csv_uri,  index=False, header=False)\n",
    "\n",
    "print(\"Train:\", train_csv_uri)\n",
    "print(\"Val:  \", val_csv_uri)\n",
    "print(\"Test: \", test_csv_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "db400ea4-a70a-457a-9441-2c4bb5b1e071",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker.telemetry.telemetry_logging:SageMaker Python SDK will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features.\n",
      "To opt out of telemetry, please disable via TelemetryOptOut parameter in SDK defaults config. For more information, refer to https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk.\n",
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2026-02-18-02-57-54-957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-18 02:57:56 Starting - Starting the training job...\n",
      "2026-02-18 02:58:10 Starting - Preparing the instances for training...\n",
      "2026-02-18 02:58:35 Downloading - Downloading input data...\n",
      "2026-02-18 02:59:20 Downloading - Downloading the training image......\n",
      "2026-02-18 03:00:21 Training - Training image download completed. Training in progress.\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m[2026-02-18 03:00:23.171 ip-10-2-103-20.ec2.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2026-02-18 03:00:23.257 ip-10-2-103-20.ec2.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:00:23:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:00:23:INFO] Failed to parse hyperparameter eval_metric value auc to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:00:23:INFO] Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:00:23:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:00:23:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:00:23:INFO] Determined 0 GPU(s) available on the instance.\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:00:23:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:00:23:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:00:23:INFO] File path /opt/ml/input/data/train of input files\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:00:23:INFO] Making smlinks from folder /opt/ml/input/data/train to folder /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:00:23:INFO] creating symlink between Path /opt/ml/input/data/train/train.csv and destination /tmp/sagemaker_xgboost_input_data/train.csv-6930547573976008858\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:00:23:INFO] files path: /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:00:23:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:00:23:INFO] File path /opt/ml/input/data/validation of input files\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:00:23:INFO] Making smlinks from folder /opt/ml/input/data/validation to folder /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:00:23:INFO] creating symlink between Path /opt/ml/input/data/validation/val.csv and destination /tmp/sagemaker_xgboost_input_data/val.csv8900373662513482675\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:00:23:INFO] files path: /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:00:23:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:00:23:INFO] Single node training.\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:00:23:INFO] Train matrix has 39776 rows and 8 columns\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:00:23:INFO] Validation matrix has 9944 rows\u001b[0m\n",
      "\u001b[34m[2026-02-18 03:00:23.664 ip-10-2-103-20.ec2.internal:7 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2026-02-18 03:00:23.665 ip-10-2-103-20.ec2.internal:7 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2026-02-18 03:00:23.665 ip-10-2-103-20.ec2.internal:7 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2026-02-18 03:00:23.665 ip-10-2-103-20.ec2.internal:7 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:00:23:INFO] Debug hook created from config\u001b[0m\n",
      "\u001b[34m[0]#011train-auc:0.57718#011validation-auc:0.60987\u001b[0m\n",
      "\u001b[34m[2026-02-18 03:00:23.733 ip-10-2-103-20.ec2.internal:7 INFO hook.py:427] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[34m[2026-02-18 03:00:23.735 ip-10-2-103-20.ec2.internal:7 INFO hook.py:491] Hook is writing from the hook with pid: 7\u001b[0m\n",
      "\u001b[34m[1]#011train-auc:0.58678#011validation-auc:0.61750\u001b[0m\n",
      "\u001b[34m[2]#011train-auc:0.58967#011validation-auc:0.61922\u001b[0m\n",
      "\u001b[34m[3]#011train-auc:0.59464#011validation-auc:0.60855\u001b[0m\n",
      "\u001b[34m[4]#011train-auc:0.60033#011validation-auc:0.61503\u001b[0m\n",
      "\u001b[34m[5]#011train-auc:0.60274#011validation-auc:0.61439\u001b[0m\n",
      "\u001b[34m[6]#011train-auc:0.60120#011validation-auc:0.61226\u001b[0m\n",
      "\u001b[34m[7]#011train-auc:0.60385#011validation-auc:0.61266\u001b[0m\n",
      "\u001b[34m[8]#011train-auc:0.60486#011validation-auc:0.61274\u001b[0m\n",
      "\u001b[34m[9]#011train-auc:0.60711#011validation-auc:0.61181\u001b[0m\n",
      "\u001b[34m[10]#011train-auc:0.60697#011validation-auc:0.61046\u001b[0m\n",
      "\u001b[34m[11]#011train-auc:0.60916#011validation-auc:0.60854\u001b[0m\n",
      "\u001b[34m[12]#011train-auc:0.61248#011validation-auc:0.60882\u001b[0m\n",
      "\u001b[34m[13]#011train-auc:0.61380#011validation-auc:0.60906\u001b[0m\n",
      "\u001b[34m[14]#011train-auc:0.61496#011validation-auc:0.60803\u001b[0m\n",
      "\u001b[34m[15]#011train-auc:0.61577#011validation-auc:0.60883\u001b[0m\n",
      "\u001b[34m[16]#011train-auc:0.61938#011validation-auc:0.60737\u001b[0m\n",
      "\u001b[34m[17]#011train-auc:0.62391#011validation-auc:0.60716\u001b[0m\n",
      "\u001b[34m[18]#011train-auc:0.62429#011validation-auc:0.60665\u001b[0m\n",
      "\u001b[34m[19]#011train-auc:0.62490#011validation-auc:0.60728\u001b[0m\n",
      "\u001b[34m[20]#011train-auc:0.62650#011validation-auc:0.60556\u001b[0m\n",
      "\u001b[34m[21]#011train-auc:0.62888#011validation-auc:0.60332\u001b[0m\n",
      "\u001b[34m[22]#011train-auc:0.63095#011validation-auc:0.60233\u001b[0m\n",
      "\u001b[34m[23]#011train-auc:0.63570#011validation-auc:0.59728\u001b[0m\n",
      "\u001b[34m[24]#011train-auc:0.63690#011validation-auc:0.59524\u001b[0m\n",
      "\u001b[34m[25]#011train-auc:0.63829#011validation-auc:0.59603\u001b[0m\n",
      "\u001b[34m[26]#011train-auc:0.64073#011validation-auc:0.59587\u001b[0m\n",
      "\u001b[34m[27]#011train-auc:0.64565#011validation-auc:0.59651\u001b[0m\n",
      "\u001b[34m[28]#011train-auc:0.64638#011validation-auc:0.59769\u001b[0m\n",
      "\u001b[34m[29]#011train-auc:0.64903#011validation-auc:0.59423\u001b[0m\n",
      "\u001b[34m[30]#011train-auc:0.65247#011validation-auc:0.59341\u001b[0m\n",
      "\u001b[34m[31]#011train-auc:0.65328#011validation-auc:0.59380\u001b[0m\n",
      "\u001b[34m[32]#011train-auc:0.65542#011validation-auc:0.59545\u001b[0m\n",
      "\u001b[34m[33]#011train-auc:0.65942#011validation-auc:0.59481\u001b[0m\n",
      "\u001b[34m[34]#011train-auc:0.66074#011validation-auc:0.59615\u001b[0m\n",
      "\u001b[34m[35]#011train-auc:0.66255#011validation-auc:0.59685\u001b[0m\n",
      "\u001b[34m[36]#011train-auc:0.66308#011validation-auc:0.59537\u001b[0m\n",
      "\u001b[34m[37]#011train-auc:0.66411#011validation-auc:0.59450\u001b[0m\n",
      "\u001b[34m[38]#011train-auc:0.66587#011validation-auc:0.59220\u001b[0m\n",
      "\u001b[34m[39]#011train-auc:0.66659#011validation-auc:0.59020\u001b[0m\n",
      "\u001b[34m[40]#011train-auc:0.67067#011validation-auc:0.58837\u001b[0m\n",
      "\u001b[34m[41]#011train-auc:0.67208#011validation-auc:0.59180\u001b[0m\n",
      "\u001b[34m[42]#011train-auc:0.67470#011validation-auc:0.59018\u001b[0m\n",
      "\u001b[34m[43]#011train-auc:0.67599#011validation-auc:0.58973\u001b[0m\n",
      "\u001b[34m[44]#011train-auc:0.67859#011validation-auc:0.59031\u001b[0m\n",
      "\u001b[34m[45]#011train-auc:0.68025#011validation-auc:0.59029\u001b[0m\n",
      "\u001b[34m[46]#011train-auc:0.68055#011validation-auc:0.58914\u001b[0m\n",
      "\u001b[34m[47]#011train-auc:0.68338#011validation-auc:0.58682\u001b[0m\n",
      "\u001b[34m[48]#011train-auc:0.68535#011validation-auc:0.58531\u001b[0m\n",
      "\u001b[34m[49]#011train-auc:0.68663#011validation-auc:0.58506\u001b[0m\n",
      "\u001b[34m[50]#011train-auc:0.68960#011validation-auc:0.58067\u001b[0m\n",
      "\u001b[34m[51]#011train-auc:0.69082#011validation-auc:0.57944\u001b[0m\n",
      "\u001b[34m[52]#011train-auc:0.69350#011validation-auc:0.57876\u001b[0m\n",
      "\u001b[34m[53]#011train-auc:0.69431#011validation-auc:0.57790\u001b[0m\n",
      "\u001b[34m[54]#011train-auc:0.69508#011validation-auc:0.57814\u001b[0m\n",
      "\u001b[34m[55]#011train-auc:0.69788#011validation-auc:0.57586\u001b[0m\n",
      "\u001b[34m[56]#011train-auc:0.69958#011validation-auc:0.57689\u001b[0m\n",
      "\u001b[34m[57]#011train-auc:0.69967#011validation-auc:0.57617\u001b[0m\n",
      "\u001b[34m[58]#011train-auc:0.70106#011validation-auc:0.57703\u001b[0m\n",
      "\u001b[34m[59]#011train-auc:0.70124#011validation-auc:0.57684\u001b[0m\n",
      "\u001b[34m[60]#011train-auc:0.70351#011validation-auc:0.57662\u001b[0m\n",
      "\u001b[34m[61]#011train-auc:0.70420#011validation-auc:0.57699\u001b[0m\n",
      "\u001b[34m[62]#011train-auc:0.70539#011validation-auc:0.57893\u001b[0m\n",
      "\u001b[34m[63]#011train-auc:0.70704#011validation-auc:0.57891\u001b[0m\n",
      "\u001b[34m[64]#011train-auc:0.70791#011validation-auc:0.57796\u001b[0m\n",
      "\u001b[34m[65]#011train-auc:0.71070#011validation-auc:0.57732\u001b[0m\n",
      "\u001b[34m[66]#011train-auc:0.71187#011validation-auc:0.57656\u001b[0m\n",
      "\u001b[34m[67]#011train-auc:0.71214#011validation-auc:0.57604\u001b[0m\n",
      "\u001b[34m[68]#011train-auc:0.71341#011validation-auc:0.57686\u001b[0m\n",
      "\u001b[34m[69]#011train-auc:0.71415#011validation-auc:0.57558\u001b[0m\n",
      "\u001b[34m[70]#011train-auc:0.71535#011validation-auc:0.57584\u001b[0m\n",
      "\u001b[34m[71]#011train-auc:0.71621#011validation-auc:0.57537\u001b[0m\n",
      "\u001b[34m[72]#011train-auc:0.71661#011validation-auc:0.57624\u001b[0m\n",
      "\u001b[34m[73]#011train-auc:0.71805#011validation-auc:0.57630\u001b[0m\n",
      "\u001b[34m[74]#011train-auc:0.71962#011validation-auc:0.57667\u001b[0m\n",
      "\u001b[34m[75]#011train-auc:0.72031#011validation-auc:0.57737\u001b[0m\n",
      "\u001b[34m[76]#011train-auc:0.72202#011validation-auc:0.57646\u001b[0m\n",
      "\u001b[34m[77]#011train-auc:0.72265#011validation-auc:0.57596\u001b[0m\n",
      "\u001b[34m[78]#011train-auc:0.72382#011validation-auc:0.57490\u001b[0m\n",
      "\u001b[34m[79]#011train-auc:0.72416#011validation-auc:0.57438\u001b[0m\n",
      "\u001b[34m[80]#011train-auc:0.72454#011validation-auc:0.57428\u001b[0m\n",
      "\u001b[34m[81]#011train-auc:0.72604#011validation-auc:0.57419\u001b[0m\n",
      "\u001b[34m[82]#011train-auc:0.72744#011validation-auc:0.57596\u001b[0m\n",
      "\u001b[34m[83]#011train-auc:0.72923#011validation-auc:0.57486\u001b[0m\n",
      "\u001b[34m[84]#011train-auc:0.73094#011validation-auc:0.57390\u001b[0m\n",
      "\u001b[34m[85]#011train-auc:0.73201#011validation-auc:0.57408\u001b[0m\n",
      "\u001b[34m[86]#011train-auc:0.73278#011validation-auc:0.57454\u001b[0m\n",
      "\u001b[34m[87]#011train-auc:0.73306#011validation-auc:0.57505\u001b[0m\n",
      "\u001b[34m[88]#011train-auc:0.73337#011validation-auc:0.57517\u001b[0m\n",
      "\u001b[34m[89]#011train-auc:0.73389#011validation-auc:0.57457\u001b[0m\n",
      "\u001b[34m[90]#011train-auc:0.73583#011validation-auc:0.57328\u001b[0m\n",
      "\u001b[34m[91]#011train-auc:0.73729#011validation-auc:0.57133\u001b[0m\n",
      "\u001b[34m[92]#011train-auc:0.73771#011validation-auc:0.57227\u001b[0m\n",
      "\u001b[34m[93]#011train-auc:0.73876#011validation-auc:0.57303\u001b[0m\n",
      "\u001b[34m[94]#011train-auc:0.74023#011validation-auc:0.57281\u001b[0m\n",
      "\u001b[34m[95]#011train-auc:0.74183#011validation-auc:0.57206\u001b[0m\n",
      "\u001b[34m[96]#011train-auc:0.74366#011validation-auc:0.57112\u001b[0m\n",
      "\u001b[34m[97]#011train-auc:0.74468#011validation-auc:0.57151\u001b[0m\n",
      "\u001b[34m[98]#011train-auc:0.74505#011validation-auc:0.57078\u001b[0m\n",
      "\u001b[34m[99]#011train-auc:0.74611#011validation-auc:0.57011\u001b[0m\n",
      "\n",
      "2026-02-18 03:00:45 Uploading - Uploading generated training model\n",
      "2026-02-18 03:00:45 Completed - Training job completed\n",
      "Training seconds: 129\n",
      "Billable seconds: 129\n",
      "Training job: sagemaker-xgboost-2026-02-18-02-57-54-957\n"
     ]
    }
   ],
   "source": [
    "#M4-2-3 Train Model\n",
    "# TRAINING CELL (DO NOT RERUN)\n",
    "# This cell was executed once to train the initial XGBoost model.\n",
    "# Re-running this cell will retrain the model and incur additional cost.\n",
    "# The trained model is reused below via attachment for evaluation and deployment.\n",
    "\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "# Built-in XGBoost container\n",
    "xgb_image = retrieve(\n",
    "    framework=\"xgboost\",\n",
    "    region=region,\n",
    "    version=\"1.7-1\"\n",
    ")\n",
    "\n",
    "output_path = f\"s3://{bucket}/modeling/xgb_v1/output/\"\n",
    "\n",
    "xgb = Estimator(\n",
    "    image_uri=xgb_image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",  # budget-friendly\n",
    "    output_path=output_path,\n",
    "    sagemaker_session=sess,\n",
    ")\n",
    "\n",
    "# Simple, reasonable first-pass hyperparameters\n",
    "xgb.set_hyperparameters(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\",\n",
    "    num_round=100,\n",
    "    max_depth=4,\n",
    "    eta=0.2,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    ")\n",
    "\n",
    "train_input = TrainingInput(train_csv_uri, content_type=\"text/csv\")\n",
    "val_input   = TrainingInput(val_csv_uri, content_type=\"text/csv\")\n",
    "\n",
    "xgb.fit({\n",
    "    \"train\": train_input,\n",
    "    \"validation\": val_input\n",
    "})\n",
    "\n",
    "training_job_name = xgb.latest_training_job.name\n",
    "print(f\"Training job: {training_job_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "0151caee-82d9-4259-819d-bf0696a22481",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using training job: sagemaker-xgboost-2026-02-18-02-57-54-957\n",
      "Attached. Ready for Batch Transform.\n"
     ]
    }
   ],
   "source": [
    "# SKIP THIS CELL M4-2-3b Attach to Existing Trained Model (No Retraining)\n",
    "import boto3, sagemaker\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.session.Session().region_name\n",
    "sm = boto3.client(\"sagemaker\", region_name=region)\n",
    "\n",
    "# Pick the most recent completed training job\n",
    "jobs = sm.list_training_jobs(SortBy=\"CreationTime\", SortOrder=\"Descending\", MaxResults=20)[\"TrainingJobSummaries\"]\n",
    "training_job_name = next(j[\"TrainingJobName\"] for j in jobs if j[\"TrainingJobStatus\"] == \"Completed\")\n",
    "print(\"Using training job:\", training_job_name)\n",
    "\n",
    "# Recreate estimator and attach (no retraining)\n",
    "xgb_image = retrieve(framework=\"xgboost\", region=region, version=\"1.7-1\")\n",
    "xgb = Estimator(\n",
    "    image_uri=xgb_image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    output_path=f\"s3://{bucket}/modeling/xgb_v1/output/\",\n",
    "    sagemaker_session=sess,\n",
    ")\n",
    "xgb._current_job_name = training_job_name\n",
    "print(\"Attached. Ready for Batch Transform.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "574dce64-f186-495e-a51a-7c63553268cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: s3://sagemaker-us-east-1-587322031938/modeling/xgb_v1/train/train.csv\n",
      "Val:   s3://sagemaker-us-east-1-587322031938/modeling/xgb_v1/val/val.csv\n",
      "Test:  s3://sagemaker-us-east-1-587322031938/modeling/xgb_v1/test/test.csv\n"
     ]
    }
   ],
   "source": [
    "# M4-2-2b Recreate CSV URIs\n",
    "\n",
    "xgb_prefix = f\"s3://{bucket}/modeling/xgb_v1/\"\n",
    "\n",
    "train_csv_uri = f\"{xgb_prefix}train/train.csv\"\n",
    "val_csv_uri   = f\"{xgb_prefix}val/val.csv\"\n",
    "test_csv_uri  = f\"{xgb_prefix}test/test.csv\"\n",
    "\n",
    "print(\"Train:\", train_csv_uri)\n",
    "print(\"Val:  \", val_csv_uri)\n",
    "print(\"Test: \", test_csv_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "f7bf01c7-3661-40d1-8cb6-9298677f125d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote inference CSV: s3://sagemaker-us-east-1-587322031938/modeling/xgb_v1/test/test_infer.csv\n",
      "Shape (should be 9944 x 8): (9944, 8)\n"
     ]
    }
   ],
   "source": [
    "# M4-3.0 Create inference-only TEST input (features only)\n",
    "# Code developed using ChatGPT (ChatGPT, 2024) as a paired programmer.\n",
    "\n",
    "# test_xgb currently has: is_late + 8 features\n",
    "test_infer = test_xgb.drop(columns=[\"is_late\"]).copy()\n",
    "\n",
    "test_infer_csv_uri = f\"{xgb_prefix}test/test_infer.csv\"\n",
    "\n",
    "# IMPORTANT: no header, no index\n",
    "wr.s3.to_csv(test_infer, test_infer_csv_uri, index=False, header=False)\n",
    "\n",
    "print(\"Wrote inference CSV:\", test_infer_csv_uri)\n",
    "print(\"Shape (should be 9944 x 8):\", test_infer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "56bcb971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Training Job: sagemaker-xgboost-2026-02-18-02-57-54-957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating model with name: xgb-model-1771383672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Created Model: xgb-model-1771383672\n"
     ]
    }
   ],
   "source": [
    "# Fix: Ensure model_name is defined for Batch Transform\n",
    "# We use the training job from the previous step to create a SageMaker Model object\n",
    "\n",
    "if \"training_job_name\" not in locals():\n",
    "    # Fallback if running fresh\n",
    "    jobs = sm.list_training_jobs(SortBy=\"CreationTime\", SortOrder=\"Descending\", MaxResults=1)[\"TrainingJobSummaries\"]\n",
    "    training_job_name = jobs[0][\"TrainingJobName\"]\n",
    "\n",
    "print(f\"Using Training Job: {training_job_name}\")\n",
    "\n",
    "# Define model name\n",
    "import time\n",
    "model_name = f\"xgb-model-{int(time.time())}\"\n",
    "\n",
    "# Create Model object (registers it in SageMaker, needed for Transform)\n",
    "# We reuse the estimator definition from above if available, or lightweight recreation\n",
    "\n",
    "from sagemaker.model import Model\n",
    "\n",
    "# Get model artifacts from the training job\n",
    "info = sm.describe_training_job(TrainingJobName=training_job_name)\n",
    "model_data = info[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "xgb_image = retrieve(framework=\"xgboost\", region=region, version=\"1.7-1\")\n",
    "\n",
    "model = Model(\n",
    "    image_uri=xgb_image,\n",
    "    model_data=model_data,\n",
    "    role=role,\n",
    "    sagemaker_session=sess,\n",
    "    name=model_name\n",
    ")\n",
    "\n",
    "model.create(instance_type=\"ml.m5.large\")\n",
    "print(f\"\u2705 Created Model: {model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "93fd6908-b378-4c81-924d-062f65c4215a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating transform job with name: sagemaker-xgboost-2026-02-18-03-01-13-655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch transform (features-only input)...\n",
      "..................................\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:06:54:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:06:54:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:06:54:INFO] nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2026-02-18 03:06:54 +0000] [13] [INFO] Starting gunicorn 23.0.0\u001b[0m\n",
      "\u001b[34m[2026-02-18 03:06:54 +0000] [13] [INFO] Listening at: unix:/tmp/gunicorn.sock (13)\u001b[0m\n",
      "\u001b[34m[2026-02-18 03:06:54 +0000] [13] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2026-02-18 03:06:54 +0000] [16] [INFO] Booting worker with pid: 16\u001b[0m\n",
      "\u001b[34m[2026-02-18 03:06:54 +0000] [17] [INFO] Booting worker with pid: 17\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:06:56:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:06:56:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:06:56:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:06:56:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:06:56:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:06:56:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[32m2026-02-18T03:07:03.713:[sagemaker logs]: MaxConcurrentTransforms=2, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:07:03:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [18/Feb/2026:03:07:03 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [18/Feb/2026:03:07:03 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2026-02-18:03:07:03:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [18/Feb/2026:03:07:03 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [18/Feb/2026:03:07:03 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:07:03:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [18/Feb/2026:03:07:04 +0000] \"POST /invocations HTTP/1.1\" 200 200088 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2026-02-18:03:07:03:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [18/Feb/2026:03:07:04 +0000] \"POST /invocations HTTP/1.1\" 200 200088 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:06:54:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:06:54:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:06:54:INFO] nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2026-02-18 03:06:54 +0000] [13] [INFO] Starting gunicorn 23.0.0\u001b[0m\n",
      "\u001b[34m[2026-02-18 03:06:54 +0000] [13] [INFO] Listening at: unix:/tmp/gunicorn.sock (13)\u001b[0m\n",
      "\u001b[34m[2026-02-18 03:06:54 +0000] [13] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2026-02-18 03:06:54 +0000] [16] [INFO] Booting worker with pid: 16\u001b[0m\n",
      "\u001b[34m[2026-02-18 03:06:54 +0000] [17] [INFO] Booting worker with pid: 17\u001b[0m\n",
      "\u001b[35m[2026-02-18:03:06:54:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2026-02-18:03:06:54:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2026-02-18:03:06:54:INFO] nginx config: \u001b[0m\n",
      "\u001b[35mworker_processes auto;\u001b[0m\n",
      "\u001b[35mdaemon off;\u001b[0m\n",
      "\u001b[35mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[35merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[35mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[35mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35m[2026-02-18 03:06:54 +0000] [13] [INFO] Starting gunicorn 23.0.0\u001b[0m\n",
      "\u001b[35m[2026-02-18 03:06:54 +0000] [13] [INFO] Listening at: unix:/tmp/gunicorn.sock (13)\u001b[0m\n",
      "\u001b[35m[2026-02-18 03:06:54 +0000] [13] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[35m[2026-02-18 03:06:54 +0000] [16] [INFO] Booting worker with pid: 16\u001b[0m\n",
      "\u001b[35m[2026-02-18 03:06:54 +0000] [17] [INFO] Booting worker with pid: 17\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:06:56:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:06:56:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:06:56:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:06:56:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:06:56:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:06:56:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[35m[2026-02-18:03:06:56:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2026-02-18:03:06:56:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[35m[2026-02-18:03:06:56:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[35m[2026-02-18:03:06:56:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2026-02-18:03:06:56:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[35m[2026-02-18:03:06:56:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[32m2026-02-18T03:07:03.713:[sagemaker logs]: MaxConcurrentTransforms=2, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:07:03:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [18/Feb/2026:03:07:03 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [18/Feb/2026:03:07:03 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2026-02-18:03:07:03:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [18/Feb/2026:03:07:03 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [18/Feb/2026:03:07:03 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:07:03:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [18/Feb/2026:03:07:04 +0000] \"POST /invocations HTTP/1.1\" 200 200088 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2026-02-18:03:07:03:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [18/Feb/2026:03:07:04 +0000] \"POST /invocations HTTP/1.1\" 200 200088 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "Batch transform finished: s3://sagemaker-us-east-1-587322031938/modeling/xgb_v1/batch-out/test_v2/\n"
     ]
    }
   ],
   "source": [
    "# M4-3.1 Batch Transform on TEST (Evaluation) - v2 (features-only)\n",
    "from sagemaker.transformer import Transformer\n",
    "\n",
    "test_transform_output_v2 = f\"s3://{bucket}/modeling/xgb_v1/batch-out/test_v2/\"\n",
    "\n",
    "transformer = Transformer(\n",
    "    model_name=model_name,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    output_path=test_transform_output_v2,\n",
    "    assemble_with=\"Line\",\n",
    "    accept=\"text/csv\"\n",
    ")\n",
    "\n",
    "print(\"Starting batch transform (features-only input)...\")\n",
    "transformer.transform(\n",
    "    data=test_infer_csv_uri,\n",
    "    content_type=\"text/csv\",\n",
    "    split_type=\"Line\"\n",
    ")\n",
    "\n",
    "transformer.wait()\n",
    "print(\"Batch transform finished:\", test_transform_output_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "7c23b185-e888-4cb8-a1bf-1c2d89f33e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading predictions from: s3://sagemaker-us-east-1-587322031938/modeling/xgb_v1/batch-out/test_v2/test_infer.csv.out\n",
      "Model metrics: {'auc': 0.5635208855035949, 'accuracy': 0.8621279163314561, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline_always_on_time</th>\n",
       "      <td>0.862128</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb_v1</th>\n",
       "      <td>0.862128</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.563521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         accuracy  precision  recall   f1       auc\n",
       "baseline_always_on_time  0.862128        0.0     0.0  0.0       NaN\n",
       "xgb_v1                   0.862128        0.0     0.0  0.0  0.563521"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# M4-3.2 Load predictions and evaluate\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "resp = s3.list_objects_v2(Bucket=bucket, Prefix=\"modeling/xgb_v1/batch-out/test_v2/\")\n",
    "keys = [o[\"Key\"] for o in resp.get(\"Contents\", [])]\n",
    "out_files = [k for k in keys if k.endswith(\".out\")]\n",
    "\n",
    "if not out_files:\n",
    "    raise RuntimeError(f\"No .out files found in test_v2 output. Keys seen: {keys[:10]}\")\n",
    "\n",
    "out_key = sorted(out_files)[-1]\n",
    "out_uri = f\"s3://{bucket}/{out_key}\"\n",
    "print(\"Reading predictions from:\", out_uri)\n",
    "\n",
    "pred_df = wr.s3.read_csv(out_uri, header=None)\n",
    "y_prob = pred_df[0].astype(float).reset_index(drop=True)\n",
    "\n",
    "y_true = test_xgb[\"is_late\"].astype(int).reset_index(drop=True)\n",
    "y_hat = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "model_metrics = {\n",
    "    \"auc\": roc_auc_score(y_true, y_prob),\n",
    "    \"accuracy\": accuracy_score(y_true, y_hat),\n",
    "    \"precision\": precision_score(y_true, y_hat, zero_division=0),\n",
    "    \"recall\": recall_score(y_true, y_hat, zero_division=0),\n",
    "    \"f1\": f1_score(y_true, y_hat, zero_division=0),\n",
    "}\n",
    "\n",
    "comparison = pd.DataFrame(\n",
    "    [baseline_metrics, model_metrics],\n",
    "    index=[\"baseline_always_on_time\", \"xgb_v1\"]\n",
    ")\n",
    "\n",
    "print(\"Model metrics:\", model_metrics)\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "907c66f6-bf25-49d8-ac61-ca93e7100c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted model: xgb-model-1771383672\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sm = boto3.client(\"sagemaker\", region_name=region)\n",
    "sm.delete_model(ModelName=model_name)\n",
    "print(\"Deleted model:\", model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "80622011-15db-4d64-8218-fcea39c9552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impement:\n",
    "# Implement model monitors on your ML system.\n",
    "# Implement data monitors on your ML system.\n",
    "# Implement infrastructure monitors on your ML system.\n",
    "# Create a monitoring dashboard for your ML endpoint/job on CloudWatch.\n",
    "# Generate model and data reports on SageMaker.\n",
    "\n",
    "#0\n",
    "import time, json\n",
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker.model import Model\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.session.Session().region_name\n",
    "sm = boto3.client(\"sagemaker\", region_name=region)\n",
    "cw = boto3.client(\"cloudwatch\", region_name=region)\n",
    "\n",
    "xgb_prefix = f\"s3://{bucket}/modeling/xgb_v1/\"\n",
    "\n",
    "# Training job from earlier\n",
    "training_job_name = \"sagemaker-xgboost-2026-01-31-18-00-53-460\"\n",
    "\n",
    "# Your feature list (must match training)\n",
    "FEATURES = [\n",
    "    \"num_items\",\"total_price\",\"total_freight_value\",\"num_sellers\",\n",
    "    \"payment_value\",\"payment_installments\",\"purchase_dow\",\"purchase_hour\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "12e3532c-d63a-4fbf-82c0-c821e6437f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Found valid training job: sagemaker-xgboost-2026-02-18-02-57-54-957\n",
      "Job Status: Completed\n"
     ]
    }
   ],
   "source": [
    "# FORCE FIND A VALID TRAINING JOB\n",
    "# This fixes the \"Requested resource not found\" error by finding the most recent successful job in YOUR account.\n",
    "\n",
    "import boto3\n",
    "sm = boto3.client(\"sagemaker\")\n",
    "\n",
    "# List recent completed jobs\n",
    "response = sm.list_training_jobs(\n",
    "    SortBy=\"CreationTime\", \n",
    "    SortOrder=\"Descending\", \n",
    "    StatusEquals=\"Completed\", \n",
    "    MaxResults=5\n",
    ")\n",
    "\n",
    "if not response[\"TrainingJobSummaries\"]:\n",
    "    raise RuntimeError(\"No completed training jobs found! You must run the xgb.fit() cell at least once.\")\n",
    "\n",
    "# Pick the most recent one\n",
    "training_job_name = response[\"TrainingJobSummaries\"][0][\"TrainingJobName\"]\n",
    "print(f\"\u2705 Found valid training job: {training_job_name}\")\n",
    "\n",
    "# Now we can safely describe it\n",
    "tj = sm.describe_training_job(TrainingJobName=training_job_name)\n",
    "print(\"Job Status:\", tj[\"TrainingJobStatus\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "5bf618d6-efc6-4b9c-be82-dbf034a77870",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating model with name: xgb-v1-monitor-1771384069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model artifact: s3://sagemaker-us-east-1-587322031938/modeling/xgb_v1/output/sagemaker-xgboost-2026-02-18-02-57-54-957/output/model.tar.gz\n",
      "Created model: xgb-v1-monitor-1771384069\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "tj = sm.describe_training_job(TrainingJobName=training_job_name)\n",
    "model_data = tj[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "print(\"Model artifact:\", model_data)\n",
    "\n",
    "xgb_image = retrieve(framework=\"xgboost\", region=region, version=\"1.7-1\")\n",
    "\n",
    "model_name = f\"xgb-v1-monitor-{int(time.time())}\"\n",
    "model = Model(\n",
    "    image_uri=xgb_image,\n",
    "    model_data=model_data,\n",
    "    role=role,\n",
    "    sagemaker_session=sess,\n",
    "    name=model_name,\n",
    ")\n",
    "\n",
    "model.create(instance_type=\"ml.m5.large\")\n",
    "print(\"Created model:\", model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "6098aa9d-7856-463d-a2aa-a1ce178d0c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker sdk version: 2.245.0\n",
      "Transformer.transform signature:\n",
      " (self, data: Union[str, sagemaker.workflow.entities.PipelineVariable], data_type: Union[str, sagemaker.workflow.entities.PipelineVariable] = 'S3Prefix', content_type: Union[str, sagemaker.workflow.entities.PipelineVariable, NoneType] = None, compression_type: Union[str, sagemaker.workflow.entities.PipelineVariable, NoneType] = None, split_type: Union[str, sagemaker.workflow.entities.PipelineVariable, NoneType] = None, job_name: Optional[str] = None, input_filter: Union[str, sagemaker.workflow.entities.PipelineVariable, NoneType] = None, output_filter: Union[str, sagemaker.workflow.entities.PipelineVariable, NoneType] = None, join_source: Union[str, sagemaker.workflow.entities.PipelineVariable, NoneType] = None, experiment_config: Optional[Dict[str, str]] = None, model_client_config: Optional[Dict[str, Union[str, sagemaker.workflow.entities.PipelineVariable]]] = None, batch_data_capture_config: sagemaker.inputs.BatchDataCaptureConfig = None, wait: bool = True, logs: bool = True)\n"
     ]
    }
   ],
   "source": [
    "#2A\n",
    "import sagemaker, inspect\n",
    "from sagemaker.transformer import Transformer\n",
    "\n",
    "print(\"sagemaker sdk version:\", sagemaker.__version__)\n",
    "print(\"Transformer.transform signature:\\n\", inspect.signature(Transformer.transform))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "866e0ea7-413c-40ac-a676-1ea404585110",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating transform job with name: sagemaker-xgboost-2026-02-18-03-07-50-815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch transform WITH batch data capture...\n",
      "................................\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:13:10:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:13:10:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:13:10:INFO] nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2026-02-18 03:13:10 +0000] [14] [INFO] Starting gunicorn 23.0.0\u001b[0m\n",
      "\u001b[34m[2026-02-18 03:13:10 +0000] [14] [INFO] Listening at: unix:/tmp/gunicorn.sock (14)\u001b[0m\n",
      "\u001b[34m[2026-02-18 03:13:10 +0000] [14] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2026-02-18 03:13:10 +0000] [17] [INFO] Booting worker with pid: 17\u001b[0m\n",
      "\u001b[34m[2026-02-18 03:13:10 +0000] [18] [INFO] Booting worker with pid: 18\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:13:12:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:13:12:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:13:12:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:13:12:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:13:12:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:13:12:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\n",
      "\u001b[34m[2026-02-18:03:13:19:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [18/Feb/2026:03:13:19 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:13:19:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [18/Feb/2026:03:13:19 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2026-02-18:03:13:20:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [18/Feb/2026:03:13:20 +0000] \"POST /invocations HTTP/1.1\" 200 200088 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2026-02-18:03:13:19:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [18/Feb/2026:03:13:19 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2026-02-18:03:13:19:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [18/Feb/2026:03:13:19 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2026-02-18:03:13:20:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [18/Feb/2026:03:13:20 +0000] \"POST /invocations HTTP/1.1\" 200 200088 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2026-02-18T03:13:19.752:[sagemaker logs]: MaxConcurrentTransforms=2, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "Transform complete\n",
      "Transform output: s3://sagemaker-us-east-1-587322031938/monitoring/batch-transform/output/1771384070/\n",
      "Captured data: s3://sagemaker-us-east-1-587322031938/monitoring/batch-transform/capture/1771384070/\n"
     ]
    }
   ],
   "source": [
    "# M5-2B Batch Transform with Batch Data Capture (SDK 2.245.0)\n",
    "\n",
    "from sagemaker.transformer import Transformer\n",
    "from sagemaker.inputs import BatchDataCaptureConfig\n",
    "\n",
    "transform_output = f\"s3://{bucket}/monitoring/batch-transform/output/{int(time.time())}/\"\n",
    "capture_uri = f\"s3://{bucket}/monitoring/batch-transform/capture/{int(time.time())}/\"\n",
    "\n",
    "batch_capture = BatchDataCaptureConfig(\n",
    "    destination_s3_uri=capture_uri,\n",
    "    generate_inference_id=True,\n",
    ")\n",
    "\n",
    "transformer = Transformer(\n",
    "    model_name=model_name,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    output_path=transform_output,\n",
    "    assemble_with=\"Line\",\n",
    "    accept=\"text/csv\",\n",
    ")\n",
    "\n",
    "prod_infer_uri = test_infer_csv_uri\n",
    "\n",
    "print(\"Starting batch transform WITH batch data capture...\")\n",
    "transformer.transform(\n",
    "    data=prod_infer_uri,\n",
    "    content_type=\"text/csv\",\n",
    "    split_type=\"Line\",\n",
    "    batch_data_capture_config=batch_capture,  # <-- correct name for your SDK\n",
    "    wait=True,\n",
    "    logs=True,\n",
    ")\n",
    "\n",
    "print(\"Transform complete\")\n",
    "print(\"Transform output:\", transform_output)\n",
    "print(\"Captured data:\", capture_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "8ea95b7f-6293-46da-bbcc-5cbd96769a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline CSV: s3://sagemaker-us-east-1-587322031938/monitoring/baselines/data_quality/train_baseline.csv shape: (39776, 8)\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "# M5-3.0 Data quality baseline dataset (TRAIN features)\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "split_base = f\"s3://{bucket}/splits/olist/features/version=v1/\"\n",
    "train = wr.s3.read_parquet(f\"{split_base}train/\")\n",
    "\n",
    "train_baseline = train[FEATURES].copy()\n",
    "for c in FEATURES:\n",
    "    train_baseline[c] = pd.to_numeric(train_baseline[c], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "baseline_uri = f\"s3://{bucket}/monitoring/baselines/data_quality/train_baseline.csv\"\n",
    "wr.s3.to_csv(train_baseline, baseline_uri, index=False, header=False)\n",
    "\n",
    "print(\"Baseline CSV:\", baseline_uri, \"shape:\", train_baseline.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "098a20c9-9cb5-4254-a550-1740a46889bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating processing-job with name baseline-suggestion-job-2026-02-18-03-13-45-105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......................\u001b[34m2026-02-18 03:17:23.256439: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:23.256488: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:24.856116: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:24.856156: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:24.856185: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-2-248-192.ec2.internal): /proc/driver/nvidia/version does not exist\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:24.856497: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\u001b[0m\n",
      "\u001b[34mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:26,451 - __main__ - INFO - All params:{'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:587322031938:processing-job/baseline-suggestion-job-2026-02-18-03-13-45-105', 'ProcessingJobName': 'baseline-suggestion-job-2026-02-18-03-13-45-105', 'Environment': {'dataset_format': '{\"csv\": {\"header\": false, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}, 'AppSpecification': {'ImageUri': '156813124566.dkr.ecr.us-east-1.amazonaws.com/sagemaker-model-monitor-analyzer', 'ContainerEntrypoint': None, 'ContainerArguments': None}, 'ProcessingInputs': [{'InputName': 'baseline_dataset_input', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3Uri': 's3://sagemaker-us-east-1-587322031938/monitoring/baselines/data_quality/train_baseline.csv', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinitionInput': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'monitoring_output', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/output', 'S3Uri': 's3://sagemaker-us-east-1-587322031938/monitoring/baselines/data_quality/results/', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.m5.large', 'VolumeSizeInGB': 20, 'VolumeKmsKeyId': None}}, 'NetworkConfig': {'VpcConfig': None, 'EnableNetworkIsolation': False, 'EnableInterContainerTrafficEncryption': False}, 'RoleArn': 'arn:aws:iam::587322031938:role/LabRole', 'StoppingCondition': {'MaxRuntimeInSeconds': 3600}}\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:26,452 - __main__ - INFO - Current Environment:{'dataset_format': '{\"csv\": {\"header\": false, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:26,452 - __main__ - INFO - categorical_drift_method:None\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:26,452 - DefaultDataAnalyzer - INFO - Performing analysis with input: {\"dataset_source\": \"/opt/ml/processing/input/baseline_dataset_input\", \"dataset_format\": {\"csv\": {\"header\": false, \"output_columns_position\": \"START\"}}, \"output_path\": \"/opt/ml/processing/output\", \"monitoring_input_type\": null, \"analysis_type\": null, \"problem_type\": null, \"inference_attribute\": null, \"probability_attribute\": null, \"ground_truth_attribute\": null, \"probability_threshold_attribute\": null, \"positive_label\": null, \"exclude_features_attribute\": null, \"record_preprocessor_script\": null, \"post_analytics_processor_script\": null, \"baseline_constraints\": null, \"baseline_statistics\": null, \"data_quality_monitoring_config\": {\"evaluate_constraints\": \"Enabled\", \"emit_metrics\": \"Enabled\", \"datatype_check_threshold\": 1.0, \"domain_content_threshold\": 1.0, \"distribution_constraints\": {\"perform_comparison\": \"Enabled\", \"comparison_threshold\": 0.1, \"comparison_method\": \"Robust\", \"categorical_comparison_threshold\": 0.1, \"categorical_drift_method\": \"LInfinity\"}}, \"start_time\": null, \"end_time\": null, \"metric_time\": null, \"cloudwatch_metrics_directory\": \"/opt/ml/output/metrics/cloudwatch\", \"publish_cloudwatch_metrics\": \"Disabled\", \"sagemaker_endpoint_name\": null, \"sagemaker_monitoring_schedule_name\": null, \"output_message_file\": \"/opt/ml/output/message\", \"detect_outliers\": null, \"detect_drift\": null, \"image_data\": null, \"report_enabled\": false, \"auto_ml_job_detail\": null}\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:26,452 - DefaultDataAnalyzer - INFO - Bootstrapping yarn\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:26,452 - bootstrap - INFO - Copy aws jars\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:26,678 - bootstrap - INFO - Copy cluster config\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:26,679 - bootstrap - INFO - Write runtime cluster config\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:26,679 - bootstrap - INFO - Resource Config is: {'current_host': 'algo-1', 'current_instance_type': 'ml.m5.large', 'current_group_name': 'homogeneousCluster', 'hosts': ['algo-1'], 'instance_groups': [{'instance_group_name': 'homogeneousCluster', 'instance_type': 'ml.m5.large', 'hosts': ['algo-1']}], 'network_interface_name': 'eth0', 'topology': None}\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:26,690 - bootstrap - INFO - Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:26,690 - bootstrap - INFO - Starting spark process for master node algo-1\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:26,690 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs namenode -format -force\u001b[0m\n",
      "\u001b[34mWARNING: /usr/hadoop-3.0.0/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:28,127 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.2.248.192\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.0.0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.0.0/etc/hadoop:/usr/hadoop-3.0.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/junit-4.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.199.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-aws-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-kms-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-csv-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jamon-runtime-2.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-client-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-el-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-annotations-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop2-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-common-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/joni-2.1.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/disruptor-3.3.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-prefix-tree-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-2.2.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-procedure-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/servlet-api-2.5-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jcodings-1.0.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/findbugs-annotations-1.3.9-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/htrace-core-3.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-server-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-protocol-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-api-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-runtime-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-compiler-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-tests-3.0.0.jar:/usr/hadoop-3\u001b[0m\n",
      "\u001b[34m.0.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-router-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-api-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-registry-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.0.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533; compiled by 'andrew' on 2017-12-08T19:16Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_462\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:28,157 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:28,163 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-025681a5-c969-4a79-a035-2a24b0a880d2\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,103 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,125 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,126 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,130 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,147 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,147 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,147 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,148 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,208 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,230 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,230 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,235 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,239 INFO blockmanagement.BlockManager: The block deletion will start around 2026 Feb 18 03:17:29\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,241 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,241 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,244 INFO util.GSet: 2.0% max memory 1.4 GB = 28.0 MB\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,244 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,278 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,282 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,282 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,282 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,282 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,282 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,283 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,283 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,283 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,283 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,283 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,283 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,348 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,348 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,348 INFO util.GSet: 1.0% max memory 1.4 GB = 14.0 MB\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,348 INFO util.GSet: capacity      = 2^21 = 2097152 entries\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,350 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,350 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,350 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,350 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,355 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,360 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,360 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,360 INFO util.GSet: 0.25% max memory 1.4 GB = 3.5 MB\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,360 INFO util.GSet: capacity      = 2^19 = 524288 entries\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,369 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,369 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,369 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,373 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,373 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,376 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,376 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,376 INFO util.GSet: 0.029999999329447746% max memory 1.4 GB = 430.4 KB\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,376 INFO util.GSet: capacity      = 2^16 = 65536 entries\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,408 INFO namenode.FSImage: Allocated new BlockPoolId: BP-880740458-10.2.248.192-1771384649398\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,430 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,447 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,560 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 386 bytes saved in 0 seconds.\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,581 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,589 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.2.248.192\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:29,598 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:31,678 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode, return code 1\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:31,678 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:33,890 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode, return code 1\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:33,891 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:36,150 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager, return code 1\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:36,150 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:38,715 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager, return code 1\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:38,715 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:41,240 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver, return code 1\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:41,240 - DefaultDataAnalyzer - INFO - Total number of hosts in the cluster: 1\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:51,247 - DefaultDataAnalyzer - INFO - Running command: bin/spark-submit --master yarn --deploy-mode client --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.serializer=org.apache.spark.serializer.KryoSerializer /opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:54,001 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:54,762 INFO Main: Start analyzing with args: --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:54,838 INFO Main: Analytics input path: DataAnalyzerParams(/tmp/spark_job_config.json,yarn)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:54,878 INFO FileUtil: Read file from path /tmp/spark_job_config.json.\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:56,018 INFO spark.SparkContext: Running Spark version 3.3.0\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:56,051 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:56,052 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:56,053 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:56,054 INFO spark.SparkContext: Submitted application: SageMakerDataAnalyzer\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:56,101 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 5611, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:56,116 INFO resource.ResourceProfile: Limiting resource is cpus at 1 tasks per executor\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:56,118 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:56,208 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:56,209 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:56,209 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:56,210 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:56,210 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:56,841 INFO util.Utils: Successfully started service 'sparkDriver' on port 41431.\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:56,911 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:56,979 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:57,012 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:57,013 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:57,063 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:57,114 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-efecbdc9-1d28-4f32-abaa-f632f494b86d\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:57,141 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MiB\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:57,201 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:57,257 INFO spark.SparkContext: Added JAR file:/opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar at spark://10.2.248.192:41431/jars/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar with timestamp 1771384676011\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:58,260 INFO client.RMProxy: Connecting to ResourceManager at /10.2.248.192:8032\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:59,362 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:59,363 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:59,370 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (7652 MB per container)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:59,370 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:59,371 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:59,371 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:59,378 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2026-02-18 03:17:59,472 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:01,925 INFO yarn.Client: Uploading resource file:/tmp/spark-6170083d-2762-47e6-969e-64ca34a9137c/__spark_libs__1519251104967560302.zip -> hdfs://10.2.248.192/user/root/.sparkStaging/application_1771384658318_0001/__spark_libs__1519251104967560302.zip\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:04,473 INFO yarn.Client: Uploading resource file:/tmp/spark-6170083d-2762-47e6-969e-64ca34a9137c/__spark_conf__3889468148811499797.zip -> hdfs://10.2.248.192/user/root/.sparkStaging/application_1771384658318_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:04,596 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:04,598 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:04,599 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:04,600 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:04,602 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:04,691 INFO yarn.Client: Submitting application application_1771384658318_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:05,047 INFO impl.YarnClientImpl: Submitted application application_1771384658318_0001\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:06,052 INFO yarn.Client: Application report for application_1771384658318_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:06,057 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: [Wed Feb 18 03:18:05 +0000 2026] Scheduler has assigned a container for AM, waiting for AM container to be launched\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1771384684870\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1771384658318_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:07,061 INFO yarn.Client: Application report for application_1771384658318_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:08,073 INFO yarn.Client: Application report for application_1771384658318_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:09,078 INFO yarn.Client: Application report for application_1771384658318_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:10,087 INFO yarn.Client: Application report for application_1771384658318_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:11,092 INFO yarn.Client: Application report for application_1771384658318_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:12,106 INFO yarn.Client: Application report for application_1771384658318_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:13,114 INFO yarn.Client: Application report for application_1771384658318_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:13,119 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.2.248.192\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1771384684870\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1771384658318_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:13,122 INFO cluster.YarnClientSchedulerBackend: Application application_1771384658318_0001 has started running.\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:13,179 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40951.\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:13,179 INFO netty.NettyBlockTransferService: Server created on 10.2.248.192:40951\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:13,183 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:13,213 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.2.248.192, 40951, None)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:13,229 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.2.248.192:40951 with 1458.6 MiB RAM, BlockManagerId(driver, 10.2.248.192, 40951, None)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:13,237 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.2.248.192, 40951, None)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:13,250 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.2.248.192, 40951, None)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:13,577 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1771384658318_0001), /proxy/application_1771384658318_0001\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:13,670 INFO util.log: Logging initialized @22071ms to org.sparkproject.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:15,771 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:21,741 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.2.248.192:44048) with ID 1,  ResourceProfileId 0\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:22,024 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:42615 with 2.7 GiB RAM, BlockManagerId(1, algo-1, 42615, None)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:27,945 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:28,188 WARN spark.SparkContext: Spark is not running in local mode, therefore the checkpoint directory must not be on the local filesystem. Directory '/tmp' appears to be on the local filesystem.\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:28,257 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:28,263 INFO internal.SharedState: Warehouse path is 'file:/usr/spark-3.3.0/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:30,806 INFO datasources.InMemoryFileIndex: It took 235 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:31,559 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 416.9 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:32,079 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 39.2 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:32,083 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.2.248.192:40951 (size: 39.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:32,089 INFO spark.SparkContext: Created broadcast 0 from csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:32,868 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:32,873 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:32,878 INFO input.CombineFileInputFormat: DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 1154874\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:33,018 INFO spark.SparkContext: Starting job: csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:33,058 INFO scheduler.DAGScheduler: Got job 0 (csv at DatasetReader.scala:99) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:33,058 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (csv at DatasetReader.scala:99)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:33,059 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:33,061 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:33,069 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:33,163 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.3 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:33,170 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:33,174 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.2.248.192:40951 (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:33,177 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:33,205 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:33,210 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:33,309 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4627 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:33,728 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-1:42615 (size: 4.2 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:34,843 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-1:42615 (size: 39.2 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:35,402 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2128 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:35,412 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:35,423 INFO scheduler.DAGScheduler: ResultStage 0 (csv at DatasetReader.scala:99) finished in 2.309 s\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:35,432 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:35,433 INFO cluster.YarnScheduler: Killing all running tasks in stage 0: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:35,434 INFO scheduler.DAGScheduler: Job 0 finished: csv at DatasetReader.scala:99, took 2.415707 s\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:35,699 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.2.248.192:40951 in memory (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:35,744 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-1:42615 in memory (size: 4.2 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:35,842 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 10.2.248.192:40951 in memory (size: 39.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:35,857 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on algo-1:42615 in memory (size: 39.2 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:39,213 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:39,215 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:39,219 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: string, _c1: string, _c2: string, _c3: string, _c4: string ... 6 more fields>\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:39,732 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 416.5 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:39,770 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 39.1 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:39,772 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.2.248.192:40951 (size: 39.1 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:39,773 INFO spark.SparkContext: Created broadcast 2 from head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:39,824 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:39,971 INFO spark.SparkContext: Starting job: head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:39,973 INFO scheduler.DAGScheduler: Got job 1 (head at DataAnalyzer.scala:124) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:39,974 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (head at DataAnalyzer.scala:124)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:39,974 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:39,978 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:39,981 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:40,108 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 16.1 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:40,190 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:40,191 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.2.248.192:40951 (size: 7.7 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:40,194 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:40,195 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:40,195 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:40,202 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4955 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:40,310 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-1:42615 (size: 7.7 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:41,639 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-1:42615 (size: 39.1 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:42,389 INFO storage.BlockManagerInfo: Added rdd_7_0 in memory on algo-1:42615 (size: 829.5 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:42,574 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2375 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:42,575 INFO scheduler.DAGScheduler: ResultStage 1 (head at DataAnalyzer.scala:124) finished in 2.583 s\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:42,576 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:42,577 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:42,577 INFO cluster.YarnScheduler: Killing all running tasks in stage 1: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:42,578 INFO scheduler.DAGScheduler: Job 1 finished: head at DataAnalyzer.scala:124, took 2.606124 s\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:43,068 INFO codegen.CodeGenerator: Code generated in 342.443145 ms\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:44,143 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:44,345 INFO scheduler.DAGScheduler: Registering RDD 16 (collect at AnalysisRunner.scala:326) as input to shuffle 0\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:44,349 INFO scheduler.DAGScheduler: Got map stage job 2 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:44,352 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:44,353 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:44,355 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:44,361 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:44,400 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 113.1 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:44,404 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:44,405 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.2.248.192:40951 (size: 34.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:44,407 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:44,410 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:44,410 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:44,419 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4944 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:44,453 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-1:42615 (size: 34.7 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:46,719 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 2302 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:46,722 INFO scheduler.DAGScheduler: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326) finished in 2.356 s\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:46,722 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:46,723 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:46,723 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:46,723 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:46,727 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:46,889 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:46,892 INFO scheduler.DAGScheduler: Got job 3 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:46,892 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:46,892 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:46,895 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:46,898 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:46,927 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 165.6 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:46,932 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 45.7 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:46,933 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.2.248.192:40951 (size: 45.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:46,937 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:46,938 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:46,938 INFO cluster.YarnScheduler: Adding task set 4.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:46,942 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:46,981 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-1:42615 (size: 45.7 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:47,111 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.2.248.192:44048\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:47,600 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 659 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:47,600 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:47,601 INFO scheduler.DAGScheduler: ResultStage 4 (collect at AnalysisRunner.scala:326) finished in 0.693 s\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:47,604 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:47,604 INFO cluster.YarnScheduler: Killing all running tasks in stage 4: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:47,605 INFO scheduler.DAGScheduler: Job 3 finished: collect at AnalysisRunner.scala:326, took 0.714986 s\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:47,670 INFO codegen.CodeGenerator: Code generated in 54.765498 ms\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:48,005 INFO codegen.CodeGenerator: Code generated in 44.159525 ms\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:48,095 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:48,097 INFO scheduler.DAGScheduler: Got job 4 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:48,097 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:48,097 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:48,098 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:48,099 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:48,132 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 36.9 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:48,138 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 16.1 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:48,139 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.2.248.192:40951 (size: 16.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:48,142 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:48,145 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:48,149 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:48,151 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4955 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:48,167 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-1:42615 (size: 16.1 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:49,869 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 1719 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:49,869 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:49,871 INFO scheduler.DAGScheduler: ResultStage 5 (treeReduce at KLLRunner.scala:107) finished in 1.770 s\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:49,876 INFO scheduler.DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:49,877 INFO cluster.YarnScheduler: Killing all running tasks in stage 5: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:49,878 INFO scheduler.DAGScheduler: Job 4 finished: treeReduce at KLLRunner.scala:107, took 1.782156 s\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:50,710 INFO codegen.CodeGenerator: Code generated in 202.840204 ms\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:50,723 INFO scheduler.DAGScheduler: Registering RDD 34 (collect at AnalysisRunner.scala:326) as input to shuffle 1\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:50,724 INFO scheduler.DAGScheduler: Got map stage job 5 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:50,724 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:50,724 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:50,725 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:50,795 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:50,807 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.2.248.192:40951 in memory (size: 7.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:50,808 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-1:42615 in memory (size: 7.7 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:50,817 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 73.3 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:50,820 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 23.8 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:50,824 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.2.248.192:40951 (size: 23.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:50,825 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:50,826 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:50,827 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:50,829 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4944 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:50,851 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-1:42615 (size: 23.8 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:50,871 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.2.248.192:40951 in memory (size: 34.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:50,883 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-1:42615 in memory (size: 34.7 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:50,937 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.2.248.192:40951 in memory (size: 16.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:50,939 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-1:42615 in memory (size: 16.1 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,022 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.2.248.192:40951 in memory (size: 45.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,023 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-1:42615 in memory (size: 45.7 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,183 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 354 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,184 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,186 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326) finished in 0.388 s\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,187 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,187 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,188 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,188 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,379 INFO codegen.CodeGenerator: Code generated in 85.044148 ms\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,403 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,405 INFO scheduler.DAGScheduler: Got job 6 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,405 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,405 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,406 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,407 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,412 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 66.8 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,414 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 19.6 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,416 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.2.248.192:40951 (size: 19.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,417 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,418 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,418 INFO cluster.YarnScheduler: Adding task set 8.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,420 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,435 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-1:42615 (size: 19.6 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,458 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.2.248.192:44048\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,614 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 195 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,616 INFO scheduler.DAGScheduler: ResultStage 8 (collect at AnalysisRunner.scala:326) finished in 0.205 s\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,617 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,617 INFO scheduler.DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,618 INFO cluster.YarnScheduler: Killing all running tasks in stage 8: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,619 INFO scheduler.DAGScheduler: Job 6 finished: collect at AnalysisRunner.scala:326, took 0.214975 s\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,743 INFO codegen.CodeGenerator: Code generated in 99.24848 ms\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,923 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,928 INFO scheduler.DAGScheduler: Registering RDD 45 (countByKey at ColumnProfiler.scala:592) as input to shuffle 2\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,929 INFO scheduler.DAGScheduler: Got job 7 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,929 INFO scheduler.DAGScheduler: Final stage: ResultStage 10 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,929 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,930 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,934 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,947 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 29.3 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,951 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 13.6 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,962 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.2.248.192:40951 (size: 13.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,963 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,964 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,964 INFO cluster.YarnScheduler: Adding task set 9.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,966 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4944 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:51,987 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-1:42615 (size: 13.6 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,006 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 2040 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,009 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,010 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (countByKey at ColumnProfiler.scala:592) finished in 2.075 s\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,013 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,014 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,015 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 10)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,015 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,016 INFO scheduler.DAGScheduler: Submitting ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,018 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 5.1 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,028 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,029 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.2.248.192:40951 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,030 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,031 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,031 INFO cluster.YarnScheduler: Adding task set 10.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,033 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 8) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,055 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-1:42615 (size: 3.0 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,075 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.2.248.192:44048\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,127 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 8) in 95 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,130 INFO cluster.YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,132 INFO scheduler.DAGScheduler: ResultStage 10 (countByKey at ColumnProfiler.scala:592) finished in 0.114 s\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,133 INFO scheduler.DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,133 INFO cluster.YarnScheduler: Killing all running tasks in stage 10: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,134 INFO scheduler.DAGScheduler: Job 7 finished: countByKey at ColumnProfiler.scala:592, took 2.211191 s\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,321 INFO scheduler.DAGScheduler: Registering RDD 51 (collect at AnalysisRunner.scala:326) as input to shuffle 3\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,322 INFO scheduler.DAGScheduler: Got map stage job 8 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,322 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,322 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,323 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,325 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,331 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 61.6 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,333 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 22.0 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,333 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.2.248.192:40951 (size: 22.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,337 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,338 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,339 INFO cluster.YarnScheduler: Adding task set 11.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,340 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 9) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4944 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,352 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-1:42615 (size: 22.0 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,828 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 9) in 488 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,829 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,830 INFO scheduler.DAGScheduler: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326) finished in 0.504 s\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,830 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,830 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,831 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,831 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,883 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,888 INFO scheduler.DAGScheduler: Got job 9 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,888 INFO scheduler.DAGScheduler: Final stage: ResultStage 13 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,888 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,889 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,890 INFO scheduler.DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,908 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 113.9 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,920 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,921 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.2.248.192:40951 (size: 34.6 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,921 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,922 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,922 INFO cluster.YarnScheduler: Adding task set 13.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,928 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 10) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,939 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-1:42615 (size: 34.6 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:54,959 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 10.2.248.192:44048\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:55,098 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 10) in 171 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:55,100 INFO cluster.YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:55,101 INFO scheduler.DAGScheduler: ResultStage 13 (collect at AnalysisRunner.scala:326) finished in 0.209 s\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:55,103 INFO scheduler.DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:55,103 INFO cluster.YarnScheduler: Killing all running tasks in stage 13: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:55,103 INFO scheduler.DAGScheduler: Job 9 finished: collect at AnalysisRunner.scala:326, took 0.216693 s\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:55,146 INFO codegen.CodeGenerator: Code generated in 34.57342 ms\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:55,277 INFO codegen.CodeGenerator: Code generated in 10.491743 ms\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:55,310 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:55,312 INFO scheduler.DAGScheduler: Got job 10 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:55,312 INFO scheduler.DAGScheduler: Final stage: ResultStage 14 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:55,312 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:55,313 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:55,314 INFO scheduler.DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:55,325 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 34.5 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:55,327 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 15.3 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:55,327 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.2.248.192:40951 (size: 15.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:55,328 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:55,328 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:55,328 INFO cluster.YarnScheduler: Adding task set 14.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:55,330 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 11) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4955 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:55,341 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-1:42615 (size: 15.3 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,020 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 11) in 690 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,021 INFO scheduler.DAGScheduler: ResultStage 14 (treeReduce at KLLRunner.scala:107) finished in 0.706 s\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,022 INFO cluster.YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,022 INFO scheduler.DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,022 INFO cluster.YarnScheduler: Killing all running tasks in stage 14: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,023 INFO scheduler.DAGScheduler: Job 10 finished: treeReduce at KLLRunner.scala:107, took 0.712500 s\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,342 INFO codegen.CodeGenerator: Code generated in 59.641925 ms\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,350 INFO scheduler.DAGScheduler: Registering RDD 69 (collect at AnalysisRunner.scala:326) as input to shuffle 4\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,350 INFO scheduler.DAGScheduler: Got map stage job 11 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,350 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,351 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,352 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,352 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,356 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 53.1 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,358 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 18.8 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,359 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.2.248.192:40951 (size: 18.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,360 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,360 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,360 INFO cluster.YarnScheduler: Adding task set 15.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,362 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 12) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4944 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,374 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-1:42615 (size: 18.8 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,513 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 12) in 151 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,513 INFO cluster.YarnScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,514 INFO scheduler.DAGScheduler: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326) finished in 0.161 s\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,515 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,515 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,515 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,515 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,610 INFO codegen.CodeGenerator: Code generated in 51.515678 ms\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,639 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,640 INFO scheduler.DAGScheduler: Got job 12 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,641 INFO scheduler.DAGScheduler: Final stage: ResultStage 17 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,641 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,641 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,641 INFO scheduler.DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,644 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 44.7 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,650 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 14.3 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,650 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.2.248.192:40951 (size: 14.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,651 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,652 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,652 INFO cluster.YarnScheduler: Adding task set 17.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,654 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 13) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,668 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-1:42615 (size: 14.3 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,678 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.2.248.192:44048\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,732 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 13) in 78 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,732 INFO cluster.YarnScheduler: Removed TaskSet 17.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,735 INFO scheduler.DAGScheduler: ResultStage 17 (collect at AnalysisRunner.scala:326) finished in 0.091 s\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,735 INFO scheduler.DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,736 INFO cluster.YarnScheduler: Killing all running tasks in stage 17: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,737 INFO scheduler.DAGScheduler: Job 12 finished: collect at AnalysisRunner.scala:326, took 0.097484 s\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,851 INFO codegen.CodeGenerator: Code generated in 96.666687 ms\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,935 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,936 INFO scheduler.DAGScheduler: Registering RDD 80 (countByKey at ColumnProfiler.scala:592) as input to shuffle 5\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,937 INFO scheduler.DAGScheduler: Got job 13 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,937 INFO scheduler.DAGScheduler: Final stage: ResultStage 19 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,937 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,937 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,941 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,949 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 29.2 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,951 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 13.6 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,953 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.2.248.192:40951 (size: 13.6 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,953 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,955 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,956 INFO cluster.YarnScheduler: Adding task set 18.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,957 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 14) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4944 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:56,969 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-1:42615 (size: 13.6 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,288 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 14) in 331 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,289 INFO cluster.YarnScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,290 INFO scheduler.DAGScheduler: ShuffleMapStage 18 (countByKey at ColumnProfiler.scala:592) finished in 0.348 s\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,294 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,294 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,294 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 19)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,295 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,295 INFO scheduler.DAGScheduler: Submitting ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,297 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 5.1 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,301 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,304 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.2.248.192:40951 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,306 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,307 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,307 INFO cluster.YarnScheduler: Adding task set 19.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,310 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 19.0 (TID 15) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,323 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-1:42615 (size: 3.0 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,330 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 10.2.248.192:44048\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,363 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 19.0 (TID 15) in 53 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,363 INFO cluster.YarnScheduler: Removed TaskSet 19.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,364 INFO scheduler.DAGScheduler: ResultStage 19 (countByKey at ColumnProfiler.scala:592) finished in 0.068 s\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,367 INFO scheduler.DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,370 INFO cluster.YarnScheduler: Killing all running tasks in stage 19: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,371 INFO scheduler.DAGScheduler: Job 13 finished: countByKey at ColumnProfiler.scala:592, took 0.435988 s\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,724 INFO FileUtil: Write to file constraints.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,769 INFO codegen.CodeGenerator: Code generated in 12.462884 ms\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,777 INFO scheduler.DAGScheduler: Registering RDD 86 (count at StatsGenerator.scala:66) as input to shuffle 6\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,778 INFO scheduler.DAGScheduler: Got map stage job 14 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,778 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 20 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,778 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,780 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,783 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[86] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,789 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 21.3 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,794 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 10.0 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,801 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.2.248.192:40951 (size: 10.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,802 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,803 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[86] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,803 INFO cluster.YarnScheduler: Adding task set 20.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,805 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 16) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4944 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,818 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-1:42615 (size: 10.0 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,909 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 16) in 104 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,909 INFO cluster.YarnScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,910 INFO scheduler.DAGScheduler: ShuffleMapStage 20 (count at StatsGenerator.scala:66) finished in 0.125 s\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,912 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,912 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,912 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,913 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,979 INFO codegen.CodeGenerator: Code generated in 42.544183 ms\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,998 INFO spark.SparkContext: Starting job: count at StatsGenerator.scala:66\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:57,999 INFO scheduler.DAGScheduler: Got job 15 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,000 INFO scheduler.DAGScheduler: Final stage: ResultStage 22 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,001 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,001 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,002 INFO scheduler.DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[89] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,005 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 11.1 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,010 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,011 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.2.248.192:40951 (size: 5.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,012 INFO spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,013 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[89] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,014 INFO cluster.YarnScheduler: Adding task set 22.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,015 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 22.0 (TID 17) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,027 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-1:42615 (size: 5.5 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,032 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.2.248.192:44048\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,101 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 22.0 (TID 17) in 86 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,102 INFO cluster.YarnScheduler: Removed TaskSet 22.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,103 INFO scheduler.DAGScheduler: ResultStage 22 (count at StatsGenerator.scala:66) finished in 0.099 s\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,104 INFO scheduler.DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,104 INFO cluster.YarnScheduler: Killing all running tasks in stage 22: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,105 INFO scheduler.DAGScheduler: Job 15 finished: count at StatsGenerator.scala:66, took 0.106458 s\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,137 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on algo-1:42615 in memory (size: 13.6 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,161 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.2.248.192:40951 in memory (size: 13.6 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,211 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-1:42615 in memory (size: 22.0 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,214 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.2.248.192:40951 in memory (size: 22.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,233 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on algo-1:42615 in memory (size: 14.3 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,241 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.2.248.192:40951 in memory (size: 14.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,327 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on algo-1:42615 in memory (size: 15.3 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,328 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.2.248.192:40951 in memory (size: 15.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,350 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on 10.2.248.192:40951 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,357 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on algo-1:42615 in memory (size: 3.0 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,408 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-1:42615 in memory (size: 23.8 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,414 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.2.248.192:40951 in memory (size: 23.8 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,434 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-1:42615 in memory (size: 3.0 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,436 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.2.248.192:40951 in memory (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,482 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on 10.2.248.192:40951 in memory (size: 10.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,483 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on algo-1:42615 in memory (size: 10.0 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,523 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.2.248.192:40951 in memory (size: 34.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,524 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-1:42615 in memory (size: 34.6 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,570 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.2.248.192:40951 in memory (size: 19.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,571 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-1:42615 in memory (size: 19.6 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,642 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.2.248.192:40951 in memory (size: 13.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,643 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-1:42615 in memory (size: 13.6 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,700 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.2.248.192:40951 in memory (size: 18.8 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,701 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on algo-1:42615 in memory (size: 18.8 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,748 INFO FileUtil: Write to file statistics.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,790 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,830 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,832 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,852 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,912 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,990 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,992 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:58,998 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:59,016 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:59,078 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:59,079 INFO Main: Completed: Job completed successfully with no violations.\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:59,079 INFO Main: Write to file /opt/ml/output/message.\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:59,111 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:59,112 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-6170083d-2762-47e6-969e-64ca34a9137c\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:59,132 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-48e3f63f-89a2-4999-8116-b01fe60c27d7\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:59,249 - DefaultDataAnalyzer - INFO - Completed spark-submit with return code : 0\u001b[0m\n",
      "\u001b[34m2026-02-18 03:18:59,250 - DefaultDataAnalyzer - INFO - Spark job completed.\u001b[0m\n",
      "\n",
      "Baseline results in: s3://sagemaker-us-east-1-587322031938/monitoring/baselines/data_quality/results/\n"
     ]
    }
   ],
   "source": [
    "# M5-3.1 Suggest baseline (stats + constraints)\n",
    "# NOTE (Feb 2026):\n",
    "# This cell follows older SageMaker examples that reference\n",
    "# `monitor.baseline_constraints()` and `monitor.baseline_statistics()`.\n",
    "# These methods are NOT available in SageMaker SDK v2.245.0.\n",
    "# \n",
    "# The baseline IS created successfully, but artifacts must be\n",
    "# retrieved directly from S3 instead.\n",
    "# \n",
    "# See M5-3.1b below for the correct, SDK-safe implementation.\n",
    "monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",  # budget-friendly\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    "    sagemaker_session=sess,\n",
    ")\n",
    "\n",
    "baseline_results_uri = f\"s3://{bucket}/monitoring/baselines/data_quality/results/\"\n",
    "monitor.suggest_baseline(\n",
    "    baseline_dataset=baseline_uri,\n",
    "    dataset_format=DatasetFormat.csv(header=False),\n",
    "    output_s3_uri=baseline_results_uri,\n",
    "    wait=True,\n",
    ")\n",
    "\n",
    "print(\"Baseline results in:\", baseline_results_uri)\n",
    "# print(\"Constraints:\", monitor.baseline_constraints())\n",
    "# print(\"Statistics:\", monitor.baseline_statistics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "6c7114eb-658c-44aa-9f89-b2f1c06c207b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found objects: 2\n",
      "monitoring/baselines/data_quality/results/constraints.json\n",
      "monitoring/baselines/data_quality/results/statistics.json\n",
      "\n",
      "statistics.json: monitoring/baselines/data_quality/results/statistics.json\n",
      "constraints.json: monitoring/baselines/data_quality/results/constraints.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('s3://sagemaker-us-east-1-587322031938/monitoring/baselines/data_quality/results/statistics.json',\n",
       " 's3://sagemaker-us-east-1-587322031938/monitoring/baselines/data_quality/results/constraints.json')"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# M5-3.1b Find baseline statistics + constraints files in S3\n",
    "\n",
    "s3 = boto3.client(\"s3\", region_name=region)\n",
    "\n",
    "baseline_prefix = baseline_results_uri.replace(f\"s3://{bucket}/\", \"\")\n",
    "resp = s3.list_objects_v2(Bucket=bucket, Prefix=baseline_prefix)\n",
    "\n",
    "keys = [o[\"Key\"] for o in resp.get(\"Contents\", [])]\n",
    "print(\"Found objects:\", len(keys))\n",
    "for k in keys[:50]:\n",
    "    print(k)\n",
    "\n",
    "# Try to auto-detect the files we need\n",
    "stats = [k for k in keys if k.endswith(\"statistics.json\")]\n",
    "constraints = [k for k in keys if k.endswith(\"constraints.json\")]\n",
    "\n",
    "print(\"\\nstatistics.json:\", stats[-1] if stats else \"NOT FOUND\")\n",
    "print(\"constraints.json:\", constraints[-1] if constraints else \"NOT FOUND\")\n",
    "\n",
    "baseline_statistics_uri = f\"s3://{bucket}/{stats[-1]}\" if stats else None\n",
    "baseline_constraints_uri = f\"s3://{bucket}/{constraints[-1]}\" if constraints else None\n",
    "\n",
    "baseline_statistics_uri, baseline_constraints_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "eaa9ca3d-52be-4125-a38b-95dd994fa4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Production inference CSV: s3://sagemaker-us-east-1-587322031938/modeling/xgb_v1/test/test_infer.csv\n",
      "prod_infer rows/cols: (9944, 8)\n",
      "Columns: ['num_items', 'total_price', 'total_freight_value', 'num_sellers', 'payment_value', 'payment_installments', 'purchase_dow', 'purchase_hour']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_items</th>\n",
       "      <th>total_price</th>\n",
       "      <th>total_freight_value</th>\n",
       "      <th>num_sellers</th>\n",
       "      <th>payment_value</th>\n",
       "      <th>payment_installments</th>\n",
       "      <th>purchase_dow</th>\n",
       "      <th>purchase_hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>29.99</td>\n",
       "      <td>14.10</td>\n",
       "      <td>1</td>\n",
       "      <td>44.09</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>107.00</td>\n",
       "      <td>12.25</td>\n",
       "      <td>1</td>\n",
       "      <td>119.25</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>69.90</td>\n",
       "      <td>13.08</td>\n",
       "      <td>1</td>\n",
       "      <td>82.98</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>75.07</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>25.80</td>\n",
       "      <td>15.56</td>\n",
       "      <td>1</td>\n",
       "      <td>41.36</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>39.99</td>\n",
       "      <td>15.79</td>\n",
       "      <td>1</td>\n",
       "      <td>61.68</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>56.50</td>\n",
       "      <td>15.15</td>\n",
       "      <td>1</td>\n",
       "      <td>71.65</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>334.90</td>\n",
       "      <td>17.09</td>\n",
       "      <td>1</td>\n",
       "      <td>351.99</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>34.99</td>\n",
       "      <td>15.10</td>\n",
       "      <td>1</td>\n",
       "      <td>50.09</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>199.90</td>\n",
       "      <td>14.51</td>\n",
       "      <td>1</td>\n",
       "      <td>214.41</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_items  total_price  total_freight_value  num_sellers  payment_value  \\\n",
       "0          1        29.99                14.10            1          44.09   \n",
       "1          1       107.00                12.25            1         119.25   \n",
       "2          1        69.90                13.08            1          82.98   \n",
       "3          0         0.00                 0.00            0          75.07   \n",
       "4          2        25.80                15.56            1          41.36   \n",
       "5          1        39.99                15.79            1          61.68   \n",
       "6          1        56.50                15.15            1          71.65   \n",
       "7          1       334.90                17.09            1         351.99   \n",
       "8          1        34.99                15.10            1          50.09   \n",
       "9          1       199.90                14.51            1         214.41   \n",
       "\n",
       "   payment_installments  purchase_dow  purchase_hour  \n",
       "0                     1             3             23  \n",
       "1                     3             3             23  \n",
       "2                     1             3             23  \n",
       "3                     5             3             23  \n",
       "4                     1             3             23  \n",
       "5                     6             3             23  \n",
       "6                     1             3             23  \n",
       "7                     8             3             23  \n",
       "8                     4             3             23  \n",
       "9                     6             3             23  "
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# M5-3.2a Verify production inference dataset (CSV)\n",
    "\n",
    "\n",
    "print(\"Production inference CSV:\", prod_infer_uri)\n",
    "\n",
    "prod_df = wr.s3.read_csv(prod_infer_uri, header=None)\n",
    "prod_df.columns = FEATURES  # we wrote it without headers\n",
    "\n",
    "print(\"prod_infer rows/cols:\", prod_df.shape)\n",
    "print(\"Columns:\", list(prod_df.columns))\n",
    "prod_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "4fab24b6-c761-4ad9-9cb8-bbced57c933a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using latest capture folder: s3://sagemaker-us-east-1-587322031938/monitoring/batch-transform/capture/1771384070/output/2026/02/18/03/e7ee07b6-2c9c-4d50-b7b6-21b631c9c553.json\n",
      "Capture objects found: 1\n",
      "monitoring/batch-transform/capture/1771384070/output/2026/02/18/03/e7ee07b6-2c9c-4d50-b7b6-21b631c9c553.json\n",
      "\n",
      "First capture object: monitoring/batch-transform/capture/1771384070/output/2026/02/18/03/e7ee07b6-2c9c-4d50-b7b6-21b631c9c553.json\n",
      "First line (raw): [{\"prefix\":\"s3://sagemaker-us-east-1-587322031938/monitoring/batch-transform/output/1771384070/\"},\"test_infer.csv.out\"] \n",
      "\n",
      "\u26a0\ufe0f Could not parse first line as JSON: 'list' object has no attribute 'keys'\n"
     ]
    }
   ],
   "source": [
    "# M5-3.2b Verify batch capture artifacts (JSONL)\n",
    "import boto3, json\n",
    "\n",
    "s3 = boto3.client(\"s3\", region_name=region)\n",
    "# Find the latest folder dynamically\n",
    "base_capture_path = f\"s3://{bucket}/monitoring/batch-transform/capture/\"\n",
    "folders = wr.s3.list_directories(base_capture_path)\n",
    "capture_uri = sorted(folders)[-1] # Pick the latest run\n",
    "print(f\"Using latest capture folder: {capture_uri}\")\n",
    "\n",
    "prefix = capture_uri.replace(f\"s3://{bucket}/\", \"\")\n",
    "\n",
    "resp = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "keys = [o[\"Key\"] for o in resp.get(\"Contents\", [])]\n",
    "\n",
    "print(\"Capture objects found:\", len(keys))\n",
    "for k in keys[:25]:\n",
    "    print(k)\n",
    "\n",
    "# Read the first non-empty object and show 1 parsed JSON record\n",
    "if not keys:\n",
    "    raise RuntimeError(\"No capture files found under: \" + capture_uri)\n",
    "\n",
    "k0 = sorted(keys)[0]\n",
    "obj = s3.get_object(Bucket=bucket, Key=k0)\n",
    "text = obj[\"Body\"].read().decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "first_line = next((ln for ln in text.splitlines() if ln.strip()), None)\n",
    "print(\"\\nFirst capture object:\", k0)\n",
    "print(\"First line (raw):\", first_line[:300], \"...\" if len(first_line) > 300 else \"\")\n",
    "\n",
    "# Try parsing as JSON\n",
    "try:\n",
    "    rec = json.loads(first_line)\n",
    "    print(\"\\nParsed JSON keys:\", list(rec.keys())[:25])\n",
    "    # Show a couple common locations\n",
    "    if \"inferenceId\" in rec:\n",
    "        print(\"inferenceId:\", rec[\"inferenceId\"])\n",
    "    if \"eventMetadata\" in rec:\n",
    "        print(\"eventMetadata keys:\", list(rec[\"eventMetadata\"].keys()))\n",
    "except Exception as e:\n",
    "    print(\"\\n\u26a0\ufe0f Could not parse first line as JSON:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "3262ee76-cf7f-46af-9e1b-fb9f45037967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total capture objects found: 1\n",
      "Input files: 0\n",
      "Output files: 1\n",
      "\u26a0\ufe0f No Input capture files found.\n",
      "\n",
      "--- OUTPUT CAPTURE ---\n",
      "Reading: monitoring/batch-transform/capture/1771374102/output/2026/02/18/00/e3fe3d4b-fe1e-470d-835b-f527c45271fd.json\n",
      "Raw Line: [{\"prefix\":\"s3://sagemaker-us-east-1-587322031938/monitoring/batch-transform/output/1771374102/\"},\"test_infer.csv.out\"]\n",
      "Type: List (Manifest file?)\n"
     ]
    }
   ],
   "source": [
    "# M5-3.2c Parse capture JSON properly (input + output) -- ROBUST VERSION\n",
    "import json\n",
    "\n",
    "# 1. Get ALL keys from the capture prefix\n",
    "resp = s3.list_objects_v2(Bucket=bucket, Prefix=capture_prefix)\n",
    "keys = [o[\"Key\"] for o in resp.get(\"Contents\", [])]\n",
    "\n",
    "print(f\"Total capture objects found: {len(keys)}\")\n",
    "\n",
    "# 2. Try to find Input vs Output files\n",
    "input_keys = [k for k in keys if \"/input/\" in k]\n",
    "output_keys = [k for k in keys if \"/output/\" in k]\n",
    "\n",
    "print(f\"Input files: {len(input_keys)}\")\n",
    "print(f\"Output files: {len(output_keys)}\")\n",
    "\n",
    "def show_first_record(file_key, label):\n",
    "    if not file_key:\n",
    "        print(f\"\\n--- {label} CAPTURE (Not Found) ---\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n--- {label} CAPTURE ---\")\n",
    "    print(f\"Reading: {file_key}\")\n",
    "    obj = s3.get_object(Bucket=bucket, Key=file_key)\n",
    "    # Read first line only\n",
    "    body = obj[\"Body\"].read().decode(\"utf-8\", errors=\"replace\").splitlines()\n",
    "    if not body:\n",
    "        print(\"File is empty.\")\n",
    "        return\n",
    "\n",
    "    first_line = body[0]\n",
    "    print(\"Raw Line:\", first_line[:200])\n",
    "    \n",
    "    try:\n",
    "        data = json.loads(first_line)\n",
    "        if isinstance(data, list):\n",
    "            print(\"Type: List (Manifest file?)\")\n",
    "        elif isinstance(data, dict):\n",
    "             print(\"Type: JSON Object (Data Capture?)\")\n",
    "             print(\"Keys:\", list(data.keys())[:10])\n",
    "    except:\n",
    "        print(\"Could not parse JSON.\")\n",
    "\n",
    "if input_keys:\n",
    "    show_first_record(input_keys[0], \"INPUT\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f No Input capture files found.\")\n",
    "\n",
    "if output_keys:\n",
    "    show_first_record(output_keys[0], \"OUTPUT\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f No Output capture files found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "4be1c514-7437-4b94-a99a-0936ef060eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TrainingJobs': ['sagemaker-xgboost-2026-02-18-02-57-54-957',\n",
       "  'pipelines-lejkxc4smoum-TrainXGBoost-laax2ShLV8',\n",
       "  'pipelines-olk8lxdtt2ge-TrainXGBoost-vdc0T45DDP',\n",
       "  'pipelines-uqdnir5f0wzm-TrainXGBoost-mWBJKpZnl6',\n",
       "  'pipelines-smk7ibe2196t-TrainXGBoost-e4yOWeKgkn'],\n",
       " 'TransformJobs': ['sagemaker-xgboost-2026-02-18-03-07-50-815',\n",
       "  'sagemaker-xgboost-2026-02-18-03-01-13-655',\n",
       "  'sagemaker-xgboost-2026-02-18-00-21-43-124',\n",
       "  'sagemaker-xgboost-2026-02-17-23-57-35-468',\n",
       "  'sagemaker-xgboost-2026-02-17-23-35-05-743'],\n",
       " 'ProcessingJobs': ['baseline-suggestion-job-2026-02-18-03-13-45-105',\n",
       "  'pipelines-lejkxc4smoum-EvaluateModel-gT0PLZMSJY',\n",
       "  'pipelines-uqdnir5f0wzm-EvaluateModel-eNx0DUsExW',\n",
       "  'pipelines-olk8lxdtt2ge-EvaluateModel-Nvu7I7WDWH',\n",
       "  'pipelines-smk7ibe2196t-EvaluateModel-bPtXEKJBPA']}"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# M5-4.1 Collect SageMaker job names for infrastructure monitoring\n",
    "\n",
    "\n",
    "sm = boto3.client(\"sagemaker\", region_name=region)\n",
    "\n",
    "jobs = {\n",
    "    \"TrainingJobs\": [],\n",
    "    \"TransformJobs\": [],\n",
    "    \"ProcessingJobs\": []\n",
    "}\n",
    "\n",
    "# Training jobs\n",
    "for j in sm.list_training_jobs(MaxResults=5)[\"TrainingJobSummaries\"]:\n",
    "    jobs[\"TrainingJobs\"].append(j[\"TrainingJobName\"])\n",
    "\n",
    "# Transform jobs\n",
    "for j in sm.list_transform_jobs(MaxResults=5)[\"TransformJobSummaries\"]:\n",
    "    jobs[\"TransformJobs\"].append(j[\"TransformJobName\"])\n",
    "\n",
    "# Processing jobs\n",
    "for j in sm.list_processing_jobs(MaxResults=5)[\"ProcessingJobSummaries\"]:\n",
    "    jobs[\"ProcessingJobs\"].append(j[\"ProcessingJobName\"])\n",
    "\n",
    "jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "f2f38b9e-ca2e-4951-a8d5-1f31cb3eeb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CloudWatch dashboard created: AAI540-Olist-MLops-Dashboard\n"
     ]
    }
   ],
   "source": [
    "# M5-4.3 Create CloudWatch Dashboard for ML Infrastructure\n",
    "\n",
    "cw = boto3.client(\"cloudwatch\", region_name=region)\n",
    "\n",
    "dashboard_name = \"AAI540-Olist-MLops-Dashboard\"\n",
    "\n",
    "dashboard_body = {\n",
    "    \"widgets\": [\n",
    "        {\n",
    "            \"type\": \"metric\",\n",
    "            \"x\": 0,\n",
    "            \"y\": 0,\n",
    "            \"width\": 12,\n",
    "            \"height\": 6,\n",
    "            \"properties\": {\n",
    "                \"title\": \"Training Job Duration\",\n",
    "                \"metrics\": [\n",
    "                    [\"AWS/SageMaker\", \"TrainingJobDuration\",\n",
    "                     \"TrainingJobName\", \"sagemaker-xgboost-2026-01-31-18-00-53-460\"]\n",
    "                ],\n",
    "                \"stat\": \"Average\",\n",
    "                \"period\": 300,\n",
    "                \"region\": region\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"metric\",\n",
    "            \"x\": 12,\n",
    "            \"y\": 0,\n",
    "            \"width\": 12,\n",
    "            \"height\": 6,\n",
    "            \"properties\": {\n",
    "                \"title\": \"Batch Transform Duration\",\n",
    "                \"metrics\": [\n",
    "                    [\"AWS/SageMaker\", \"TransformJobDuration\",\n",
    "                     \"TransformJobName\", \"sagemaker-xgboost-2026-02-08-21-21-26-827\"]\n",
    "                ],\n",
    "                \"stat\": \"Average\",\n",
    "                \"period\": 300,\n",
    "                \"region\": region\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"metric\",\n",
    "            \"x\": 0,\n",
    "            \"y\": 6,\n",
    "            \"width\": 12,\n",
    "            \"height\": 6,\n",
    "            \"properties\": {\n",
    "                \"title\": \"Processing Job Duration\",\n",
    "                \"metrics\": [\n",
    "                    [\"AWS/SageMaker\", \"ProcessingJobDuration\",\n",
    "                     \"ProcessingJobName\", \"baseline-suggestion-job-2026-02-08-21-32-41-870\"]\n",
    "                ],\n",
    "                \"stat\": \"Average\",\n",
    "                \"period\": 300,\n",
    "                \"region\": region\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"metric\",\n",
    "            \"x\": 12,\n",
    "            \"y\": 6,\n",
    "            \"width\": 12,\n",
    "            \"height\": 6,\n",
    "            \"properties\": {\n",
    "                \"title\": \"Job Failures (All SageMaker Jobs)\",\n",
    "                \"metrics\": [\n",
    "                    [\"AWS/SageMaker\", \"TrainingJobsFailed\"],\n",
    "                    [\"AWS/SageMaker\", \"TransformJobsFailed\"],\n",
    "                    [\"AWS/SageMaker\", \"ProcessingJobsFailed\"]\n",
    "                ],\n",
    "                \"stat\": \"Sum\",\n",
    "                \"period\": 300,\n",
    "                \"region\": region\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "cw.put_dashboard(\n",
    "    DashboardName=dashboard_name,\n",
    "    DashboardBody=json.dumps(dashboard_body)\n",
    ")\n",
    "\n",
    "print(\"CloudWatch dashboard created:\", dashboard_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "d02ad7f8-c411-4c51-89e3-8bb16a987654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-587322031938/monitoring/baselines/data_quality/results/statistics.json -> objects: 1\n",
      "s3://sagemaker-us-east-1-587322031938/monitoring/baselines/data_quality/results/constraints.json -> objects: 1\n",
      "s3://sagemaker-us-east-1-587322031938/monitoring/batch-transform/capture/ -> objects: 4\n"
     ]
    }
   ],
   "source": [
    "# M5-5.1 Verify monitoring report artifacts exist\n",
    "\n",
    "\n",
    "s3 = boto3.client(\"s3\", region_name=region)\n",
    "\n",
    "paths = [\n",
    "    baseline_statistics_uri,\n",
    "    baseline_constraints_uri,\n",
    "    f\"s3://{bucket}/monitoring/batch-transform/capture/\"\n",
    "]\n",
    "\n",
    "for p in paths:\n",
    "    bucket_name, key = p.replace(\"s3://\", \"\").split(\"/\", 1)\n",
    "    resp = s3.list_objects_v2(Bucket=bucket_name, Prefix=key)\n",
    "    print(f\"{p} -> objects:\", resp.get(\"KeyCount\", 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "5e9ac68c-609e-474d-9192-6464992802b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region: us-east-1\n",
      "SageMaker list_pipelines: OK\n",
      "CodePipeline list_pipelines: OK\n",
      "CodeBuild list_projects: AccessDeniedException - User: arn:aws:sts::587322031938:assumed-role/LabRole/SageMaker is not authorized to perform: codebuild:ListProjects because no identity-based policy allows the codebuild:ListProjects action\n",
      "StepFunctions list_state_machines: OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'stateMachines': [],\n",
       " 'ResponseMetadata': {'RequestId': '3b4db087-cec0-4d9c-b7e3-8a515faaa1fd',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '3b4db087-cec0-4d9c-b7e3-8a515faaa1fd',\n",
       "   'date': 'Wed, 18 Feb 2026 03:21:42 GMT',\n",
       "   'content-type': 'application/x-amz-json-1.0',\n",
       "   'content-length': '20',\n",
       "   'connection': 'keep-alive'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Capability check (prints all results)\n",
    "\n",
    "import boto3, botocore\n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "print(\"Region:\", region)\n",
    "\n",
    "def ok(name, fn):\n",
    "    try:\n",
    "        r = fn()\n",
    "        print(f\"{name}: OK\")\n",
    "        return r\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        print(f\"{name}: {e.response['Error'].get('Code')} - {e.response['Error'].get('Message')}\")\n",
    "        return None\n",
    "\n",
    "sm = boto3.client(\"sagemaker\", region_name=region)\n",
    "cp = boto3.client(\"codepipeline\", region_name=region)\n",
    "cb = boto3.client(\"codebuild\", region_name=region)\n",
    "sf = boto3.client(\"stepfunctions\", region_name=region)\n",
    "\n",
    "ok(\"SageMaker list_pipelines\", lambda: sm.list_pipelines(MaxResults=5))\n",
    "ok(\"CodePipeline list_pipelines\", lambda: cp.list_pipelines(maxResults=5))\n",
    "ok(\"CodeBuild list_projects\", lambda: cb.list_projects(sortBy=\"NAME\", sortOrder=\"ASCENDING\"))\n",
    "ok(\"StepFunctions list_state_machines\", lambda: sf.list_state_machines(maxResults=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "8a7b2ab9-76c8-41ae-8eb2-0a0e7afb407d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote pipeline scripts to ./pipeline_scripts/\n",
      "['ci_check.py', 'evaluate.py']\n"
     ]
    }
   ],
   "source": [
    "# M6-1 Create pipeline scripts locally (CI + evaluation)\n",
    "\n",
    "import os, textwrap\n",
    "\n",
    "os.makedirs(\"pipeline_scripts\", exist_ok=True)\n",
    "\n",
    "# 1) CI step: cheap, deterministic success/fail for demo\n",
    "ci_check = r\"\"\"\n",
    "import argparse, sys, json, os\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--force_fail\", type=str, default=\"false\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "force_fail = args.force_fail.strip().lower() in (\"1\",\"true\",\"yes\",\"y\")\n",
    "\n",
    "print(\"CI Check running. force_fail =\", force_fail)\n",
    "\n",
    "# You can add real checks here (schema check, file existence in /opt/ml/processing/input, etc.)\n",
    "# For the demo: deterministic failure when force_fail=true\n",
    "if force_fail:\n",
    "    print(\"Forcing CI failure for demo.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"CI checks passed.\")\n",
    "\"\"\"\n",
    "\n",
    "with open(\"pipeline_scripts/ci_check.py\", \"w\") as f:\n",
    "    f.write(textwrap.dedent(ci_check))\n",
    "\n",
    "# 2) Evaluation step: compute AUC from Batch predictions or directly from XGBoost model\n",
    "evaluate = r\"\"\"\n",
    "import argparse, json, os\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--test\", type=str, default=\"/opt/ml/processing/test/test.csv\")\n",
    "parser.add_argument(\"--model\", type=str, default=\"/opt/ml/processing/model/xgboost-model\")\n",
    "parser.add_argument(\"--output\", type=str, default=\"/opt/ml/processing/evaluation\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "# test.csv format: label first, then 8 features (no header)\n",
    "df = pd.read_csv(args.test, header=None)\n",
    "y = df.iloc[:,0].astype(int).values\n",
    "X = df.iloc[:,1:].values\n",
    "\n",
    "booster = xgb.Booster()\n",
    "booster.load_model(args.model)\n",
    "dtest = xgb.DMatrix(X)\n",
    "pred = booster.predict(dtest)\n",
    "\n",
    "auc = float(roc_auc_score(y, pred)) if len(np.unique(y)) > 1 else float(\"nan\")\n",
    "acc = float(accuracy_score(y, (pred >= 0.5).astype(int)))\n",
    "\n",
    "os.makedirs(args.output, exist_ok=True)\n",
    "report = {\"auc\": auc, \"accuracy\": acc, \"rows\": int(len(df))}\n",
    "with open(os.path.join(args.output, \"evaluation.json\"), \"w\") as f:\n",
    "    json.dump(report, f)\n",
    "\n",
    "print(\"Wrote evaluation:\", report)\n",
    "\"\"\"\n",
    "\n",
    "with open(\"pipeline_scripts/evaluate.py\", \"w\") as f:\n",
    "    f.write(textwrap.dedent(evaluate))\n",
    "\n",
    "print(\"Wrote pipeline scripts to ./pipeline_scripts/\")\n",
    "print(os.listdir(\"pipeline_scripts\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "5107f6df-367c-4a67-a131-9e2480df49d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scripts uploaded to: s3://sagemaker-us-east-1-587322031938/cicd/pipeline-scripts/v1/\n"
     ]
    }
   ],
   "source": [
    "# M6-2 Upload scripts to S3\n",
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "prefix = \"cicd/pipeline-scripts/v1\"\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "s3_scripts_uri = S3Uploader.upload(\n",
    "    local_path=\"pipeline_scripts\",\n",
    "    desired_s3_uri=f\"s3://{bucket}/{prefix}/\"\n",
    ")\n",
    "\n",
    "print(\"Scripts uploaded to:\", s3_scripts_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "5e6ec781-e26f-4598-9b19-16c673c94128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_csv_uri: s3://sagemaker-us-east-1-587322031938/modeling/xgb_v1/train/train.csv\n",
      "val_csv_uri:   s3://sagemaker-us-east-1-587322031938/modeling/xgb_v1/val/val.csv\n",
      "test_csv_uri:  s3://sagemaker-us-east-1-587322031938/modeling/xgb_v1/test/test.csv\n",
      "scripts uri:   s3://sagemaker-us-east-1-587322031938/cicd/pipeline-scripts/v1/\n"
     ]
    }
   ],
   "source": [
    "# M6-3A Reconstruct S3 URIs\n",
    "\n",
    "# Model training data\n",
    "xgb_prefix = f\"s3://{bucket}/modeling/xgb_v1/\"\n",
    "train_csv_uri = f\"{xgb_prefix}train/train.csv\"\n",
    "val_csv_uri   = f\"{xgb_prefix}val/val.csv\"\n",
    "test_csv_uri  = f\"{xgb_prefix}test/test.csv\"\n",
    "\n",
    "# CI/CD scripts location\n",
    "s3_scripts_uri = f\"s3://{bucket}/cicd/pipeline-scripts/v1/\"\n",
    "\n",
    "print(\"train_csv_uri:\", train_csv_uri)\n",
    "print(\"val_csv_uri:  \", val_csv_uri)\n",
    "print(\"test_csv_uri: \", test_csv_uri)\n",
    "print(\"scripts uri:  \", s3_scripts_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "834a9eab-d817-46fa-996b-34b13a27a258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated evaluate.py and re-uploaded scripts to: s3://sagemaker-us-east-1-587322031938/cicd/pipeline-scripts/v1/\n",
      "Scripts: ['ci_check.py', 'evaluate.py']\n"
     ]
    }
   ],
   "source": [
    "# M6-4 Update evaluate.py (no sklearn dependency) + re-upload scripts\n",
    "import os, textwrap\n",
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "# overwrite evaluate.py with a no-sklearn version\n",
    "evaluate = r\"\"\"\n",
    "import argparse, json, os, tarfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "def auc_roc(y_true, y_score):\n",
    "    # Fast AUC without sklearn (rank-based)\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_score = np.asarray(y_score).astype(float)\n",
    "\n",
    "    pos = y_true == 1\n",
    "    neg = y_true == 0\n",
    "    n_pos = np.sum(pos)\n",
    "    n_neg = np.sum(neg)\n",
    "    if n_pos == 0 or n_neg == 0:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    order = np.argsort(y_score)\n",
    "    ranks = np.empty_like(order, dtype=float)\n",
    "    ranks[order] = np.arange(1, len(y_score) + 1)\n",
    "\n",
    "    # average ranks for ties\n",
    "    unique_scores, inverse, counts = np.unique(y_score, return_inverse=True, return_counts=True)\n",
    "    if np.any(counts > 1):\n",
    "        # compute average rank per group\n",
    "        sum_ranks = np.bincount(inverse, weights=ranks)\n",
    "        avg_ranks = sum_ranks / counts\n",
    "        ranks = avg_ranks[inverse]\n",
    "\n",
    "    sum_pos_ranks = np.sum(ranks[pos])\n",
    "    auc = (sum_pos_ranks - n_pos * (n_pos + 1) / 2.0) / (n_pos * n_neg)\n",
    "    return float(auc)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--test\", type=str, default=\"/opt/ml/processing/test/test.csv\")\n",
    "parser.add_argument(\"--model_tar\", type=str, default=\"/opt/ml/processing/model/model.tar.gz\")\n",
    "parser.add_argument(\"--output\", type=str, default=\"/opt/ml/processing/evaluation\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "df = pd.read_csv(args.test, header=None)\n",
    "y = df.iloc[:, 0].astype(int).values\n",
    "X = df.iloc[:, 1:].values\n",
    "\n",
    "# Extract model from model.tar.gz\n",
    "model_dir = \"/opt/ml/processing/model_extracted\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "with tarfile.open(args.model_tar, \"r:gz\") as tar:\n",
    "    tar.extractall(path=model_dir)\n",
    "\n",
    "model_path = os.path.join(model_dir, \"xgboost-model\")\n",
    "booster = xgb.Booster()\n",
    "booster.load_model(model_path)\n",
    "\n",
    "dtest = xgb.DMatrix(X)\n",
    "pred = booster.predict(dtest)\n",
    "\n",
    "auc = auc_roc(y, pred)\n",
    "acc = float(np.mean((pred >= 0.5).astype(int) == y))\n",
    "\n",
    "os.makedirs(args.output, exist_ok=True)\n",
    "report = {\"auc\": auc, \"accuracy\": acc, \"rows\": int(len(df))}\n",
    "with open(os.path.join(args.output, \"evaluation.json\"), \"w\") as f:\n",
    "    json.dump(report, f)\n",
    "\n",
    "print(\"Wrote evaluation:\", report)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.makedirs(\"pipeline_scripts\", exist_ok=True)\n",
    "\n",
    "with open(\"pipeline_scripts/evaluate.py\", \"w\") as f:\n",
    "    f.write(textwrap.dedent(evaluate))\n",
    "\n",
    "# re-upload scripts\n",
    "prefix = \"cicd/pipeline-scripts/v1\"\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "s3_scripts_uri = S3Uploader.upload(\n",
    "    local_path=\"pipeline_scripts\",\n",
    "    desired_s3_uri=f\"s3://{bucket}/{prefix}/\"\n",
    ")\n",
    "\n",
    "print(\"Updated evaluate.py and re-uploaded scripts to:\", s3_scripts_uri)\n",
    "print(\"Scripts:\", os.listdir(\"pipeline_scripts\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "547e70c5-af87-4304-b963-da2cb7c46652",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "/opt/conda/lib/python3.12/site-packages/sagemaker/workflow/pipeline_context.py:332: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n",
      "  warnings.warn(\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline upserted: AAI540-Olist-CICD-Pipeline\n"
     ]
    }
   ],
   "source": [
    "# M6-5 Create SageMaker Pipeline DAG\n",
    "\n",
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.parameters import ParameterString, ParameterFloat\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.fail_step import FailStep\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Inputs (already defined in kernel)\n",
    "# train_csv_uri, val_csv_uri, test_csv_uri, s3_scripts_uri\n",
    "\n",
    "pipeline_sess = PipelineSession()\n",
    "\n",
    "# ---- Parameters for SUCCESS vs FAIL demos ----\n",
    "ForceFailCI  = ParameterString(name=\"ForceFailCI\", default_value=\"false\")\n",
    "AucThreshold = ParameterFloat(name=\"AucThreshold\", default_value=0.55)\n",
    "\n",
    "# ---- Images ----\n",
    "xgb_image = retrieve(framework=\"xgboost\", region=region, version=\"1.7-1\")\n",
    "\n",
    "# ---- Step 1: \"CI\" checks (cheap processing step, can fail immediately) ----\n",
    "ci_proc = ScriptProcessor(\n",
    "    image_uri=xgb_image,\n",
    "    command=[\"python3\"],\n",
    "    role=role,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    instance_count=1,\n",
    "    sagemaker_session=pipeline_sess,\n",
    ")\n",
    "\n",
    "ci_step = ProcessingStep(\n",
    "    name=\"CIChecks\",\n",
    "    processor=ci_proc,\n",
    "    code=f\"{s3_scripts_uri}ci_check.py\",\n",
    "    job_arguments=[\"--ForceFailCI\", ForceFailCI],\n",
    ")\n",
    "\n",
    "# ---- Step 2: Train model (tiny + cheap) ----\n",
    "output_path = f\"s3://{bucket}/cicd/pipeline-artifacts/model/\"\n",
    "\n",
    "xgb_est = Estimator(\n",
    "    image_uri=xgb_image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    output_path=output_path,\n",
    "    sagemaker_session=pipeline_sess,\n",
    ")\n",
    "\n",
    "xgb_est.set_hyperparameters(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\",\n",
    "    num_round=10,          # cheap\n",
    "    max_depth=4,\n",
    "    eta=0.2,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    ")\n",
    "\n",
    "train_step = TrainingStep(\n",
    "    name=\"TrainXGBoost\",\n",
    "    estimator=xgb_est,\n",
    "    inputs={\n",
    "        \"train\": sagemaker.inputs.TrainingInput(train_csv_uri, content_type=\"text/csv\"),\n",
    "        \"validation\": sagemaker.inputs.TrainingInput(val_csv_uri, content_type=\"text/csv\"),\n",
    "    },\n",
    "    depends_on=[ci_step.name],\n",
    ")\n",
    "\n",
    "# ---- Step 3: Evaluate model (processing) ----\n",
    "eval_proc = ScriptProcessor(\n",
    "    image_uri=xgb_image,\n",
    "    command=[\"python3\"],\n",
    "    role=role,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    instance_count=1,\n",
    "    sagemaker_session=pipeline_sess,\n",
    ")\n",
    "\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"EvaluationReport\",\n",
    "    output_name=\"evaluation\",\n",
    "    path=\"evaluation.json\",\n",
    ")\n",
    "\n",
    "eval_step = ProcessingStep(\n",
    "    name=\"EvaluateModel\",\n",
    "    processor=eval_proc,\n",
    "    code=f\"{s3_scripts_uri}evaluate.py\",\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=test_csv_uri,\n",
    "            destination=\"/opt/ml/processing/test/test.csv\",\n",
    "            input_name=\"test\",\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination=\"/opt/ml/processing/model/model.tar.gz\",\n",
    "            input_name=\"model\",\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"evaluation\",\n",
    "            source=\"/opt/ml/processing/evaluation\",\n",
    "            destination=f\"s3://{bucket}/cicd/pipeline-artifacts/evaluation/\",\n",
    "        )\n",
    "    ],\n",
    "    property_files=[evaluation_report],\n",
    ")\n",
    "\n",
    "# ---- Step 4: Gate on AUC ----\n",
    "auc_value = JsonGet(\n",
    "    step_name=eval_step.name,\n",
    "    property_file=evaluation_report,\n",
    "    json_path=\"auc\",\n",
    ")\n",
    "\n",
    "fail_step = FailStep(\n",
    "    name=\"FailQualityGate\",\n",
    "    error_message=\"Model did not meet AUC threshold.\",\n",
    ")\n",
    "\n",
    "# ---- Step 5: Register model (NO endpoint) using ModelStep ----\n",
    "model = Model(\n",
    "    image_uri=xgb_image,\n",
    "    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_sess,\n",
    ")\n",
    "\n",
    "register_args = model.register(\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.m5.large\"],\n",
    "    transform_instances=[\"ml.m5.large\"],\n",
    "    model_package_group_name=\"olist-late-delivery-xgb\",\n",
    "    approval_status=\"Approved\",\n",
    ")\n",
    "\n",
    "register_step = ModelStep(\n",
    "    name=\"RegisterModel\",\n",
    "    step_args=register_args\n",
    ")\n",
    "\n",
    "cond_step = ConditionStep(\n",
    "    name=\"AUCQualityGate\",\n",
    "    conditions=[ConditionGreaterThanOrEqualTo(left=auc_value, right=AucThreshold)],\n",
    "    if_steps=[register_step],\n",
    "    else_steps=[fail_step],\n",
    ")\n",
    "\n",
    "pipeline_name = \"AAI540-Olist-CICD-Pipeline\"\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[ForceFailCI, AucThreshold],\n",
    "    steps=[ci_step, train_step, eval_step, cond_step],\n",
    "    sagemaker_session=pipeline_sess,\n",
    ")\n",
    "\n",
    "pipeline.upsert(role_arn=role)\n",
    "print(\"Pipeline upserted:\", pipeline_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "444a42d7-6f40-4f86-aa6f-a3e78bcf9b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started SUCCESS execution: arn:aws:sagemaker:us-east-1:587322031938:pipeline/AAI540-Olist-CICD-Pipeline/execution/0toc9pi63ypd\n"
     ]
    }
   ],
   "source": [
    "# M6-6.1 SUCCESS execution\n",
    "\n",
    "\n",
    "execution_ok = pipeline.start(parameters={\"ForceFailCI\": \"false\", \"AucThreshold\": 0.55})\n",
    "print(\"Started SUCCESS execution:\", execution_ok.arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "a0917f68-23c7-4c5b-ae03-6006ef0db94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started FAIL execution: arn:aws:sagemaker:us-east-1:587322031938:pipeline/AAI540-Olist-CICD-Pipeline/execution/n2jwkzcgkjds\n"
     ]
    }
   ],
   "source": [
    "# M6-6.2 FAIL execution (CI failure = cheapest, no training spend)\n",
    "\n",
    "execution_fail = pipeline.start(parameters={\"ForceFailCI\": \"true\", \"AucThreshold\": 0.55})\n",
    "print(\"Started FAIL execution:\", execution_fail.arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "2343351e-1526-433c-8e81-d3a825fa5712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started SUCCESS execution: arn:aws:sagemaker:us-east-1:587322031938:pipeline/AAI540-Olist-CICD-Pipeline/execution/sxw0gjm2cvif\n"
     ]
    }
   ],
   "source": [
    "# M6-6 SUCCESS RUN\n",
    "execution_ok = pipeline.start(\n",
    "    parameters={\n",
    "        \"ForceFailCI\": \"false\",\n",
    "        \"AucThreshold\": 0.50  # temporarily lower to guarantee pass\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Started SUCCESS execution:\", execution_ok.arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "cd170373-36a6-43a4-a1cf-5dbfbaffe9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated ci_check.py\n"
     ]
    }
   ],
   "source": [
    "# M6-Fix CI Script\n",
    "\n",
    "ci_code = \"\"\"\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--ForceFailCI', type=str, default='false')\n",
    "args = parser.parse_args()\n",
    "\n",
    "force_fail = args.ForceFailCI.lower() == 'true'\n",
    "\n",
    "print(f\"ForceFailCI parameter received: {args.ForceFailCI}\")\n",
    "print(f\"Interpreted as boolean: {force_fail}\")\n",
    "\n",
    "if force_fail:\n",
    "    print(\"CI failure forced. Exiting with error.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"CI checks passed successfully.\")\n",
    "\"\"\"\n",
    "\n",
    "with open(\"pipeline_scripts/ci_check.py\", \"w\") as f:\n",
    "    f.write(ci_code)\n",
    "\n",
    "print(\"Updated ci_check.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "a7ccbec3-c394-4a29-8b6f-20e87366cf05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-uploaded scripts\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "S3Uploader.upload(\n",
    "    local_path=\"pipeline_scripts\",\n",
    "    desired_s3_uri=f\"s3://{bucket}/cicd/pipeline-scripts/v1/\"\n",
    ")\n",
    "\n",
    "print(\"Re-uploaded scripts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "503caa82-d299-4947-ab64-71d185cb7a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-upserted pipeline: AAI540-Olist-CICD-Pipeline\n"
     ]
    }
   ],
   "source": [
    "# M6-Next: Re-upsert pipeline so it picks up latest scripts\n",
    "\n",
    "\n",
    "pipeline.upsert(role_arn=role)\n",
    "print(\"Re-upserted pipeline:\", pipeline.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "dbf36acd-e144-40fa-8b72-8894b88efa49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started FAIL execution: arn:aws:sagemaker:us-east-1:587322031938:pipeline/AAI540-Olist-CICD-Pipeline/execution/dr3ip7n6t5o1\n"
     ]
    }
   ],
   "source": [
    "# M6-Run FAIL execution (CI fails immediately)\n",
    "\n",
    "\n",
    "execution_fail = pipeline.start(parameters={\"ForceFailCI\": \"true\", \"AucThreshold\": 0.50})\n",
    "print(\"Started FAIL execution:\", execution_fail.arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "6d7791eb-0a45-416c-bbf1-8ad69c3f293e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started SUCCESS execution: arn:aws:sagemaker:us-east-1:587322031938:pipeline/AAI540-Olist-CICD-Pipeline/execution/pd7ka2gv6sr9\n"
     ]
    }
   ],
   "source": [
    "# M6-Run SUCCESS execution\n",
    "\n",
    "\n",
    "execution_ok = pipeline.start(parameters={\"ForceFailCI\": \"false\", \"AucThreshold\": 0.50})\n",
    "print(\"Started SUCCESS execution:\", execution_ok.arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "2890d93a-b3ba-481b-a739-c757ea111139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last FAIL ARN: arn:aws:sagemaker:us-east-1:587322031938:pipeline/AAI540-Olist-CICD-Pipeline/execution/dr3ip7n6t5o1\n",
      "Last SUCCESS ARN: arn:aws:sagemaker:us-east-1:587322031938:pipeline/AAI540-Olist-CICD-Pipeline/execution/pd7ka2gv6sr9\n"
     ]
    }
   ],
   "source": [
    "print(\"Last FAIL ARN:\", execution_fail.arn)\n",
    "print(\"Last SUCCESS ARN:\", execution_ok.arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "3f9879e8-a493-4748-9be8-d0ef2327aab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started SUCCESS execution: arn:aws:sagemaker:us-east-1:587322031938:pipeline/AAI540-Olist-CICD-Pipeline/execution/ox0oso7hmb9z\n"
     ]
    }
   ],
   "source": [
    "execution_success = pipeline.start(\n",
    "    parameters={\n",
    "        \"ForceFailCI\": \"false\",   # do NOT fail CI\n",
    "        \"AucThreshold\": 0.50      # easy pass threshold\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Started SUCCESS execution:\", execution_success.arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "dea109d6-c93c-4c8b-9a73-52bcdd938ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Executing\n",
      "\n",
      "Steps:\n"
     ]
    }
   ],
   "source": [
    "# M6-Debug: quick status + step statuses for latest execution\n",
    "\n",
    "\n",
    "import boto3, time\n",
    "sm = boto3.client(\"sagemaker\")\n",
    "\n",
    "exec_arn = execution_success.arn\n",
    "\n",
    "def print_steps(execution_arn: str):\n",
    "    steps = sm.list_pipeline_execution_steps(\n",
    "        PipelineExecutionArn=execution_arn,\n",
    "        SortOrder=\"Ascending\",\n",
    "        MaxResults=50\n",
    "    )[\"PipelineExecutionSteps\"]\n",
    "    for s in steps:\n",
    "        name = s[\"StepName\"]\n",
    "        status = s[\"StepStatus\"]\n",
    "        reason = s.get(\"FailureReason\", \"\")\n",
    "        print(f\"- {name}: {status}\" + (f\" | {reason}\" if reason else \"\"))\n",
    "\n",
    "ex = sm.describe_pipeline_execution(PipelineExecutionArn=exec_arn)\n",
    "print(\"Status:\", ex[\"PipelineExecutionStatus\"])\n",
    "if ex.get(\"FailureReason\"):\n",
    "    print(\"FailureReason:\", ex[\"FailureReason\"])\n",
    "\n",
    "print(\"\\nSteps:\")\n",
    "print_steps(exec_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "905221c3-b2ce-433f-87d9-0031eff153ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found: 4\n",
      "s3://sagemaker-us-east-1-587322031938/cicd/pipeline-scripts/v1//ci_check.py\n",
      "s3://sagemaker-us-east-1-587322031938/cicd/pipeline-scripts/v1//evaluate.py\n",
      "s3://sagemaker-us-east-1-587322031938/cicd/pipeline-scripts/v1/ci_check.py\n",
      "s3://sagemaker-us-east-1-587322031938/cicd/pipeline-scripts/v1/evaluate.py\n"
     ]
    }
   ],
   "source": [
    "#A\n",
    "\n",
    "prefix = \"cicd/pipeline-scripts/v1/\"\n",
    "paths = wr.s3.list_objects(f\"s3://{bucket}/{prefix}\")\n",
    "print(\"Found:\", len(paths))\n",
    "for p in paths:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "f79a9afa-cac4-4e6e-baaf-027f22d4db53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#B\n",
    "s3_scripts_prefix = f\"s3://{bucket}/cicd/pipeline-scripts/v1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "ee320cc4-e81d-454e-82fd-7249fc15c776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sagemaker.processing.ProcessingInput at 0x7fae6e243020>"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ProcessingInput(\n",
    "    source=s3_scripts_prefix,\n",
    "    destination=\"/opt/ml/processing/input/code\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "75a10ac4-f564-4674-bb24-8ecd1134718b",
   "metadata": {},
   "outputs": [],
   "source": [
    "code=\"/opt/ml/processing/input/code/ci_check.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "ab64b135-5d09-4859-947a-4e4e155396af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:sagemaker:us-east-1:587322031938:pipeline/AAI540-Olist-CICD-Pipeline/execution/jlzqjpwffsuw\n"
     ]
    }
   ],
   "source": [
    "#C\n",
    "execution_success = pipeline.start(\n",
    "    parameters={\"ForceFailCI\": \"false\", \"AucThreshold\": 0.50}\n",
    ")\n",
    "print(execution_success.arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "b4425090-87f7-4893-a067-097edee9a414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded scripts to: s3://sagemaker-us-east-1-587322031938/cicd/pipeline-scripts/v2/\n"
     ]
    }
   ],
   "source": [
    "# M6-Fix: upload scripts to a clean prefix (no double slashes)\n",
    "\n",
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "clean_scripts_uri = f\"s3://{bucket}/cicd/pipeline-scripts/v2/\"  # NEW folder\n",
    "\n",
    "S3Uploader.upload(\n",
    "    local_path=\"pipeline_scripts\",\n",
    "    desired_s3_uri=clean_scripts_uri\n",
    ")\n",
    "\n",
    "print(\"Uploaded scripts to:\", clean_scripts_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "e4ee2e38-47e0-4a53-9e3a-146979518dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found: 2\n",
      "s3://sagemaker-us-east-1-587322031938/cicd/pipeline-scripts/v2//ci_check.py\n",
      "s3://sagemaker-us-east-1-587322031938/cicd/pipeline-scripts/v2//evaluate.py\n"
     ]
    }
   ],
   "source": [
    "paths = wr.s3.list_objects(clean_scripts_uri)\n",
    "print(\"Found:\", len(paths))\n",
    "for p in paths:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "d815ed51-fb49-4a5e-b466-d2479ad5b776",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_scripts_uri = clean_scripts_uri  # points to .../v2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "c1f449c2-f1ab-4828-8821-6f996a4d7dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "code=f\"{s3_scripts_uri}ci_check.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "1d78a4cb-bc22-4e72-9d01-20acd0cc16c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-upserted pipeline: AAI540-Olist-CICD-Pipeline\n"
     ]
    }
   ],
   "source": [
    "pipeline.upsert(role_arn=role)\n",
    "print(\"Re-upserted pipeline:\", pipeline.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "c6631ed0-b3ef-4311-ac9d-7866bb8f90a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS ARN: arn:aws:sagemaker:us-east-1:587322031938:pipeline/AAI540-Olist-CICD-Pipeline/execution/h7fp444o4z74\n"
     ]
    }
   ],
   "source": [
    "execution_success = pipeline.start(parameters={\"ForceFailCI\":\"false\",\"AucThreshold\":0.50})\n",
    "print(\"SUCCESS ARN:\", execution_success.arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "d9e85493-f07e-4dbf-b7c9-0994d103d3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAIL ARN: arn:aws:sagemaker:us-east-1:587322031938:pipeline/AAI540-Olist-CICD-Pipeline/execution/9vwnh579feru\n"
     ]
    }
   ],
   "source": [
    "execution_fail = pipeline.start(parameters={\"ForceFailCI\":\"true\",\"AucThreshold\":0.50})\n",
    "print(\"FAIL ARN:\", execution_fail.arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "98d8cd7f-e588-4d53-a63b-101b32c09c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest execution ARN: arn:aws:sagemaker:us-east-1:587322031938:pipeline/AAI540-Olist-CICD-Pipeline/execution/9vwnh579feru\n",
      "\n",
      "Execution status: Executing\n",
      "Failure reason: None\n",
      "\n",
      "Step details:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "sm = boto3.client(\"sagemaker\")\n",
    "pipeline_name = \"AAI540-Olist-CICD-Pipeline\"\n",
    "\n",
    "# Get latest execution\n",
    "execs = sm.list_pipeline_executions(\n",
    "    PipelineName=pipeline_name,\n",
    "    MaxResults=1\n",
    ")[\"PipelineExecutionSummaries\"]\n",
    "\n",
    "latest_arn = execs[0][\"PipelineExecutionArn\"]\n",
    "print(\"Latest execution ARN:\", latest_arn)\n",
    "\n",
    "# Execution-level failure\n",
    "desc = sm.describe_pipeline_execution(\n",
    "    PipelineExecutionArn=latest_arn\n",
    ")\n",
    "\n",
    "print(\"\\nExecution status:\", desc[\"PipelineExecutionStatus\"])\n",
    "print(\"Failure reason:\", desc.get(\"FailureReason\"))\n",
    "\n",
    "# Step-level inspection\n",
    "steps = sm.list_pipeline_execution_steps(\n",
    "    PipelineExecutionArn=latest_arn,\n",
    "    MaxResults=50\n",
    ")[\"PipelineExecutionSteps\"]\n",
    "\n",
    "print(\"\\nStep details:\")\n",
    "for s in steps:\n",
    "    print(f\"\\nStep: {s['StepName']}\")\n",
    "    print(\"Status:\", s[\"StepStatus\"])\n",
    "    if \"FailureReason\" in s:\n",
    "        print(\"FailureReason:\", s[\"FailureReason\"])\n",
    "    if \"Metadata\" in s:\n",
    "        meta = s[\"Metadata\"]\n",
    "        if \"ProcessingJob\" in meta:\n",
    "            print(\"ProcessingJob ARN:\", meta[\"ProcessingJob\"].get(\"Arn\"))\n",
    "        if \"TrainingJob\" in meta:\n",
    "            print(\"TrainingJob ARN:\", meta[\"TrainingJob\"].get(\"Arn\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "243e6130-1555-41b6-aeba-add8023d6063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded: s3://sagemaker-us-east-1-587322031938/cicd/pipeline-scripts/v1/ci_check.py\n",
      "Uploaded: s3://sagemaker-us-east-1-587322031938/cicd/pipeline-scripts/v1/evaluate.py\n",
      "Verified exists: s3://sagemaker-us-east-1-587322031938/cicd/pipeline-scripts/v1/ci_check.py\n",
      "Verified exists: s3://sagemaker-us-east-1-587322031938/cicd/pipeline-scripts/v1/evaluate.py\n",
      "\n",
      "Use this scripts prefix in pipeline: s3://sagemaker-us-east-1-587322031938/cicd/pipeline-scripts/v1/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Local folder\n",
    "local_dir = \"./pipeline_scripts\"\n",
    "assert os.path.isdir(local_dir), f\"Missing local folder: {local_dir}\"\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# Canonical keys (NO double slashes)\n",
    "prefix = \"cicd/pipeline-scripts/v1\"\n",
    "uploads = {\n",
    "    \"ci_check.py\": f\"{prefix}/ci_check.py\",\n",
    "    \"evaluate.py\": f\"{prefix}/evaluate.py\",\n",
    "}\n",
    "\n",
    "for fname, key in uploads.items():\n",
    "    local_path = os.path.join(local_dir, fname)\n",
    "    assert os.path.isfile(local_path), f\"Missing local file: {local_path}\"\n",
    "    s3.upload_file(local_path, bucket, key)\n",
    "    print(\"Uploaded:\", f\"s3://{bucket}/{key}\")\n",
    "\n",
    "# Verify they exist EXACTLY at those keys\n",
    "for fname, key in uploads.items():\n",
    "    s3.head_object(Bucket=bucket, Key=key)\n",
    "    print(\"Verified exists:\", f\"s3://{bucket}/{key}\")\n",
    "\n",
    "# This is the prefix we want the pipeline to use\n",
    "s3_scripts_prefix = f\"s3://{bucket}/{prefix}/\"\n",
    "print(\"\\nUse this scripts prefix in pipeline:\", s3_scripts_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "dc17c1df-8cfe-4356-bcd1-ea32a31b2d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found: 4\n",
      " - s3://sagemaker-us-east-1-587322031938/cicd/pipeline-scripts/v1//ci_check.py\n",
      " - s3://sagemaker-us-east-1-587322031938/cicd/pipeline-scripts/v1//evaluate.py\n",
      " - s3://sagemaker-us-east-1-587322031938/cicd/pipeline-scripts/v1/ci_check.py\n",
      " - s3://sagemaker-us-east-1-587322031938/cicd/pipeline-scripts/v1/evaluate.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "prefix = \"cicd/pipeline-scripts/v1/\"\n",
    "\n",
    "resp = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "keys = [o[\"Key\"] for o in resp.get(\"Contents\", [])]\n",
    "print(\"Found:\", len(keys))\n",
    "for k in keys:\n",
    "    print(\" -\", f\"s3://{bucket}/{k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "c46d8914-b743-4ad8-ae58-e8e03376ec01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sagemaker.processing.ProcessingInput at 0x7fae70730ad0>"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ProcessingInput(\n",
    "    source=s3_scripts_prefix,                 # <-- folder/prefix, not a file\n",
    "    destination=\"/opt/ml/processing/code\",\n",
    "    input_name=\"code\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "d90ce76d-ee5d-4673-9d82-f16a1591591b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:sagemaker:us-east-1:587322031938:pipeline/AAI540-Olist-CICD-Pipeline/execution/5o2frqil7x6r\n"
     ]
    }
   ],
   "source": [
    "execution_success = pipeline.start(parameters={\"ForceFailCI\":\"false\",\"AucThreshold\":0.50})\n",
    "print(execution_success.arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "ff33497a-fae0-456c-8619-75eb05481d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:sagemaker:us-east-1:587322031938:pipeline/AAI540-Olist-CICD-Pipeline/execution/f1kkufzdjx2b\n"
     ]
    }
   ],
   "source": [
    "execution_fail = pipeline.start(parameters={\"ForceFailCI\":\"true\",\"AucThreshold\":0.50})\n",
    "print(execution_fail.arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "dea3d20e-a636-4373-a5ab-e6265f4b0e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found: 4\n",
      "s3://sagemaker-us-east-1-587322031938/cicd/pipeline-scripts/v1//ci_check.py\n",
      "s3://sagemaker-us-east-1-587322031938/cicd/pipeline-scripts/v1//evaluate.py\n",
      "s3://sagemaker-us-east-1-587322031938/cicd/pipeline-scripts/v1/ci_check.py\n",
      "s3://sagemaker-us-east-1-587322031938/cicd/pipeline-scripts/v1/evaluate.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# IMPORTANT: no trailing slash here\n",
    "s3_scripts_prefix = f\"s3://{bucket}/cicd/pipeline-scripts/v1\"\n",
    "\n",
    "# Upload local scripts -> s3://.../v1/ci_check.py and .../v1/evaluate.py\n",
    "wr.s3.upload(local_file=\"pipeline_scripts/ci_check.py\", path=f\"{s3_scripts_prefix}/ci_check.py\")\n",
    "wr.s3.upload(local_file=\"pipeline_scripts/evaluate.py\", path=f\"{s3_scripts_prefix}/evaluate.py\")\n",
    "\n",
    "# Verify exact keys exist (single slash)\n",
    "objs = wr.s3.list_objects(s3_scripts_prefix)\n",
    "print(\"Found:\", len(objs))\n",
    "for o in objs:\n",
    "    if o.endswith(\"ci_check.py\") or o.endswith(\"evaluate.py\"):\n",
    "        print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "5565f81c-d968-4fab-af49-184d98a042ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_scripts_prefix = f\"s3://{bucket}/cicd/pipeline-scripts/v1\"\n",
    "ci_script_s3  = f\"{s3_scripts_prefix}/ci_check.py\"\n",
    "eval_script_s3 = f\"{s3_scripts_prefix}/evaluate.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "57570dd0-895d-4625-87e2-fcd3c7db56fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput\n",
    "\n",
    "ci_inputs = [\n",
    "    ProcessingInput(\n",
    "        source=ci_script_s3,\n",
    "        destination=\"/opt/ml/processing/input/code/ci_check.py\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "a4b14ecc-bbd6-4ad4-a8b8-8c373adfeb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n",
      "INFO:sagemaker.image_uris:Defaulting to only supported image scope: cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ci_script_s3: s3://sagemaker-us-east-1-587322031938/cicd/pipeline-scripts/v1/ci_check.py exists: True\n",
      "eval_script_s3: s3://sagemaker-us-east-1-587322031938/cicd/pipeline-scripts/v1/evaluate.py exists: True\n",
      "pipeline_sess + ci_processor ready\n"
     ]
    }
   ],
   "source": [
    "# M6 CLEAN FOUNDATION CELL\n",
    "\n",
    "import time, boto3, sagemaker\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "from sagemaker.processing import ScriptProcessor, ProcessingInput\n",
    "from sagemaker import image_uris\n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "role   = sagemaker.get_execution_role()\n",
    "\n",
    "# --- canonical script URIs (NO double slashes) ---\n",
    "s3_scripts_prefix = f\"s3://{bucket}/cicd/pipeline-scripts/v1\"\n",
    "ci_script_s3      = f\"{s3_scripts_prefix}/ci_check.py\"\n",
    "eval_script_s3    = f\"{s3_scripts_prefix}/evaluate.py\"\n",
    "\n",
    "# --- quick existence check (prevents \"No S3 objects found\") ---\n",
    "s3 = boto3.client(\"s3\")\n",
    "def s3_exists(s3_uri: str) -> bool:\n",
    "    assert s3_uri.startswith(\"s3://\")\n",
    "    b, k = s3_uri[5:].split(\"/\", 1)\n",
    "    try:\n",
    "        s3.head_object(Bucket=b, Key=k)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "print(\"ci_script_s3:\", ci_script_s3, \"exists:\", s3_exists(ci_script_s3))\n",
    "print(\"eval_script_s3:\", eval_script_s3, \"exists:\", s3_exists(eval_script_s3))\n",
    "\n",
    "# --- pipeline session (important) ---\n",
    "pipeline_sess = PipelineSession()\n",
    "\n",
    "# --- processor used by CIChecks step ---\n",
    "sklearn_image = image_uris.retrieve(\"sklearn\", region=region, version=\"1.2-1\")\n",
    "\n",
    "ci_processor = ScriptProcessor(\n",
    "    image_uri=sklearn_image,\n",
    "    command=[\"python3\"],\n",
    "    role=role,\n",
    "    instance_type=\"ml.m5.large\",   # budget-friendly\n",
    "    instance_count=1,\n",
    "    sagemaker_session=pipeline_sess\n",
    ")\n",
    "\n",
    "print(\"pipeline_sess + ci_processor ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "d65468a4-b1fd-4b20-aba0-706c1243a768",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserted pipeline: AAI540-Olist-CICD-Demo\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingOutput\n",
    "from sagemaker.workflow.parameters import ParameterString\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "# ---- Parameters ----\n",
    "ForceFailCI = ParameterString(name=\"ForceFailCI\", default_value=\"false\")\n",
    "\n",
    "# Use the exact S3 URIs that you already verified exist\n",
    "ci_script_s3   = f\"s3://{bucket}/cicd/pipeline-scripts/v1/ci_check.py\"\n",
    "eval_script_s3 = f\"s3://{bucket}/cicd/pipeline-scripts/v1/evaluate.py\"\n",
    "\n",
    "ci_out_s3   = f\"s3://{bucket}/cicd/artifacts/ci/\"\n",
    "eval_out_s3 = f\"s3://{bucket}/cicd/artifacts/eval/\"\n",
    "\n",
    "# ---- CIChecks step ----\n",
    "ci_step_args = ci_processor.run(\n",
    "    code=ci_script_s3,\n",
    "    outputs=[ProcessingOutput(source=\"/opt/ml/processing/output\", destination=ci_out_s3)],\n",
    "    # FIX: match what ci_check.py expects (per CloudWatch: --ForceFailCI)\n",
    "    arguments=[\"--ForceFailCI\", ForceFailCI],\n",
    ")\n",
    "\n",
    "ci_step = ProcessingStep(name=\"CIChecks\", step_args=ci_step_args)\n",
    "\n",
    "# ---- Evaluate step ----\n",
    "eval_step_args = ci_processor.run(\n",
    "    code=eval_script_s3,\n",
    "    outputs=[ProcessingOutput(source=\"/opt/ml/processing/output\", destination=eval_out_s3)],\n",
    ")\n",
    "\n",
    "eval_step = ProcessingStep(name=\"Evaluate\", step_args=eval_step_args)\n",
    "\n",
    "pipeline_name = \"AAI540-Olist-CICD-Demo\"\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[ForceFailCI],\n",
    "    steps=[ci_step, eval_step],\n",
    "    sagemaker_session=pipeline_sess,\n",
    ")\n",
    "\n",
    "pipeline.upsert(role_arn=role)\n",
    "print(\"Upserted pipeline:\", pipeline_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "8146c050-326c-4721-aad1-6013da98ce5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_PipelineExecution(arn='arn:aws:sagemaker:us-east-1:587322031938:pipeline/AAI540-Olist-CICD-Demo/execution/mg2nj0wy64ye', sagemaker_session=<sagemaker.workflow.pipeline_context.PipelineSession object at 0x7fae6595e4e0>)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FAILED run (CI fails)\n",
    "pipeline.start(parameters={\"ForceFailCI\": \"true\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "dce88e98-fc00-4198-b0e7-49d48c6561f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_PipelineExecution(arn='arn:aws:sagemaker:us-east-1:587322031938:pipeline/AAI540-Olist-CICD-Demo/execution/cuplyhtomf2u', sagemaker_session=<sagemaker.workflow.pipeline_context.PipelineSession object at 0x7fae6595e4e0>)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SUCCESS run (CI passes)\n",
    "pipeline.start(parameters={\"ForceFailCI\": \"false\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "a6eb63cc-83a1-419f-9db7-262dae0f7908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated local: pipeline_scripts/evaluate.py\n",
      "Re-uploaded scripts to: s3://sagemaker-us-east-1-587322031938/cicd/pipeline-scripts/v1/\n",
      "eval_script_s3 should be: s3://sagemaker-us-east-1-587322031938/cicd/pipeline-scripts/v1/evaluate.py\n"
     ]
    }
   ],
   "source": [
    "# Rewrite evaluate.py to NOT require xgboost, then re-upload to the SAME S3 path\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# local script path\n",
    "scripts_dir = Path(\"./pipeline_scripts\")\n",
    "scripts_dir.mkdir(exist_ok=True)\n",
    "eval_path = scripts_dir / \"evaluate.py\"\n",
    "\n",
    "eval_path.write_text(\n",
    "\"\"\"#!/usr/bin/env python3\n",
    "import json, os, time\n",
    "\n",
    "# Minimal \"evaluation\" for CI/CD demo:\n",
    "# - Writes a metrics.json that the pipeline can consume\n",
    "# - No xgboost dependency (keeps processing image lightweight)\n",
    "\n",
    "# If you want this to fail, you can set env FORCE_EVAL_FAIL=true later (optional)\n",
    "force_fail = os.environ.get(\"FORCE_EVAL_FAIL\", \"false\").lower() == \"true\"\n",
    "if force_fail:\n",
    "    raise RuntimeError(\"Forced Evaluate failure (FORCE_EVAL_FAIL=true)\")\n",
    "\n",
    "# Demo metric (stable, deterministic)\n",
    "metrics = {\n",
    "    \"auc\": 0.60,\n",
    "    \"timestamp\": int(time.time())\n",
    "}\n",
    "\n",
    "out_dir = \"/opt/ml/processing/output\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "with open(os.path.join(out_dir, \"metrics.json\"), \"w\") as f:\n",
    "    json.dump(metrics, f)\n",
    "\n",
    "print(\"Wrote metrics:\", metrics)\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(\"Updated local:\", str(eval_path))\n",
    "\n",
    "# re-upload scripts to the exact S3 prefix you're using\n",
    "sess = sagemaker.Session()\n",
    "s3_scripts_uri = f\"s3://{bucket}/cicd/pipeline-scripts/v1/\"\n",
    "sess.upload_data(path=str(scripts_dir), bucket=bucket, key_prefix=\"cicd/pipeline-scripts/v1\")\n",
    "print(\"Re-uploaded scripts to:\", s3_scripts_uri)\n",
    "print(\"eval_script_s3 should be:\", s3_scripts_uri + \"evaluate.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "4ce069e6-c4ab-48a2-9271-6ab5ece4e07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started: arn:aws:sagemaker:us-east-1:587322031938:pipeline/AAI540-Olist-CICD-Demo/execution/703felxm4no1\n"
     ]
    }
   ],
   "source": [
    "execution_ok = pipeline.start(\n",
    "    parameters={\n",
    "        \"ForceFailCI\": \"false\",\n",
    "        \n",
    "    }\n",
    ")\n",
    "print(\"Started:\", execution_ok.arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "a0d441f5-77e3-40a2-9a0d-75766753418b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started FAIL: arn:aws:sagemaker:us-east-1:587322031938:pipeline/AAI540-Olist-CICD-Demo/execution/qrpv8gi9psii\n"
     ]
    }
   ],
   "source": [
    "execution_fail = pipeline.start(\n",
    "    parameters={\n",
    "        \"ForceFailCI\": \"true\",\n",
    "    }\n",
    ")\n",
    "print(\"Started FAIL:\", execution_fail.arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d0732e-b45c-4719-b7d2-aa4e6f5ab621",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}