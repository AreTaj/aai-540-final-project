{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69584688-1464-4b46-be61-06a594249ce9",
   "metadata": {},
   "source": [
    "# MLOps Pipeline Workflow & Team Notes\n",
    "\n",
    "## Project Overview\n",
    "This notebook implements an end-to-end MLOps data pipeline using the **Olist Brazilian E-Commerce dataset**. The goal was to demonstrate a production-style workflow that covers data ingestion, cataloging, exploratory analysis, feature engineering, feature storage, and dataset splitting — all using AWS services in a cost-efficient way.\n",
    "\n",
    "The pipeline was intentionally built step-by-step to mirror MLOps practices rather than a one-off modeling notebook.\n",
    "\n",
    "---\n",
    "\n",
    "## High-Level Workflow\n",
    "\n",
    "### 1. Raw Data Ingestion (S3 Data Lake)\n",
    "- Created an S3 bucket to act as a data lake.\n",
    "- Uploaded all **9 raw CSV files** from the Kaggle Olist dataset.\n",
    "- Organized raw data under: s3:///raw/olist/ingest_date=YYYY-MM-DD/\n",
    "- Each dataset was placed into its own subfolder to support Athena’s directory-based table requirements.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Data Cataloging & Querying (Athena)\n",
    "- Created an Athena database (`olist_datalake`).\n",
    "- Defined **external tables** for each dataset directly from JupyterLab (no Glue crawler required).\n",
    "- Verified schemas and row counts using Athena queries.\n",
    "- This step enabled SQL-based access to the data and served as the cataloging layer for downstream analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Exploratory Data Analysis (SageMaker + Pandas)\n",
    "- Loaded Athena tables into Pandas using `awswrangler`.\n",
    "- Performed sanity checks on row counts and joins.\n",
    "- Built an **order-level analytical view** by aggregating:\n",
    "- order items\n",
    "- payments\n",
    "- customer attributes\n",
    "- Engineered a target variable (`is_late`) based on delivery vs. estimated delivery dates.\n",
    "- Observed class imbalance (~8% late deliveries), motivating careful splitting and evaluation later.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Feature Engineering\n",
    "- Created leakage-safe, order-level features using only information available at purchase time:\n",
    "- pricing, freight, number of items/sellers\n",
    "- payment information\n",
    "- time-based features (day of week, hour of day)\n",
    "- customer state\n",
    "- Maintained a **canonical feature dataset** for analysis and splitting.\n",
    "- Created a **Feature Store–compatible version** with strict data types.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. SageMaker Feature Store (Offline Store)\n",
    "- Created a **SageMaker Feature Group** (offline store only to control cost).\n",
    "- Used `order_id` as the record identifier.\n",
    "- Used a strictly formatted ISO-8601 `event_time` with UTC (`Z`) as the event time feature.\n",
    "- Successfully ingested ~99k feature records into Feature Store.\n",
    "- Offline store data is persisted in S3 for training and future reuse.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Dataset Splitting (Time-Based)\n",
    "- Performed a **time-based split** using `event_time` to avoid temporal leakage:\n",
    "- Train: ~40%\n",
    "- Validation: ~10%\n",
    "- Test: ~10%\n",
    "- Production reserve: ~40%\n",
    "- Persisted each split as Parquet files to: s3:///splits/olist/features/version=v1/\n",
    "- This mirrors a real production setup where recent data is reserved for inference.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Engineering Decisions & Lessons Learned\n",
    "\n",
    "- **Athena LOCATION must point to directories**, not individual files.\n",
    "- **Feature Store requires strict ISO-8601 timestamps with timezone** — missing the `Z` suffix causes ingestion failures.\n",
    "- Maintaining separate:\n",
    "- canonical feature data (analysis-friendly)\n",
    "- Feature Store–safe data (schema-restricted)\n",
    "is a best practice in real MLOps systems.\n",
    "- Time-based splitting is critical to avoid data leakage in temporal datasets.\n",
    "- Offline Feature Store provides the required functionality while minimizing cost.\n",
    "\n",
    "---\n",
    "\n",
    "## Cost Management Notes\n",
    "- SageMaker compute was stopped immediately after completion.\n",
    "- Feature Store **online store was intentionally disabled** to avoid ongoing charges.\n",
    "- S3 storage costs are minimal and safe to keep until final submission.\n",
    "- Cleanup (Feature Group deletion, S3 cleanup) should only be done **after submission**.\n",
    "\n",
    "---\n",
    "\n",
    "## For Teammates\n",
    "If you need to re-run or extend this work:\n",
    "1. Start at the Athena read step (no need to re-upload raw data).\n",
    "2. Do **not** re-run ingestion unless changing the feature schema.\n",
    "3. Always stop SageMaker compute when finished.\n",
    "\n",
    "This notebook represents a complete, MLOps data pipeline.\n",
    "\n",
    "## Model Benchmark and Evaluation (Module 4)\n",
    "\n",
    "### Baseline Model\n",
    "As a benchmark, we implemented a simple heuristic model that always predicts an order will be delivered on time. This reflects the majority class in the dataset and establishes a lower bound for model performance.\n",
    "\n",
    "Due to class imbalance (~86% of orders are not late), the baseline achieves high accuracy but fails to identify late deliveries, resulting in zero precision, recall, and F1-score.\n",
    "\n",
    "### First Iteration Model (XGBoost v1)\n",
    "We trained a first-pass XGBoost binary classifier in Amazon SageMaker using a limited set of engineered features related to order size, payment behavior, and purchase timing.\n",
    "\n",
    "The model was evaluated using SageMaker Batch Transform on the held-out test dataset. Batch Transform was selected over a real-time endpoint to minimize cost and ensure automatic resource cleanup.\n",
    "\n",
    "### Results Summary\n",
    "- The XGBoost model achieved an AUC of approximately **0.56**, indicating it learned some discriminative signal beyond random chance.\n",
    "- Overall accuracy matched the baseline model due to class imbalance and use of a default classification threshold.\n",
    "- Precision, recall, and F1-score remained low, highlighting the need for future improvements such as class weighting, threshold tuning, and feature expansion.\n",
    "\n",
    "### Key Takeaways\n",
    "This iteration establishes a complete, cost-aware MLOps workflow including data ingestion, feature engineering, model training, evaluation, and deployment via batch inference. While performance improvements are needed, this version serves as a strong baseline for future model iterations and CI/CD integration in later modules.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a684ab4-5185-4603-9f30-358b125dcba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "import sagemaker\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "prefix = \"raw/olist/ingest_date=2026-01-25/\"\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "resp = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "\n",
    "keys = [obj[\"Key\"] for obj in resp.get(\"Contents\", [])]\n",
    "print(\"Found files:\", len(keys))\n",
    "for k in keys:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9273a5f2-6a04-48e3-91fc-89dd28b3cd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "results_prefix = \"athena-results/\"\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "resp = s3.list_objects_v2(Bucket=bucket, Prefix=results_prefix, MaxKeys=1)\n",
    "\n",
    "if \"Contents\" in resp:\n",
    "    print(\"✅ Athena results prefix exists:\", f\"s3://{bucket}/{results_prefix}\")\n",
    "else:\n",
    "    # create a zero-byte object so the prefix exists\n",
    "    s3.put_object(Bucket=bucket, Key=results_prefix)\n",
    "    print(\"✅ Created Athena results prefix:\", f\"s3://{bucket}/{results_prefix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca03c11-54ff-441e-bf5e-fd9c05665d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import boto3\n",
    "\n",
    "REGION = boto3.session.Session().region_name\n",
    "athena = boto3.client(\"athena\", region_name=REGION)\n",
    "\n",
    "ATHENA_OUTPUT = f\"s3://{bucket}/athena-results/\"\n",
    "DB = \"olist_datalake\"\n",
    "\n",
    "def run_athena(sql: str, database: str = \"default\"):\n",
    "    res = athena.start_query_execution(\n",
    "        QueryString=sql,\n",
    "        QueryExecutionContext={\"Database\": database},\n",
    "        ResultConfiguration={\"OutputLocation\": ATHENA_OUTPUT},\n",
    "    )\n",
    "    qid = res[\"QueryExecutionId\"]\n",
    "    while True:\n",
    "        q = athena.get_query_execution(QueryExecutionId=qid)\n",
    "        state = q[\"QueryExecution\"][\"Status\"][\"State\"]\n",
    "        if state in (\"SUCCEEDED\", \"FAILED\", \"CANCELLED\"):\n",
    "            break\n",
    "        time.sleep(1)\n",
    "    if state != \"SUCCEEDED\":\n",
    "        reason = q[\"QueryExecution\"][\"Status\"].get(\"StateChangeReason\", \"Unknown\")\n",
    "        raise RuntimeError(f\"Athena query failed: {state} - {reason}\\nSQL:\\n{sql}\")\n",
    "    return qid\n",
    "\n",
    "run_athena(f\"CREATE DATABASE IF NOT EXISTS {DB};\", database=\"default\")\n",
    "print(\"✅ Database ready:\", DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e977a0a3-98b4-4b4a-90b6-d6b93a85f392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "base_prefix = \"raw/olist/ingest_date=2026-01-25/\"\n",
    "\n",
    "files = [\n",
    "    \"olist_customers_dataset.csv\",\n",
    "    \"olist_geolocation_dataset.csv\",\n",
    "    \"olist_order_items_dataset.csv\",\n",
    "    \"olist_order_payments_dataset.csv\",\n",
    "    \"olist_order_reviews_dataset.csv\",\n",
    "    \"olist_orders_dataset.csv\",\n",
    "    \"olist_products_dataset.csv\",\n",
    "    \"olist_sellers_dataset.csv\",\n",
    "    \"product_category_name_translation.csv\",\n",
    "]\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "for f in files:\n",
    "    src_key = base_prefix + f\n",
    "    folder = f.replace(\".csv\", \"\")  # folder name = file name without .csv\n",
    "    dst_key = f\"{base_prefix}{folder}/{f}\"\n",
    "    \n",
    "    # copy\n",
    "    s3.copy_object(\n",
    "        Bucket=bucket,\n",
    "        CopySource={\"Bucket\": bucket, \"Key\": src_key},\n",
    "        Key=dst_key\n",
    "    )\n",
    "    print(\"✅ Copied to:\", dst_key)\n",
    "\n",
    "print(\"\\nDone. Next we’ll point Athena tables at these folders.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858ee701-16e0-4030-bd4c-c966fe47a717",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = s3.list_objects_v2(Bucket=bucket, Prefix=base_prefix, MaxKeys=50)\n",
    "for obj in resp.get(\"Contents\", []):\n",
    "    print(obj[\"Key\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29e34fd-478d-486e-ab8e-bf2105b59b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_BASE = f\"s3://{bucket}/{base_prefix}\"\n",
    "\n",
    "def create_csv_table(table_name: str, columns_ddl: str, folder_name: str):\n",
    "    sql = f\"\"\"\n",
    "    CREATE EXTERNAL TABLE IF NOT EXISTS {DB}.{table_name} (\n",
    "      {columns_ddl}\n",
    "    )\n",
    "    ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
    "    WITH SERDEPROPERTIES (\n",
    "      'separatorChar' = ',',\n",
    "      'quoteChar'     = '\\\"',\n",
    "      'escapeChar'    = '\\\\\\\\'\n",
    "    )\n",
    "    STORED AS TEXTFILE\n",
    "    LOCATION '{RAW_BASE}{folder_name}/'\n",
    "    TBLPROPERTIES ('skip.header.line.count'='1');\n",
    "    \"\"\"\n",
    "    run_athena(sql, database=DB)\n",
    "    print(f\"✅ Created: {DB}.{table_name}\")\n",
    "\n",
    "create_csv_table(\n",
    "    \"olist_customers_dataset\",\n",
    "    \"\"\"\n",
    "    customer_id string,\n",
    "    customer_unique_id string,\n",
    "    customer_zip_code_prefix int,\n",
    "    customer_city string,\n",
    "    customer_state string\n",
    "    \"\"\",\n",
    "    \"olist_customers_dataset\"\n",
    ")\n",
    "\n",
    "create_csv_table(\n",
    "    \"olist_geolocation_dataset\",\n",
    "    \"\"\"\n",
    "    geolocation_zip_code_prefix int,\n",
    "    geolocation_lat double,\n",
    "    geolocation_lng double,\n",
    "    geolocation_city string,\n",
    "    geolocation_state string\n",
    "    \"\"\",\n",
    "    \"olist_geolocation_dataset\"\n",
    ")\n",
    "\n",
    "create_csv_table(\n",
    "    \"olist_order_items_dataset\",\n",
    "    \"\"\"\n",
    "    order_id string,\n",
    "    order_item_id int,\n",
    "    product_id string,\n",
    "    seller_id string,\n",
    "    shipping_limit_date string,\n",
    "    price double,\n",
    "    freight_value double\n",
    "    \"\"\",\n",
    "    \"olist_order_items_dataset\"\n",
    ")\n",
    "\n",
    "create_csv_table(\n",
    "    \"olist_order_payments_dataset\",\n",
    "    \"\"\"\n",
    "    order_id string,\n",
    "    payment_sequential int,\n",
    "    payment_type string,\n",
    "    payment_installments int,\n",
    "    payment_value double\n",
    "    \"\"\",\n",
    "    \"olist_order_payments_dataset\"\n",
    ")\n",
    "\n",
    "create_csv_table(\n",
    "    \"olist_order_reviews_dataset\",\n",
    "    \"\"\"\n",
    "    review_id string,\n",
    "    order_id string,\n",
    "    review_score int,\n",
    "    review_comment_title string,\n",
    "    review_comment_message string,\n",
    "    review_creation_date string,\n",
    "    review_answer_timestamp string\n",
    "    \"\"\",\n",
    "    \"olist_order_reviews_dataset\"\n",
    ")\n",
    "\n",
    "create_csv_table(\n",
    "    \"olist_orders_dataset\",\n",
    "    \"\"\"\n",
    "    order_id string,\n",
    "    customer_id string,\n",
    "    order_status string,\n",
    "    order_purchase_timestamp string,\n",
    "    order_approved_at string,\n",
    "    order_delivered_carrier_date string,\n",
    "    order_delivered_customer_date string,\n",
    "    order_estimated_delivery_date string\n",
    "    \"\"\",\n",
    "    \"olist_orders_dataset\"\n",
    ")\n",
    "\n",
    "create_csv_table(\n",
    "    \"olist_products_dataset\",\n",
    "    \"\"\"\n",
    "    product_id string,\n",
    "    product_category_name string,\n",
    "    product_name_lenght int,\n",
    "    product_description_lenght int,\n",
    "    product_photos_qty int,\n",
    "    product_weight_g int,\n",
    "    product_length_cm int,\n",
    "    product_height_cm int,\n",
    "    product_width_cm int\n",
    "    \"\"\",\n",
    "    \"olist_products_dataset\"\n",
    ")\n",
    "\n",
    "create_csv_table(\n",
    "    \"olist_sellers_dataset\",\n",
    "    \"\"\"\n",
    "    seller_id string,\n",
    "    seller_zip_code_prefix int,\n",
    "    seller_city string,\n",
    "    seller_state string\n",
    "    \"\"\",\n",
    "    \"olist_sellers_dataset\"\n",
    ")\n",
    "\n",
    "create_csv_table(\n",
    "    \"product_category_name_translation\",\n",
    "    \"\"\"\n",
    "    product_category_name string,\n",
    "    product_category_name_english string\n",
    "    \"\"\",\n",
    "    \"product_category_name_translation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393bfe94-4366-42c4-b48c-8a556f415750",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_athena(f\"SHOW TABLES IN {DB};\", database=DB)\n",
    "print(\"✅ SHOW TABLES succeeded\")\n",
    "\n",
    "run_athena(f\"SELECT COUNT(*) FROM {DB}.olist_orders_dataset;\", database=DB)\n",
    "print(\"✅ COUNT orders succeeded\")\n",
    "\n",
    "run_athena(f\"SELECT order_status, COUNT(*) c FROM {DB}.olist_orders_dataset GROUP BY 1 ORDER BY c DESC;\", database=DB)\n",
    "print(\"✅ GROUP BY order_status succeeded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d48e162-12df-4319-ba71-f3cf7f1412a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6.1\n",
    "import awswrangler as wr\n",
    "import pandas as pd\n",
    "\n",
    "DB = \"olist_datalake\"\n",
    "\n",
    "orders = wr.athena.read_sql_query(\n",
    "    sql=f\"SELECT * FROM {DB}.olist_orders_dataset\",\n",
    "    database=DB,\n",
    "    ctas_approach=False\n",
    ")\n",
    "\n",
    "order_items = wr.athena.read_sql_query(\n",
    "    sql=f\"SELECT * FROM {DB}.olist_order_items_dataset\",\n",
    "    database=DB,\n",
    "    ctas_approach=False\n",
    ")\n",
    "\n",
    "payments = wr.athena.read_sql_query(\n",
    "    sql=f\"SELECT * FROM {DB}.olist_order_payments_dataset\",\n",
    "    database=DB,\n",
    "    ctas_approach=False\n",
    ")\n",
    "\n",
    "customers = wr.athena.read_sql_query(\n",
    "    sql=f\"SELECT * FROM {DB}.olist_customers_dataset\",\n",
    "    database=DB,\n",
    "    ctas_approach=False\n",
    ")\n",
    "\n",
    "print(\"orders:\", orders.shape)\n",
    "print(\"order_items:\", order_items.shape)\n",
    "print(\"payments:\", payments.shape)\n",
    "print(\"customers:\", customers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfa6170-a17c-4f5c-8cf7-8770b8b46166",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6.2\n",
    "# Parse timestamps\n",
    "timestamp_cols = [\n",
    "    \"order_purchase_timestamp\",\n",
    "    \"order_approved_at\",\n",
    "    \"order_delivered_carrier_date\",\n",
    "    \"order_delivered_customer_date\",\n",
    "    \"order_estimated_delivery_date\",\n",
    "]\n",
    "for col in timestamp_cols:\n",
    "    orders[col] = pd.to_datetime(orders[col], errors=\"coerce\")\n",
    "\n",
    "# Aggregations\n",
    "items_agg = (\n",
    "    order_items.groupby(\"order_id\")\n",
    "    .agg(\n",
    "        num_items=(\"order_item_id\", \"count\"),\n",
    "        total_price=(\"price\", \"sum\"),\n",
    "        total_freight_value=(\"freight_value\", \"sum\"),\n",
    "        num_sellers=(\"seller_id\", \"nunique\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "payments_agg = (\n",
    "    payments.groupby(\"order_id\")\n",
    "    .agg(\n",
    "        payment_value=(\"payment_value\", \"sum\"),\n",
    "        payment_installments=(\"payment_installments\", \"max\"),\n",
    "        payment_type=(\"payment_type\", lambda x: x.value_counts().index[0]),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "eda_df = (\n",
    "    orders\n",
    "    .merge(items_agg, on=\"order_id\", how=\"left\")\n",
    "    .merge(payments_agg, on=\"order_id\", how=\"left\")\n",
    "    .merge(customers[[\"customer_id\", \"customer_state\"]], on=\"customer_id\", how=\"left\")\n",
    ")\n",
    "\n",
    "# Time features\n",
    "eda_df[\"purchase_dow\"] = eda_df[\"order_purchase_timestamp\"].dt.dayofweek\n",
    "eda_df[\"purchase_hour\"] = eda_df[\"order_purchase_timestamp\"].dt.hour\n",
    "\n",
    "# Label: late delivery\n",
    "eda_df[\"is_late\"] = (\n",
    "    (eda_df[\"order_delivered_customer_date\"].notna()) &\n",
    "    (eda_df[\"order_estimated_delivery_date\"].notna()) &\n",
    "    (eda_df[\"order_delivered_customer_date\"] > eda_df[\"order_estimated_delivery_date\"])\n",
    ").astype(int)\n",
    "\n",
    "print(\"Orders rows:\", len(orders))\n",
    "print(\"EDA rows:\", len(eda_df))\n",
    "print(\"Row loss:\", len(orders) - len(eda_df))\n",
    "print(\"Late rate:\\n\", eda_df[\"is_late\"].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c44e50-a27a-4052-b192-82f344558f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7.0 + 7B\n",
    "# Canonical features (with purchase timestamp)\n",
    "feat = eda_df[[\n",
    "    \"order_id\",\n",
    "    \"order_purchase_timestamp\",\n",
    "    \"customer_state\",\n",
    "    \"num_items\",\n",
    "    \"total_price\",\n",
    "    \"total_freight_value\",\n",
    "    \"num_sellers\",\n",
    "    \"payment_value\",\n",
    "    \"payment_installments\",\n",
    "    \"payment_type\",\n",
    "    \"purchase_dow\",\n",
    "    \"purchase_hour\",\n",
    "    \"is_late\"\n",
    "]].copy()\n",
    "\n",
    "feat[\"customer_state\"] = feat[\"customer_state\"].fillna(\"unknown\").astype(str)\n",
    "feat[\"payment_type\"] = feat[\"payment_type\"].fillna(\"unknown\").astype(str)\n",
    "\n",
    "for c in [\"num_items\", \"num_sellers\", \"payment_installments\", \"purchase_dow\", \"purchase_hour\", \"is_late\"]:\n",
    "    feat[c] = feat[c].fillna(0).astype(int)\n",
    "\n",
    "for c in [\"total_price\", \"total_freight_value\", \"payment_value\"]:\n",
    "    feat[c] = feat[c].fillna(0.0).astype(float)\n",
    "\n",
    "# Create strict ISO-8601 event time WITH timezone \"Z\"\n",
    "feat[\"event_time\"] = (\n",
    "    pd.to_datetime(feat[\"order_purchase_timestamp\"], errors=\"coerce\")\n",
    "    .dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    ")\n",
    "\n",
    "feat = feat.dropna(subset=[\"order_id\", \"event_time\"]).reset_index(drop=True)\n",
    "\n",
    "# FeatureStore-safe version (remove datetime64 column)\n",
    "feat_fs = feat.drop(columns=[\"order_purchase_timestamp\"]).copy()\n",
    "feat_fs[\"event_time\"] = feat_fs[\"event_time\"].astype(str)\n",
    "\n",
    "print(\"✅ feat shape:\", feat.shape)\n",
    "print(\"✅ feat_fs shape:\", feat_fs.shape)\n",
    "print(\"event_time sample:\", feat_fs[\"event_time\"].head().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f15a681-f32e-4103-be35-fab9b0646be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7.1\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "\n",
    "bucket = \"aai540-olist-mlops-chris-7f3k2p\"\n",
    "region = boto3.session.Session().region_name\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "feature_group_name = \"olist-order-features-v1\"\n",
    "offline_store_s3_uri = f\"s3://{bucket}/feature-store/{feature_group_name}/\"\n",
    "\n",
    "fg = FeatureGroup(name=feature_group_name, sagemaker_session=sess)\n",
    "\n",
    "print(\"Region:\", region)\n",
    "print(\"Role:\", role)\n",
    "print(\"Offline store URI:\", offline_store_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b652859d-e72d-4a6b-8af1-72af458ed97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7.2\n",
    "import time\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "sm = boto3.client(\"sagemaker\", region_name=region)\n",
    "\n",
    "def feature_group_exists(name: str) -> bool:\n",
    "    try:\n",
    "        sm.describe_feature_group(FeatureGroupName=name)\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        if \"ResourceNotFound\" in str(e):\n",
    "            return False\n",
    "        raise\n",
    "\n",
    "def wait_for_fg_created(name: str, timeout_sec: int = 600, poll_sec: int = 10):\n",
    "    start = time.time()\n",
    "    while True:\n",
    "        desc = sm.describe_feature_group(FeatureGroupName=name)\n",
    "        status = desc.get(\"FeatureGroupStatus\")\n",
    "        offline_status = desc.get(\"OfflineStoreStatus\", {}).get(\"Status\", \"UNKNOWN\")\n",
    "        print(f\"Status={status}, OfflineStoreStatus={offline_status}\")\n",
    "        if status == \"Created\" and offline_status in (\"Active\", \"UNKNOWN\"):\n",
    "            return desc\n",
    "        if status in (\"CreateFailed\", \"DeleteFailed\"):\n",
    "            raise RuntimeError(f\"Feature Group failed with status={status}. Details: {desc}\")\n",
    "        if time.time() - start > timeout_sec:\n",
    "            raise TimeoutError(f\"Timed out waiting for Feature Group to be Created: {name}\")\n",
    "        time.sleep(poll_sec)\n",
    "\n",
    "if feature_group_exists(feature_group_name):\n",
    "    print(f\"✅ Feature Group already exists: {feature_group_name}\")\n",
    "else:\n",
    "    fg.load_feature_definitions(data_frame=feat_fs)\n",
    "    fg.create(\n",
    "        s3_uri=offline_store_s3_uri,\n",
    "        record_identifier_name=\"order_id\",\n",
    "        event_time_feature_name=\"event_time\",\n",
    "        role_arn=role,\n",
    "        enable_online_store=False,\n",
    "    )\n",
    "    print(\"⏳ Create request submitted\")\n",
    "\n",
    "wait_for_fg_created(feature_group_name)\n",
    "print(\"✅ Feature Group ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2460fd33-5601-4e33-a636-da6a6ce54fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7.3\n",
    "ingest_response = fg.ingest(data_frame=feat_fs, max_workers=2, wait=True)\n",
    "print(\"✅ Ingest complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1afae01-1c16-48a2-ab19-d0115ae85d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7.4\n",
    "import boto3\n",
    "\n",
    "sm = boto3.client(\"sagemaker\", region_name=region)\n",
    "desc = sm.describe_feature_group(FeatureGroupName=feature_group_name)\n",
    "\n",
    "print(\"FeatureGroupStatus:\", desc[\"FeatureGroupStatus\"])\n",
    "print(\"OfflineStoreStatus:\", desc[\"OfflineStoreStatus\"][\"Status\"])\n",
    "print(\"S3 Offline Store Uri:\", desc[\"OfflineStoreConfig\"][\"S3StorageConfig\"][\"S3Uri\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68adb030-ec45-4e9a-bd5a-abbbce5622ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8.0\n",
    "import awswrangler as wr\n",
    "\n",
    "bucket = \"aai540-olist-mlops-chris-7f3k2p\"\n",
    "\n",
    "# Use feat_fs for splits (Feature Store compatible)\n",
    "feat_sorted = feat_fs.sort_values(\"event_time\").reset_index(drop=True)\n",
    "n = len(feat_sorted)\n",
    "\n",
    "train_end = int(n * 0.40)\n",
    "val_end   = int(n * 0.50)\n",
    "test_end  = int(n * 0.60)\n",
    "\n",
    "train_df = feat_sorted.iloc[:train_end]\n",
    "val_df   = feat_sorted.iloc[train_end:val_end]\n",
    "test_df  = feat_sorted.iloc[val_end:test_end]\n",
    "prod_df  = feat_sorted.iloc[test_end:]\n",
    "\n",
    "print(\"✅ Split sizes\")\n",
    "print(\"train:\", len(train_df))\n",
    "print(\"val:  \", len(val_df))\n",
    "print(\"test: \", len(test_df))\n",
    "print(\"prod: \", len(prod_df))\n",
    "\n",
    "split_base = f\"s3://{bucket}/splits/olist/features/version=v1/\"\n",
    "wr.s3.to_parquet(train_df, f\"{split_base}train/\", dataset=True, mode=\"overwrite\")\n",
    "wr.s3.to_parquet(val_df,   f\"{split_base}val/\",   dataset=True, mode=\"overwrite\")\n",
    "wr.s3.to_parquet(test_df,  f\"{split_base}test/\",  dataset=True, mode=\"overwrite\")\n",
    "wr.s3.to_parquet(prod_df,  f\"{split_base}prod/\",  dataset=True, mode=\"overwrite\")\n",
    "\n",
    "print(\"✅ Wrote splits to:\", split_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ab440e-a4ea-477e-bad5-fc372615b00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5822e971-6f0e-4ddb-acb7-41898a947a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "prefix = \"splits/olist/features/version=v1/\"\n",
    "\n",
    "resp = s3.list_objects_v2(Bucket=bucket, Prefix=prefix, MaxKeys=50)\n",
    "print(\"Objects found:\", resp.get(\"KeyCount\", 0))\n",
    "for obj in resp.get(\"Contents\", [])[:20]:\n",
    "    print(obj[\"Key\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab04fb0-989f-4744-bbda-b33cb55b73ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#M4 Start\n",
    "try:\n",
    "    print(len(train))\n",
    "except NameError:\n",
    "    print(\"❌ Kernel is fresh — variables not loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0034099-aa6f-4045-a5b2-4375191fdacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import awswrangler as wr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "bucket = \"aai540-olist-mlops-chris-7f3k2p\"\n",
    "split_base = f\"s3://{bucket}/splits/olist/features/version=v1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9faaf5-2b66-4cc1-b3e1-7d0e8e541b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = wr.s3.read_parquet(f\"{split_base}train/\")\n",
    "val   = wr.s3.read_parquet(f\"{split_base}val/\")\n",
    "test  = wr.s3.read_parquet(f\"{split_base}test/\")\n",
    "\n",
    "print(train.shape, val.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4945ad-cf8d-4724-a7a2-9a62e20ad344",
   "metadata": {},
   "outputs": [],
   "source": [
    "#M4-1 Benchmark Model Baseline\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Ground truth\n",
    "y_test = test[\"is_late\"].astype(int)\n",
    "\n",
    "# Baseline prediction: always predict NOT late\n",
    "y_pred_baseline = pd.Series(0, index=test.index)\n",
    "\n",
    "baseline_metrics = {\n",
    "    \"accuracy\": accuracy_score(y_test, y_pred_baseline),\n",
    "    \"precision\": precision_score(y_test, y_pred_baseline, zero_division=0),\n",
    "    \"recall\": recall_score(y_test, y_pred_baseline, zero_division=0),\n",
    "    \"f1\": f1_score(y_test, y_pred_baseline, zero_division=0),\n",
    "}\n",
    "\n",
    "baseline_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e4bb0b-3070-47b1-9036-21f7f01bbbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#M4-2 1st Model Sage Maker\n",
    "\n",
    "FEATURES = [\n",
    "    \"num_items\",\n",
    "    \"total_price\",\n",
    "    \"total_freight_value\",\n",
    "    \"num_sellers\",\n",
    "    \"payment_value\",\n",
    "    \"payment_installments\",\n",
    "    \"purchase_dow\",\n",
    "    \"purchase_hour\",\n",
    "]\n",
    "\n",
    "def to_xgb_matrix(df):\n",
    "    out = df[[\"is_late\"] + FEATURES].copy()\n",
    "    for c in FEATURES:\n",
    "        out[c] = pd.to_numeric(out[c], errors=\"coerce\").fillna(0.0)\n",
    "    out[\"is_late\"] = out[\"is_late\"].astype(int)\n",
    "    return out\n",
    "\n",
    "train_xgb = to_xgb_matrix(train)\n",
    "val_xgb   = to_xgb_matrix(val)\n",
    "test_xgb  = to_xgb_matrix(test)\n",
    "\n",
    "train_xgb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07fbf40-aeb6-4aa5-ab61-cdb2068dba2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#M4-2-2\n",
    "## NOTE: CSVs already written prior to training — do not rerun\n",
    "xgb_prefix = f\"s3://{bucket}/modeling/xgb_v1/\"\n",
    "\n",
    "train_csv_uri = f\"{xgb_prefix}train/train.csv\"\n",
    "val_csv_uri   = f\"{xgb_prefix}val/val.csv\"\n",
    "test_csv_uri  = f\"{xgb_prefix}test/test.csv\"\n",
    "\n",
    "wr.s3.to_csv(train_xgb, train_csv_uri, index=False, header=False)\n",
    "wr.s3.to_csv(val_xgb,   val_csv_uri,   index=False, header=False)\n",
    "wr.s3.to_csv(test_xgb,  test_csv_uri,  index=False, header=False)\n",
    "\n",
    "print(\"Train:\", train_csv_uri)\n",
    "print(\"Val:  \", val_csv_uri)\n",
    "print(\"Test: \", test_csv_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db400ea4-a70a-457a-9441-2c4bb5b1e071",
   "metadata": {},
   "outputs": [],
   "source": [
    "#M4-2-3 Train Model\n",
    "# TRAINING CELL (DO NOT RERUN)\n",
    "# This cell was executed once to train the initial XGBoost model.\n",
    "# Re-running this cell will retrain the model and incur additional cost.\n",
    "# The trained model is reused below via attachment for evaluation and deployment.\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "# Built-in XGBoost container\n",
    "xgb_image = retrieve(\n",
    "    framework=\"xgboost\",\n",
    "    region=region,\n",
    "    version=\"1.7-1\"\n",
    ")\n",
    "\n",
    "output_path = f\"s3://{bucket}/modeling/xgb_v1/output/\"\n",
    "\n",
    "xgb = Estimator(\n",
    "    image_uri=xgb_image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",  # budget-friendly\n",
    "    output_path=output_path,\n",
    "    sagemaker_session=sess,\n",
    ")\n",
    "\n",
    "# Simple, reasonable first-pass hyperparameters\n",
    "xgb.set_hyperparameters(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\",\n",
    "    num_round=100,\n",
    "    max_depth=4,\n",
    "    eta=0.2,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    ")\n",
    "\n",
    "train_input = TrainingInput(train_csv_uri, content_type=\"text/csv\")\n",
    "val_input   = TrainingInput(val_csv_uri, content_type=\"text/csv\")\n",
    "\n",
    "#xgb.fit({\n",
    "#    \"train\": train_input,\n",
    "#    \"validation\": val_input\n",
    "#})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0151caee-82d9-4259-819d-bf0696a22481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M4-2-3b Attach to Existing Trained Model (No Retraining)\n",
    "import boto3, sagemaker\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.session.Session().region_name\n",
    "sm = boto3.client(\"sagemaker\", region_name=region)\n",
    "\n",
    "# Pick the most recent completed training job\n",
    "jobs = sm.list_training_jobs(SortBy=\"CreationTime\", SortOrder=\"Descending\", MaxResults=20)[\"TrainingJobSummaries\"]\n",
    "training_job_name = next(j[\"TrainingJobName\"] for j in jobs if j[\"TrainingJobStatus\"] == \"Completed\")\n",
    "print(\"✅ Using training job:\", training_job_name)\n",
    "\n",
    "# Recreate estimator and attach (no retraining)\n",
    "xgb_image = retrieve(framework=\"xgboost\", region=region, version=\"1.7-1\")\n",
    "xgb = Estimator(\n",
    "    image_uri=xgb_image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    output_path=f\"s3://{bucket}/modeling/xgb_v1/output/\",\n",
    "    sagemaker_session=sess,\n",
    ")\n",
    "xgb._current_job_name = training_job_name\n",
    "print(\"✅ Attached. Ready for Batch Transform.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574dce64-f186-495e-a51a-7c63553268cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M4-2-2b Recreate CSV URIs\n",
    "\n",
    "xgb_prefix = f\"s3://{bucket}/modeling/xgb_v1/\"\n",
    "\n",
    "train_csv_uri = f\"{xgb_prefix}train/train.csv\"\n",
    "val_csv_uri   = f\"{xgb_prefix}val/val.csv\"\n",
    "test_csv_uri  = f\"{xgb_prefix}test/test.csv\"\n",
    "\n",
    "print(\"Train:\", train_csv_uri)\n",
    "print(\"Val:  \", val_csv_uri)\n",
    "print(\"Test: \", test_csv_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bf01c7-3661-40d1-8cb6-9298677f125d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M4-3.0 Create inference-only TEST input (features only)\n",
    "# Code developed using ChatGPT (ChatGPT, 2024) as a paired programmer.\n",
    "\n",
    "# test_xgb currently has: is_late + 8 features\n",
    "test_infer = test_xgb.drop(columns=[\"is_late\"]).copy()\n",
    "\n",
    "test_infer_csv_uri = f\"{xgb_prefix}test/test_infer.csv\"\n",
    "\n",
    "# IMPORTANT: no header, no index\n",
    "wr.s3.to_csv(test_infer, test_infer_csv_uri, index=False, header=False)\n",
    "\n",
    "print(\"✅ Wrote inference CSV:\", test_infer_csv_uri)\n",
    "print(\"Shape (should be 9944 x 8):\", test_infer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fd6908-b378-4c81-924d-062f65c4215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M4-3.1 Batch Transform on TEST (Evaluation) - v2 (features-only)\n",
    "from sagemaker.transformer import Transformer\n",
    "\n",
    "test_transform_output_v2 = f\"s3://{bucket}/modeling/xgb_v1/batch-out/test_v2/\"\n",
    "\n",
    "transformer = Transformer(\n",
    "    model_name=model_name,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    output_path=test_transform_output_v2,\n",
    "    assemble_with=\"Line\",\n",
    "    accept=\"text/csv\"\n",
    ")\n",
    "\n",
    "print(\"⏳ Starting batch transform (features-only input)...\")\n",
    "transformer.transform(\n",
    "    data=test_infer_csv_uri,\n",
    "    content_type=\"text/csv\",\n",
    "    split_type=\"Line\"\n",
    ")\n",
    "\n",
    "transformer.wait()\n",
    "print(\"✅ Batch transform finished:\", test_transform_output_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c23b185-e888-4cb8-a1bf-1c2d89f33e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M4-3.2 Load predictions and evaluate\n",
    "import boto3\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "resp = s3.list_objects_v2(Bucket=bucket, Prefix=\"modeling/xgb_v1/batch-out/test_v2/\")\n",
    "keys = [o[\"Key\"] for o in resp.get(\"Contents\", [])]\n",
    "out_files = [k for k in keys if k.endswith(\".out\")]\n",
    "\n",
    "if not out_files:\n",
    "    raise RuntimeError(f\"No .out files found in test_v2 output. Keys seen: {keys[:10]}\")\n",
    "\n",
    "out_key = sorted(out_files)[-1]\n",
    "out_uri = f\"s3://{bucket}/{out_key}\"\n",
    "print(\"Reading predictions from:\", out_uri)\n",
    "\n",
    "pred_df = wr.s3.read_csv(out_uri, header=None)\n",
    "y_prob = pred_df[0].astype(float).reset_index(drop=True)\n",
    "\n",
    "y_true = test_xgb[\"is_late\"].astype(int).reset_index(drop=True)\n",
    "y_hat = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "model_metrics = {\n",
    "    \"auc\": roc_auc_score(y_true, y_prob),\n",
    "    \"accuracy\": accuracy_score(y_true, y_hat),\n",
    "    \"precision\": precision_score(y_true, y_hat, zero_division=0),\n",
    "    \"recall\": recall_score(y_true, y_hat, zero_division=0),\n",
    "    \"f1\": f1_score(y_true, y_hat, zero_division=0),\n",
    "}\n",
    "\n",
    "comparison = pd.DataFrame(\n",
    "    [baseline_metrics, model_metrics],\n",
    "    index=[\"baseline_always_on_time\", \"xgb_v1\"]\n",
    ")\n",
    "\n",
    "print(\"✅ Model metrics:\", model_metrics)\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907c66f6-bf25-49d8-ac61-ca93e7100c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "sm = boto3.client(\"sagemaker\", region_name=region)\n",
    "sm.delete_model(ModelName=model_name)\n",
    "print(\"✅ Deleted model:\", model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80622011-15db-4d64-8218-fcea39c9552e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
