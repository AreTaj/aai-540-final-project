{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Part 3: Deployment & Monitoring (SageMaker)\n",
    "\n",
    "## Overview\n",
    "This notebook covers the **Deployment** and **Monitoring** phases of the MLOps lifecycle for the Olist E-Commerce project. \n",
    "\n",
    "**Objectives:**\n",
    "1. **Deploy** the XGBoost model trained in `02_Modeling.ipynb` to a real-time SageMaker endpoint.\n",
    "2. **Enable Data Capture** to log all inference requests and predictions to S3.\n",
    "3. **Implement Model Monitoring**:\n",
    "   - **Data Quality Monitor**: Detects drift in input features (e.g., changes in `payment_value_sum` distribution).\n",
    "   - **Model Quality Monitor**: continually evaluates model performance (Accuracy, F1, AUC) by comparing predictions against ground truth labels.\n",
    "4. **Visualize** the monitoring results and CloudWatch metrics.\n",
    "\n",
    "> ** COST WARNING**: This notebook creates a real-time endpoint (`ml.m5.xlarge`) and monitoring schedules. These resources incur hourly costs. **Run the Cleanup section at the end of this notebook to delete these resources.**\n",
    "\n",
    "**Reference**: This implementation is adapted from `lab-5-1-model-monitoring-with-sagemaker-and-cloudwatch`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-intro",
   "metadata": {},
   "source": [
    "### Setup & Configuration\n",
    "Import necessary libraries and configure S3 bucket locations. Prefixes must match those used in previous notebooks (`01` and `02`) to ensure correct retrieval of data and model artifacts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region: us-east-1\n",
      "Role: arn:aws:iam::587322031938:role/LabRole\n",
      "Bucket: sagemaker-us-east-1-587322031938\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import sagemaker\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import awswrangler as wr\n",
    "from datetime import datetime, timedelta\n",
    "from time import sleep\n",
    "from threading import Thread\n",
    "\n",
    "from sagemaker import image_uris, get_execution_role\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import CSVDeserializer\n",
    "from sagemaker.model_monitor import (\n",
    "    DataCaptureConfig,\n",
    "    DefaultModelMonitor,\n",
    "    ModelQualityMonitor,\n",
    "    DatasetFormat,\n",
    "    EndpointInput,\n",
    "    CronExpressionGenerator,\n",
    ")\n",
    "from sagemaker.s3 import S3Downloader, S3Uploader\n",
    "\n",
    "sm_sess = sagemaker.Session()\n",
    "region = sm_sess.boto_region_name\n",
    "try:\n",
    "    role = get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "bucket = sm_sess.default_bucket()\n",
    "\n",
    "# Prefixes (Must align with 02_Modeling)\n",
    "prefix = \"datalake/olist/monitoring\"\n",
    "model_prefix = \"modeling/output\"\n",
    "data_capture_prefix = f\"{prefix}/datacapture\"\n",
    "reports_prefix = f\"{prefix}/reports\"\n",
    "base_prefix = f\"s3://{bucket}/modeling/xgb-baseline/\"\n",
    "\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Role: {role}\")\n",
    "print(f\"Bucket: {bucket}\")\n",
    "\n",
    "sched_name = \"olist-data-quality-monitor\"\n",
    "model_sched_name = \"olist-model-quality-monitor\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-output-expl",
   "metadata": {},
   "source": [
    "#### Output Verification\n",
    "Verify that the outputs display the expected AWS Region, IAM Execution Role ARN, and default S3 bucket name.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-model",
   "metadata": {},
   "source": [
    "### 1. Load Trained Model Artifact\n",
    "Locate the latest model trained in `02_Modeling.ipynb` to avoid retraining. The function `get_latest_model_artifact` scans the output directory and picks the most recent `model.tar.gz`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "find-model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model artifact: s3://sagemaker-us-east-1-587322031938/modeling/output/sagemaker-xgboost-2026-02-17-19-39-09-126/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "\n",
    "def get_latest_model_artifact(bucket, prefix):\n",
    "    resp = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "    contents = sorted(resp.get('Contents', []), key=lambda x: x['LastModified'], reverse=True)\n",
    "    for c in contents:\n",
    "        if c['Key'].endswith('/output/model.tar.gz'):\n",
    "            return f\"s3://{bucket}/{c['Key']}\"\n",
    "    return None\n",
    "\n",
    "model_data = get_latest_model_artifact(bucket, model_prefix)\n",
    "\n",
    "if not model_data:\n",
    "    raise ValueError(f\"No model artifact found in s3://{bucket}/{model_prefix}. Please run 02_Modeling.ipynb first.\")\n",
    "\n",
    "print(f\"Using model artifact: {model_data}\")\n",
    "\n",
    "# Training image URI (XGBoost)\n",
    "image_uri = image_uris.retrieve(framework=\"xgboost\", region=region, version=\"1.5-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-model-output-expl",
   "metadata": {},
   "source": [
    "#### Model Confirmation\n",
    "The output above should display the S3 URI of the model artifact found (e.g., `s3://.../model.tar.gz`). If it fails, ensure that you have successfully run the training step in `02_Modeling.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deploy",
   "metadata": {},
   "source": [
    "### 2. Live Deployment with Data Capture\n",
    "Deploy the model to an `ml.m5.xlarge` instance and enable **Data Capture** to save all inputs and outputs to S3 for monitoring.\n",
    "\n",
    "**Safety Check**: If the endpoint already exists, we skip deployment to avoid errors and duplicate costs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "deploy-endpoint",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint olist-xgb-monitoring-ep already exists. Status: InService\n"
     ]
    }
   ],
   "source": [
    "endpoint_name = \"olist-xgb-monitoring-ep\"\n",
    "data_capture_uri = f\"s3://{bucket}/{data_capture_prefix}\"\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "try:\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    print(f\"Endpoint {endpoint_name} already exists. Status: {resp['EndpointStatus']}\")\n",
    "    predictor = Predictor(endpoint_name=endpoint_name, sagemaker_session=sm_sess, serializer=CSVSerializer())\n",
    "except sm_client.exceptions.ClientError:\n",
    "    print(f\"Creating new endpoint: {endpoint_name}...\")\n",
    "    \n",
    "    model = Model(\n",
    "        image_uri=image_uri,\n",
    "        model_data=model_data,\n",
    "        role=role,\n",
    "        sagemaker_session=sm_sess\n",
    "    )\n",
    "    \n",
    "    data_capture_config = DataCaptureConfig(\n",
    "        enable_capture=True,\n",
    "        sampling_percentage=100,\n",
    "        destination_s3_uri=data_capture_uri\n",
    "    )\n",
    "    \n",
    "    model.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type=\"ml.m5.xlarge\",\n",
    "        endpoint_name=endpoint_name,\n",
    "        data_capture_config=data_capture_config\n",
    "    )\n",
    "    predictor = Predictor(endpoint_name=endpoint_name, sagemaker_session=sm_sess, serializer=CSVSerializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deploy-output-expl",
   "metadata": {},
   "source": [
    "#### Deployment Status\n",
    "If the endpoint was already running, you will see `Endpoint ... already exists. Status: InService`. If it is new, SageMaker will print a series of dashes (`-`) indicating the provisioning progress. Once complete, the endpoint is ready for real-time inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd5dffd7-0fff-4703-9c26-0288674ded35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created/updated infrastructure alarms for errors, latency, CPU, and memory.\n"
     ]
    }
   ],
   "source": [
    "#NEW CELL- Infrastructure Monitoring\n",
    "# This cell creates CloudWatch alarms to monitor the health of the\n",
    "# SageMaker endpoint, including invocation errors, inference latency,\n",
    "# and CPU/memory utilization. These monitors detect infrastructure-\n",
    "# level failures that can impact ML system reliability and availability.\n",
    "\n",
    "\n",
    "import boto3, json\n",
    "cw = boto3.client(\"cloudwatch\", region_name=region)\n",
    "\n",
    "# Common endpoint dimensions\n",
    "# NOTE: VariantName is often \"AllTraffic\" unless you set a custom variant name.\n",
    "endpoint_dims = [\n",
    "    {\"Name\": \"EndpointName\", \"Value\": endpoint_name},\n",
    "    {\"Name\": \"VariantName\", \"Value\": \"AllTraffic\"},\n",
    "]\n",
    "\n",
    "alarm_topic_arn = None  # Optional: set to an SNS topic ARN to notify you\n",
    "\n",
    "def put_alarm(name, metric, stat=\"Average\", period=60, eval_periods=5, threshold=1, comp=\"GreaterThanThreshold\"):\n",
    "    args = dict(\n",
    "        AlarmName=name,\n",
    "        MetricName=metric,\n",
    "        Namespace=\"AWS/SageMaker\",\n",
    "        Dimensions=endpoint_dims,\n",
    "        Statistic=stat,\n",
    "        Period=period,\n",
    "        EvaluationPeriods=eval_periods,\n",
    "        Threshold=threshold,\n",
    "        ComparisonOperator=comp,\n",
    "        TreatMissingData=\"notBreaching\",\n",
    "    )\n",
    "    if alarm_topic_arn:\n",
    "        args[\"AlarmActions\"] = [alarm_topic_arn]\n",
    "        args[\"OKActions\"] = [alarm_topic_arn]\n",
    "    cw.put_metric_alarm(**args)\n",
    "\n",
    "# 4XX/5XX errors (bad requests / server errors)\n",
    "put_alarm(f\"{endpoint_name}-4xx\", \"Invocation4XXErrors\", stat=\"Sum\", threshold=5)\n",
    "put_alarm(f\"{endpoint_name}-5xx\", \"Invocation5XXErrors\", stat=\"Sum\", threshold=1)\n",
    "\n",
    "# Latency alarms (tune thresholds to your use case)\n",
    "put_alarm(f\"{endpoint_name}-model-latency\", \"ModelLatency\", stat=\"Average\", threshold=2000)      # ms\n",
    "put_alarm(f\"{endpoint_name}-overhead-latency\", \"OverheadLatency\", stat=\"Average\", threshold=500) # ms\n",
    "\n",
    "# Capacity / saturation style signals (optional but common)\n",
    "put_alarm(f\"{endpoint_name}-cpu-high\", \"CPUUtilization\", stat=\"Average\", threshold=80)\n",
    "put_alarm(f\"{endpoint_name}-mem-high\", \"MemoryUtilization\", stat=\"Average\", threshold=80)\n",
    "\n",
    "print(\"Created/updated infrastructure alarms for errors, latency, CPU, and memory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f205697-3816-4775-b7cc-e3b8a5c81091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created CloudWatch dashboard: olist-xgb-monitoring-ep-mlops-dashboard\n"
     ]
    }
   ],
   "source": [
    "# CloudWatch Monitoring Dashboard\n",
    "# This cell creates a CloudWatch dashboard to visualize\n",
    "# infrastructure, performance, and monitoring metrics for the\n",
    "# deployed SageMaker endpoint, including traffic, errors,\n",
    "# latency, resource utilization, and model monitoring signals.\n",
    "\n",
    "dashboard_name = f\"{endpoint_name}-mlops-dashboard\"\n",
    "\n",
    "dashboard_body = {\n",
    "  \"widgets\": [\n",
    "    {\n",
    "      \"type\": \"metric\",\n",
    "      \"x\": 0, \"y\": 0, \"width\": 12, \"height\": 6,\n",
    "      \"properties\": {\n",
    "        \"metrics\": [\n",
    "          [ \"AWS/SageMaker\", \"Invocations\", \"EndpointName\", endpoint_name, \"VariantName\", \"AllTraffic\" ],\n",
    "          [ \".\", \"Invocation4XXErrors\", \".\", \".\", \".\", \".\" ],\n",
    "          [ \".\", \"Invocation5XXErrors\", \".\", \".\", \".\", \".\" ],\n",
    "        ],\n",
    "        \"period\": 60,\n",
    "        \"stat\": \"Sum\",\n",
    "        \"region\": region,\n",
    "        \"title\": \"Endpoint traffic & errors\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"type\": \"metric\",\n",
    "      \"x\": 12, \"y\": 0, \"width\": 12, \"height\": 6,\n",
    "      \"properties\": {\n",
    "        \"metrics\": [\n",
    "          [ \"AWS/SageMaker\", \"ModelLatency\", \"EndpointName\", endpoint_name, \"VariantName\", \"AllTraffic\" ],\n",
    "          [ \".\", \"OverheadLatency\", \".\", \".\", \".\", \".\" ],\n",
    "        ],\n",
    "        \"period\": 60,\n",
    "        \"stat\": \"Average\",\n",
    "        \"region\": region,\n",
    "        \"title\": \"Endpoint latency (ms)\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"type\": \"metric\",\n",
    "      \"x\": 0, \"y\": 6, \"width\": 12, \"height\": 6,\n",
    "      \"properties\": {\n",
    "        \"metrics\": [\n",
    "          [ \"AWS/SageMaker\", \"CPUUtilization\", \"EndpointName\", endpoint_name, \"VariantName\", \"AllTraffic\" ],\n",
    "          [ \".\", \"MemoryUtilization\", \".\", \".\", \".\", \".\" ],\n",
    "        ],\n",
    "        \"period\": 60,\n",
    "        \"stat\": \"Average\",\n",
    "        \"region\": region,\n",
    "        \"title\": \"Endpoint instance utilization\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"type\": \"metric\",\n",
    "      \"x\": 12, \"y\": 6, \"width\": 12, \"height\": 6,\n",
    "      \"properties\": {\n",
    "        \"metrics\": [\n",
    "          # Model Monitor emits metrics when enable_cloudwatch_metrics=True.\n",
    "          # These commonly appear under AWS/SageMaker and/or a ModelMonitor-related namespace depending on SDK/version.\n",
    "          # If you don't see them, keep the dashboard but verify exact namespace/dimensions in CloudWatch Metrics UI.\n",
    "          [ \"AWS/SageMaker\", \"ConstraintViolations\", \"MonitoringSchedule\", sched_name ],\n",
    "          [ \".\", \"ConstraintViolations\", \".\", model_sched_name ],\n",
    "        ],\n",
    "        \"period\": 3600,\n",
    "        \"stat\": \"Sum\",\n",
    "        \"region\": region,\n",
    "        \"title\": \"Model Monitor violations (data/model schedules)\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "cw.put_dashboard(DashboardName=dashboard_name, DashboardBody=json.dumps(dashboard_body))\n",
    "print(\"Created CloudWatch dashboard:\", dashboard_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-mon",
   "metadata": {},
   "source": [
    "### 3. Data Quality Monitoring\n",
    "Create a baseline using the training dataset (from `02_Modeling`) to detect if the distribution of incoming data shifts significantly (Data Drift).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "baseline-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baselining with training data: s3://sagemaker-us-east-1-587322031938/modeling/xgb-baseline/train/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name baseline-suggestion-job-2026-02-17-20-07-21-544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Data Quality Baseline Job...\n",
      "...........................................................!"
     ]
    }
   ],
   "source": [
    "# Load training data for baseline (Must match what was used in 02_Modeling)\n",
    "train_uri = f\"{base_prefix}train/\"\n",
    "baseline_results_uri = f\"s3://{bucket}/{prefix}/baselining/data_quality\"\n",
    "\n",
    "print(f\"Baselining with training data: {train_uri}\")\n",
    "\n",
    "data_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "\n",
    "# Check if baseline already exists to run expensive job only once\n",
    "existing_baseline = s3_client.list_objects_v2(Bucket=bucket, Prefix=f\"{prefix}/baselining/data_quality/statistics.json\")\n",
    "if existing_baseline.get('KeyCount', 0) > 0:\n",
    "    print(\"Found existing Data Quality Baseline. Skipping baseline job.\")\n",
    "    # Attach to existing monitor logic would go here if we needed the object, \n",
    "    # but we can reuse the monitor object for scheduling.\n",
    "else:\n",
    "    print(\"Starting Data Quality Baseline Job...\")\n",
    "    data_monitor.suggest_baseline(\n",
    "        baseline_dataset=train_uri,\n",
    "        dataset_format=DatasetFormat.csv(header=False),\n",
    "        output_s3_uri=baseline_results_uri,\n",
    "        wait=True,\n",
    "        logs=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baseline-data-expl",
   "metadata": {},
   "source": [
    "#### Baseline Job Status\n",
    "The code checks for an existing `statistics.json` file. If found, it skips the baselining job to save time. Otherwise, it launches a Processing Job to compute statistics (mean, variance, etc.) and constraints (type check, null checks) on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "schedule-data-intro",
   "metadata": {},
   "source": [
    "#### Schedule Data Quality Monitor\n",
    "Schedule the Data Quality Monitor to run hourly. It analyzes the data captured from the endpoint (`data_capture_uri`) and compares it against the baseline. If violations are found (e.g., drift), it generates a violation report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "schedule-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Monitoring Schedule: olist-data-quality-monitor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.model_monitor.model_monitoring:Creating Monitoring Schedule with name: olist-data-quality-monitor\n"
     ]
    }
   ],
   "source": [
    "# Schedule Data Quality Monitor\n",
    "sched_name = \"olist-data-quality-monitor\"\n",
    "\n",
    "try:\n",
    "    sm_client.describe_monitoring_schedule(MonitoringScheduleName=sched_name)\n",
    "    print(f\"Schedule {sched_name} already exists.\")\n",
    "except sm_client.exceptions.ResourceNotFound:\n",
    "    print(f\"Creating Monitoring Schedule: {sched_name}\")\n",
    "    data_monitor.create_monitoring_schedule(\n",
    "        monitor_schedule_name=sched_name,\n",
    "        endpoint_input=EndpointInput(\n",
    "            endpoint_name=endpoint_name,\n",
    "            destination=\"/opt/ml/processing/input/endpoint_data\",\n",
    "        ),\n",
    "        output_s3_uri=f\"s3://{bucket}/{reports_prefix}/data_quality\",\n",
    "        schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "        enable_cloudwatch_metrics=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "schedule-data-expl",
   "metadata": {},
   "source": [
    "#### Schedule Confirmation\n",
    "Ensure the output confirms that `olist-data-quality-monitor` has been created or already exists.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-mon",
   "metadata": {},
   "source": [
    "### 4. Model Quality Monitoring\n",
    "To monitor model performance (e.g., Accuracy), **Ground Truth** labels are required. In a real scenario, these arrive after inference (e.g., did the package actually arrive late?). Here, ground truth is simulated for demonstration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "model-quality-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    }
   ],
   "source": [
    "model_monitor = ModelQualityMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=1800,\n",
    "    sagemaker_session=sm_sess\n",
    ")\n",
    "\n",
    "# Create baseline for Model Quality (requires predictions + labels)\n",
    "# Skip the explicit `suggest_baseline` step here to save time/cost in this demo, \n",
    "# as it requires merging validation predictions with labels manually first. \n",
    "# Instead, we'll proceed to creating the schedule which assumes we upload ground truth later.\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-qa-schedule-intro",
   "metadata": {},
   "source": [
    "#### Schedule Model Quality Monitor\n",
    "Create a schedule to monitor model performance, specifying `BinaryClassification` as the problem type. The monitor matches inference IDs from the endpoint requests with ground truth data found in the `ground_truth_input` S3 path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "schedule-model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model Quality Schedule: olist-model-quality-monitor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.model_monitor.model_monitoring:Creating Monitoring Schedule with name: olist-model-quality-monitor\n"
     ]
    }
   ],
   "source": [
    "model_sched_name = \"olist-model-quality-monitor\"\n",
    "ground_truth_path = f\"s3://{bucket}/{prefix}/ground_truth_data\"\n",
    "\n",
    "try:\n",
    "    sm_client.describe_monitoring_schedule(MonitoringScheduleName=model_sched_name)\n",
    "    print(f\"Schedule {model_sched_name} already exists.\")\n",
    "except sm_client.exceptions.ResourceNotFound:\n",
    "    print(f\"Creating Model Quality Schedule: {model_sched_name}\")\n",
    "    # Note: The same constraints from data quality are used for simplicity, \n",
    "    # or we could point to a pre-calculated constraints file. \n",
    "    # For this demo, we effectively monitor without strict baseline constraints just to show setup.\n",
    "    \n",
    "    model_monitor.create_monitoring_schedule(\n",
    "        monitor_schedule_name=model_sched_name,\n",
    "        endpoint_input=EndpointInput(\n",
    "            endpoint_name=endpoint_name,\n",
    "            inference_attribute=\"0\",           # First column is probability\n",
    "            probability_threshold_attribute=0.5,\n",
    "            destination=\"/opt/ml/processing/input_data\",\n",
    "        ),\n",
    "        problem_type=\"BinaryClassification\",\n",
    "        ground_truth_input=ground_truth_path,\n",
    "        output_s3_uri=f\"s3://{bucket}/{reports_prefix}/model_quality\",\n",
    "        schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "        enable_cloudwatch_metrics=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-qa-expl",
   "metadata": {},
   "source": [
    "#### Schedule Status\n",
    "The output should confirm `olist-model-quality-monitor` creation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traffic",
   "metadata": {},
   "source": [
    "### 5. Simulate Traffic & Ground Truth\n",
    "Start a thread to send requests to the endpoint and upload corresponding simulated ground truth labels, providing data for the monitor to process. \n",
    "\n",
    "**Note:** `awswrangler` is used here to read the test dataset directly from S3, as `pd.read_csv` cannot handle S3 directory prefixes natively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "sim-traffic",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:awswrangler.distributed.ray._core:Initializing a Ray instance\n",
      "2026-02-17 20:14:54,786\tWARNING services.py:2070 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 1909395456 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=4.48gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2026-02-17 20:14:54,840\tINFO worker.py:1852 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending traffic... (Stop kernel to end)\n"
     ]
    }
   ],
   "source": [
    "# Load some test data as a sample payload\n",
    "# From 02_Modeling splits\n",
    "test_uri = f\"{base_prefix}test/\"\n",
    "\n",
    "# FIX: Use awswrangler to read from S3 prefix (native pandas fails on folders)\n",
    "test_df = wr.s3.read_csv(test_uri, header=None)\n",
    "\n",
    "sample_payloads = test_df.iloc[:10, 1:].to_csv(header=False, index=False).split(\"\\n\") # Drop label col 0\n",
    "\n",
    "def simulate_traffic():\n",
    "    print(\"Sending traffic... (Stop kernel to end)\")\n",
    "    for i, payload in enumerate(sample_payloads):\n",
    "        if not payload: continue\n",
    "        try:\n",
    "            resp = predictor.predict(payload, inference_id=str(i)) # Inference ID is key for joining\n",
    "            # print(f\"Prediction {i}: {resp}\")\n",
    "            sleep(0.5)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            \n",
    "# Run simulation once for demo\n",
    "simulate_traffic()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traffic-expl",
   "metadata": {},
   "source": [
    "#### Traffic Simulation\n",
    "The output will display \"Sending traffic...\" followed by successful execution. If errors occur, check the endpoint status in the SageMaker console.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup",
   "metadata": {},
   "source": [
    "## 6. Cleanup (Teardown)\n",
    "**CRITICAL**: Run this cell to delete resources and avoid unexpected costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "teardown",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up resources...\n",
      "Skipped schedule olist-data-quality-monitor: An error occurred (ResourceNotFound) when calling the DeleteMonitoringSchedule operation: Monitoring Schedule arn:aws:sagemaker:us-east-1:587322031938:monitoring-schedule/olist-data-quality-monitor not found\n",
      "Skipped schedule olist-model-quality-monitor: An error occurred (ResourceNotFound) when calling the DeleteMonitoringSchedule operation: Monitoring Schedule arn:aws:sagemaker:us-east-1:587322031938:monitoring-schedule/olist-model-quality-monitor not found\n",
      "Waiting for schedules to clear...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint configuration with name: olist-xgb-monitoring-ep\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped endpoint: An error occurred (ValidationException) when calling the DeleteEndpointConfig operation: Could not find endpoint configuration \"olist-xgb-monitoring-ep\".\n"
     ]
    }
   ],
   "source": [
    "def cleanup():\n",
    "    print(\"Cleaning up resources...\")\n",
    "    # Delete Schedules\n",
    "    for schedule in [sched_name, model_sched_name]:\n",
    "        try:\n",
    "            sm_client.delete_monitoring_schedule(MonitoringScheduleName=schedule)\n",
    "            print(f\"Deleted schedule: {schedule}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Skipped schedule {schedule}: {e}\")\n",
    "            \n",
    "    # --- ADD THIS WAIT STEP ---\n",
    "    print(\"Waiting for schedules to clear...\")\n",
    "    time.sleep(15) \n",
    "    # --------------------------\n",
    "    \n",
    "    # Delete Endpoint\n",
    "    try:\n",
    "        predictor.delete_endpoint(delete_endpoint_config=True) # Good practice to delete config too\n",
    "        print(f\"Deleted endpoint: {endpoint_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Skipped endpoint: {e}\")\n",
    "\n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "486eec49-b80e-4a91-916a-0b94c48f833a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully deleted endpoint: olist-xgb-monitoring-ep\n",
      "Deleted CloudWatch Dashboard.\n",
      "Cleanup complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1771360110.287907    4746 chttp2_transport.cc:1182] ipv4:169.255.255.2:37745: Got goaway [2] err=UNAVAILABLE:GOAWAY received; Error code: 2; Debug Text: Cancelling all calls {created_time:\"2026-02-17T20:28:30.287891393+00:00\", http2_error:2, grpc_status:14}\n",
      "*** SIGTERM received at time=1771360112 on cpu 0 ***\n",
      "PC: @     0x7f9e76e0de9e  (unknown)  epoll_wait\n",
      "    @     0x7f9e1e66db0d         64  absl::lts_20240722::AbslFailureSignalHandler()\n",
      "    @     0x7f9e76d2a520  (unknown)  (unknown)\n",
      "[2026-02-17 20:28:32,664 E 4111 4111] logging.cc:497: *** SIGTERM received at time=1771360112 on cpu 0 ***\n",
      "[2026-02-17 20:28:32,664 E 4111 4111] logging.cc:497: PC: @     0x7f9e76e0de9e  (unknown)  epoll_wait\n",
      "[2026-02-17 20:28:32,665 E 4111 4111] logging.cc:497:     @     0x7f9e1e66db39         64  absl::lts_20240722::AbslFailureSignalHandler()\n",
      "[2026-02-17 20:28:32,665 E 4111 4111] logging.cc:497:     @     0x7f9e76d2a520  (unknown)  (unknown)\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "sm = boto3.client(\"sagemaker\")\n",
    "\n",
    "# 1. Check Endpoint\n",
    "try:\n",
    "    sm.delete_endpoint(EndpointName=endpoint_name)\n",
    "    print(f\"Successfully deleted endpoint: {endpoint_name}\")\n",
    "except Exception as e:\n",
    "    if \"Could not find\" in str(e) or \"ResourceNotFound\" in str(e):\n",
    "        print(\"Endpoint is already gone.\")\n",
    "    else:\n",
    "        print(f\"Endpoint status: {e}\")\n",
    "\n",
    "# 2. Check Dashboard\n",
    "try:\n",
    "    cw = boto3.client(\"cloudwatch\")\n",
    "    cw.delete_dashboards(DashboardNames=[f\"{endpoint_name}-mlops-dashboard\"])\n",
    "    print(\"Deleted CloudWatch Dashboard.\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"Cleanup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup-expl",
   "metadata": {},
   "source": [
    "#### Cleanup Confirmation\n",
    "When you run the cleanup function, verify that it successfully deletes both schedules and the endpoint. This ensures no residual charges will accrue."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
