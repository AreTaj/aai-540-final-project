{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64e36e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# If you're running this in SageMaker Studio:\n",
    "# - Keep this notebook in the same folder as 01_Data_Preparation.ipynb\n",
    "%run ./01_Data_Preparation.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deded73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    ")\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "# Session / region / role\n",
    "sm_sess = sagemaker.Session()\n",
    "region = sm_sess.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "print(\"Region:\", region)\n",
    "print(\"Role  :\", role)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066ded47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check: make sure that these exist from Data_Preparation\n",
    "assert \"df_train\" in globals(), \"df_train not found. Ensure 01_Data_Preparation.ipynb ran successfully.\"\n",
    "assert \"df_test\" in globals(), \"df_test not found. Ensure 01_Data_Preparation.ipynb ran successfully.\"\n",
    "assert \"df_val\" in globals(), \"df_val not found. Ensure 01_Data_Preparation.ipynb ran successfully.\"\n",
    "\n",
    "label_col = \"label_satisfied\"\n",
    "for _df, _name in [(df_train,\"df_train\"), (df_test,\"df_test\"), (df_val,\"df_val\")]:\n",
    "    assert label_col in _df.columns, f\"{label_col} missing from {_name}\"\n",
    "\n",
    "print(\"Train shape:\", df_train.shape)\n",
    "print(\"Test  shape:\", df_test.shape)\n",
    "print(\"Val   shape:\", df_val.shape)\n",
    "print(\"Label prevalence (train):\", df_train[label_col].mean().round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e6a393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature configuration \n",
    "num_features = [\n",
    "    \"total_items\",\n",
    "    \"total_price\",\n",
    "    \"total_freight\",\n",
    "    \"payment_value_sum\",\n",
    "    \"payment_installments_max\",\n",
    "    \"delivery_time_days\",\n",
    "    \"estimated_time_days\",\n",
    "    \"delivered_late\",\n",
    "]\n",
    "cat_features = [\"customer_state\", \"payment_types\"]\n",
    "\n",
    "# Keep only existing columns \n",
    "num_features = [c for c in num_features if c in df_train.columns]\n",
    "cat_features = [c for c in cat_features if c in df_train.columns]\n",
    "\n",
    "print(\"Numeric features:\", num_features)\n",
    "print(\"Categorical features:\", cat_features)\n",
    "\n",
    "def make_model_frame(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    cols = num_features + cat_features + [label_col]\n",
    "    out = df[cols].copy()\n",
    "    # Ensure types\n",
    "    for c in num_features:\n",
    "        out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "    out[num_features] = out[num_features].fillna(out[num_features].median(numeric_only=True))\n",
    "    for c in cat_features:\n",
    "        out[c] = out[c].fillna(\"UNK\").astype(str)\n",
    "    out[label_col] = out[label_col].astype(int)\n",
    "    return out\n",
    "\n",
    "train_df = make_model_frame(df_train)\n",
    "test_df  = make_model_frame(df_test)\n",
    "val_df   = make_model_frame(df_val)\n",
    "\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9b4dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "X_train = train_df.drop(columns=[label_col])\n",
    "y_train = train_df[label_col].values\n",
    "\n",
    "X_test = test_df.drop(columns=[label_col])\n",
    "y_test = test_df[label_col].values\n",
    "\n",
    "# Benchmark A: majority-class baseline\n",
    "dummy = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy.fit(X_train, y_train)\n",
    "pred_dummy = dummy.predict(X_test)\n",
    "\n",
    "def classification_metrics(y_true, y_pred, y_score=None):\n",
    "    out = {\n",
    "        \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
    "        \"precision\": float(precision_score(y_true, y_pred, zero_division=0)),\n",
    "        \"recall\": float(recall_score(y_true, y_pred, zero_division=0)),\n",
    "        \"f1\": float(f1_score(y_true, y_pred, zero_division=0)),\n",
    "    }\n",
    "    if y_score is not None:\n",
    "        try:\n",
    "            out[\"roc_auc\"] = float(roc_auc_score(y_true, y_score))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return out\n",
    "\n",
    "metrics_dummy = classification_metrics(y_test, pred_dummy)\n",
    "print(\"Benchmark A \u2014 DummyClassifier:\", metrics_dummy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a33ff44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark B: logistic regression on features\n",
    "# delivered_late + delivery_time_days + total_price\n",
    "tiny_feats = [c for c in [\"delivered_late\", \"delivery_time_days\", \"total_price\"] if c in X_train.columns]\n",
    "assert len(tiny_feats) >= 1, \"No tiny benchmark features found; adjust tiny_feats list.\"\n",
    "\n",
    "pre = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", \"passthrough\", tiny_feats),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "bench_lr = Pipeline(steps=[\n",
    "    (\"pre\", pre),\n",
    "    (\"clf\", LogisticRegression(max_iter=200, n_jobs=None)),\n",
    "])\n",
    "bench_lr.fit(X_train, y_train)\n",
    "\n",
    "pred_lr = bench_lr.predict(X_test)\n",
    "proba_lr = None\n",
    "if hasattr(bench_lr.named_steps[\"clf\"], \"predict_proba\"):\n",
    "    proba_lr = bench_lr.predict_proba(X_test)[:, 1]\n",
    "\n",
    "metrics_lr = classification_metrics(y_test, pred_lr, y_score=proba_lr)\n",
    "print(\"Benchmark B \u2014 Tiny LogisticRegression:\", metrics_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b421af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full preprocessing for model training \n",
    "ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", \"passthrough\", num_features),\n",
    "        (\"cat\", ohe, cat_features),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "# Fit on train only\n",
    "X_train_mat = preprocess.fit_transform(X_train)\n",
    "X_val_mat   = preprocess.transform(val_df.drop(columns=[label_col]))\n",
    "X_test_mat  = preprocess.transform(X_test)\n",
    "\n",
    "y_val = val_df[label_col].values\n",
    "\n",
    "# Helper to create XGBoost CSV \n",
    "def to_xgb_csv(X_mat, y_vec) -> pd.DataFrame:\n",
    "    y_vec = np.asarray(y_vec).reshape(-1, 1)\n",
    "    arr = np.hstack([y_vec, X_mat])\n",
    "    return pd.DataFrame(arr)\n",
    "\n",
    "train_xgb = to_xgb_csv(X_train_mat, y_train)\n",
    "val_xgb   = to_xgb_csv(X_val_mat, y_val)\n",
    "test_xgb  = to_xgb_csv(X_test_mat, y_test)\n",
    "\n",
    "train_xgb.shape, val_xgb.shape, test_xgb.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b92afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import awswrangler as wr\n",
    "\n",
    "if \"DATALAKE_BUCKET\" in globals() and isinstance(DATALAKE_BUCKET, str) and len(DATALAKE_BUCKET) > 0:\n",
    "    bucket = DATALAKE_BUCKET\n",
    "else:\n",
    "    bucket = sm_sess.default_bucket()\n",
    "\n",
    "base_prefix = f\"s3://{bucket}/modeling/xgb-baseline/\"\n",
    "\n",
    "train_prefix = base_prefix + \"train/\"\n",
    "val_prefix   = base_prefix + \"val/\"\n",
    "test_prefix  = base_prefix + \"test/\"\n",
    "\n",
    "# Writes one or more CSV files under each prefix, overwriting existing data\n",
    "wr.s3.to_csv(train_xgb, path=train_prefix, index=False, header=False, dataset=True, mode=\"overwrite\")\n",
    "wr.s3.to_csv(val_xgb,   path=val_prefix,   index=False, header=False, dataset=True, mode=\"overwrite\")\n",
    "wr.s3.to_csv(test_xgb,  path=test_prefix,  index=False, header=False, dataset=True, mode=\"overwrite\")\n",
    "\n",
    "print(\"Uploaded dataset prefixes:\")\n",
    "print(\"  train:\", train_prefix)\n",
    "print(\"  val  :\", val_prefix)\n",
    "print(\"  test :\", test_prefix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fix_missing_defs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths used in training/validation\n",
    "s3_train = train_prefix\n",
    "s3_val   = val_prefix\n",
    "s3_test  = test_prefix\n",
    "\n",
    "output_path = f\"s3://{bucket}/modeling/output\"\n",
    "transform_output = base_prefix + \"transform-output/\"\n",
    "\n",
    "# Image URI\n",
    "xgb_image = image_uris.retrieve(\n",
    "    framework=\"xgboost\",\n",
    "    region=region,\n",
    "    version=\"1.5-1\"\n",
    ")\n",
    "\n",
    "# Estimator Definition\n",
    "xgb = sagemaker.estimator.Estimator(\n",
    "    image_uri=xgb_image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    output_path=output_path,\n",
    "    sagemaker_session=sm_sess\n",
    ")\n",
    "\n",
    "xgb.set_hyperparameters(\n",
    "    max_depth=5,\n",
    "    eta=0.2,\n",
    "    gamma=4,\n",
    "    min_child_weight=6,\n",
    "    subsample=0.8,\n",
    "    objective=\"binary:logistic\",\n",
    "    num_round=50\n",
    ")\n",
    "\n",
    "# Transformer Definition\n",
    "transformer = xgb.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    output_path=transform_output\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e70aa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = TrainingInput(s3_data=os.path.dirname(s3_train) + \"/\", content_type=\"text/csv\")\n",
    "val_input   = TrainingInput(s3_data=os.path.dirname(s3_val) + \"/\",   content_type=\"text/csv\")\n",
    "\n",
    "# --- COST SAFETY CHECK ---\n",
    "import boto3\n",
    "import time\n",
    "from urllib.parse import urlparse\n",
    "from sagemaker.model import Model\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "def check_s3_prefix_has_contents(bucket_name, prefix):\n",
    "    resp = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "    return resp.get('KeyCount', 0) > 0\n",
    "\n",
    "# Parse the output path defined in previous cells\n",
    "p = urlparse(output_path)\n",
    "out_bucket = p.netloc\n",
    "out_key_prefix = p.path.lstrip('/')\n",
    "\n",
    "if check_s3_prefix_has_contents(out_bucket, out_key_prefix):\n",
    "    print(f'Found existing training artifacts in {output_path}. Skipping Training to save cost.')\n",
    "    # Find latest model artifact\n",
    "    resp = s3_client.list_objects_v2(Bucket=out_bucket, Prefix=out_key_prefix)\n",
    "    contents = sorted(resp.get('Contents', []), key=lambda x: x['LastModified'], reverse=True)\n",
    "    model_uri = None\n",
    "    for c in contents:\n",
    "        if c['Key'].endswith('/output/model.tar.gz'):\n",
    "            model_uri = f's3://{out_bucket}/{c[\"Key\"]}'\n",
    "            break\n",
    "    if model_uri:\n",
    "        print(f'   Using latest model artifact: {model_uri}')\n",
    "        # Recreate Estimator/Model so next cells work\n",
    "        xgb_model = Model(\n",
    "            image_uri=xgb_image,\n",
    "            model_data=model_uri,\n",
    "            role=role,\n",
    "            sagemaker_session=sm_sess\n",
    "        )\n",
    "        # Swap xgb (Estimator) to xgb_model (Model) for transformer usage\n",
    "        xgb = xgb_model\n",
    "    else:\n",
    "        print('   Output dir exists but no model found. Retraining...')\n",
    "        xgb.fit({'train': train_input, 'validation': val_input}, logs=False)\n",
    "else:\n",
    "    print('No existing training artifacts found. Starting Training...')\n",
    "    xgb.fit({'train': train_input, 'validation': val_input}, logs=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1829fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For transform, we provide features only\n",
    "test_features_only = test_xgb.drop(columns=[0])  \n",
    "\n",
    "# Write as a dataset under a prefix \n",
    "test_features_prefix = f\"s3://{bucket}/{prefix}test/features_only/\"\n",
    "\n",
    "wr.s3.to_csv(\n",
    "    test_features_only,\n",
    "    path=test_features_prefix,\n",
    "    index=False,\n",
    "    header=False,\n",
    "    dataset=True,\n",
    "    mode=\"overwrite\",\n",
    ")\n",
    "\n",
    "print(\"Transform input prefix (features):\", test_features_prefix)\n",
    "print(\"Transform output path            :\", transform_output)\n",
    "\n",
    "# --- COST SAFETY CHECK ---\n",
    "t_parse = urlparse(transform_output)\n",
    "t_bucket = t_parse.netloc\n",
    "t_prefix = t_parse.path.lstrip('/')\n",
    "\n",
    "if check_s3_prefix_has_contents(t_bucket, t_prefix):\n",
    "    print(f'Found existing transform output in {transform_output}. Skipping Transform.')\n",
    "else:\n",
    "    print('No existing transform output. Starting Batch Transform...')\n",
    "    transformer.transform(\n",
    "        data=test_features_prefix,\n",
    "        content_type='text/csv',\n",
    "        split_type='Line',\n",
    "    )\n",
    "    transformer.wait()\n",
    "    print('Batch transform complete.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f60821",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "# List objects under output prefix to find the output file\n",
    "s3 = boto3.client(\"s3\")\n",
    "out_prefix = f\"{prefix}batch-output/\"\n",
    "\n",
    "resp = s3.list_objects_v2(Bucket=bucket, Prefix=out_prefix)\n",
    "keys = [obj[\"Key\"] for obj in resp.get(\"Contents\", [])]\n",
    "print(\"Output objects:\", keys)\n",
    "\n",
    "out_files = [k for k in keys if k.endswith(\".out\") or k.endswith(\".csv\") or \"test_features\" in k]\n",
    "\n",
    "candidate = None\n",
    "for k in keys:\n",
    "    if k.endswith(\".out\"):\n",
    "        candidate = k\n",
    "        break\n",
    "if candidate is None:\n",
    "    raise RuntimeError(\"Could not find batch transform output .out file. Check S3 output prefix listing above.\")\n",
    "\n",
    "print(\"Using output file:\", candidate)\n",
    "\n",
    "obj = s3.get_object(Bucket=bucket, Key=candidate)\n",
    "raw = obj[\"Body\"].read().decode(\"utf-8\").strip().splitlines()\n",
    "\n",
    "# Each line is a probability \n",
    "y_score = np.array([float(x.strip().split(\",\")[0]) for x in raw])\n",
    "y_pred = (y_score >= 0.5).astype(int)\n",
    "\n",
    "metrics_xgb = classification_metrics(y_test, y_pred, y_score=y_score)\n",
    "\n",
    "print(\"SageMaker XGBoost metrics:\", metrics_xgb)\n",
    "print()\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a84a50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side comparison\n",
    "compare = pd.DataFrame([\n",
    "    {\"model\": \"Benchmark A: Dummy (most_frequent)\", **metrics_dummy},\n",
    "    {\"model\": f\"Benchmark B: Tiny LR ({', '.join(tiny_feats)})\", **metrics_lr},\n",
    "    {\"model\": \"SageMaker: XGBoost (batch transform)\", **metrics_xgb},\n",
    "])\n",
    "\n",
    "# Reorder columns\n",
    "cols = [\"model\"] + [c for c in [\"accuracy\",\"precision\",\"recall\",\"f1\",\"roc_auc\"] if c in compare.columns]\n",
    "compare = compare[cols]\n",
    "compare\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}