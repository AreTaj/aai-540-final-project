{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e4d5c57",
   "metadata": {},
   "source": [
    "# Part 2: Modeling & Evaluation\n",
    "\n",
    "## Overview\n",
    "In this notebook, we load the features prepared in `01_Data_Preparation.ipynb`, training a Baseline model and an XGBoost model, and evaluate their performance.\n",
    "\n",
    "### Setup & Configuration\n",
    "We optimize the setup by loading the pre-split data directly from S3, avoiding the need to re-run the previous notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c64e36e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "Region: us-east-1\n",
      "Role: arn:aws:iam::587322031938:role/LabRole\n",
      "Default Bucket: sagemaker-us-east-1-587322031938\n",
      "Loading splits from s3://sagemaker-us-east-1-587322031938/datalake/olist/splits/order_level/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-17 19:39:00,135\tWARNING services.py:2070 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 1909399552 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=4.54gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2026-02-17 19:39:00,272\tINFO worker.py:1852 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Train shape: (39469, 23)\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sagemaker import image_uris\n",
    "import awswrangler as wr\n",
    "\n",
    "# --- Lightweight Setup (Optimized) ---\n",
    "# Replaces time-consuming %run ./01_Data_Preparation.ipynb\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sm_sess = sagemaker.Session()\n",
    "bucket = sm_sess.default_bucket()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Role: {role}\")\n",
    "print(f\"Default Bucket: {bucket}\")\n",
    "\n",
    "# --- Load Dataframes from S3 ---\n",
    "# Restores variables expected by downstream cells (previously created by 01)\n",
    "SPLITS_PREFIX = f\"s3://{bucket}/datalake/olist/splits/order_level/\"\n",
    "\n",
    "print(f\"Loading splits from {SPLITS_PREFIX}...\")\n",
    "df_train = wr.s3.read_parquet(path=SPLITS_PREFIX + \"train/\")\n",
    "df_val   = wr.s3.read_parquet(path=SPLITS_PREFIX + \"val/\")\n",
    "df_test  = wr.s3.read_parquet(path=SPLITS_PREFIX + \"test/\")\n",
    "\n",
    "print(\"Data loaded successfully.\")\n",
    "print(\"Train shape:\", df_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deded73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region: us-east-1\n",
      "Role  : arn:aws:iam::587322031938:role/LabRole\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    ")\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "# Session / region / role\n",
    "sm_sess = sagemaker.Session()\n",
    "region = sm_sess.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "print(\"Region:\", region)\n",
    "print(\"Role  :\", role)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "066ded47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (39469, 23)\n",
      "Test  shape: (9867, 23)\n",
      "Val   shape: (9867, 23)\n",
      "Label prevalence (train): 0.7717\n"
     ]
    }
   ],
   "source": [
    "# Check: make sure that these exist from Data_Preparation\n",
    "assert \"df_train\" in globals(), \"df_train not found. Ensure 01_Data_Preparation.ipynb ran successfully.\"\n",
    "assert \"df_test\" in globals(), \"df_test not found. Ensure 01_Data_Preparation.ipynb ran successfully.\"\n",
    "assert \"df_val\" in globals(), \"df_val not found. Ensure 01_Data_Preparation.ipynb ran successfully.\"\n",
    "\n",
    "label_col = \"label_satisfied\"\n",
    "for _df, _name in [(df_train,\"df_train\"), (df_test,\"df_test\"), (df_val,\"df_val\")]:\n",
    "    assert label_col in _df.columns, f\"{label_col} missing from {_name}\"\n",
    "\n",
    "print(\"Train shape:\", df_train.shape)\n",
    "print(\"Test  shape:\", df_test.shape)\n",
    "print(\"Val   shape:\", df_val.shape)\n",
    "print(\"Label prevalence (train):\", df_train[label_col].mean().round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98e6a393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric features: ['total_items', 'total_price', 'total_freight', 'payment_value_sum', 'payment_installments_max', 'delivery_time_days', 'estimated_time_days', 'delivered_late']\n",
      "Categorical features: ['customer_state', 'payment_types']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_items</th>\n",
       "      <th>total_price</th>\n",
       "      <th>total_freight</th>\n",
       "      <th>payment_value_sum</th>\n",
       "      <th>payment_installments_max</th>\n",
       "      <th>delivery_time_days</th>\n",
       "      <th>estimated_time_days</th>\n",
       "      <th>delivered_late</th>\n",
       "      <th>customer_state</th>\n",
       "      <th>payment_types</th>\n",
       "      <th>label_satisfied</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>72.89</td>\n",
       "      <td>63.34</td>\n",
       "      <td>136.23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.206227</td>\n",
       "      <td>45.114363</td>\n",
       "      <td>0</td>\n",
       "      <td>RR</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>59.50</td>\n",
       "      <td>15.56</td>\n",
       "      <td>75.06</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.206227</td>\n",
       "      <td>52.989190</td>\n",
       "      <td>0</td>\n",
       "      <td>RS</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>86.90</td>\n",
       "      <td>17.16</td>\n",
       "      <td>40.95</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.206227</td>\n",
       "      <td>16.358113</td>\n",
       "      <td>0</td>\n",
       "      <td>SP</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>134.97</td>\n",
       "      <td>8.49</td>\n",
       "      <td>105.28</td>\n",
       "      <td>2.0</td>\n",
       "      <td>54.813194</td>\n",
       "      <td>18.488449</td>\n",
       "      <td>1</td>\n",
       "      <td>SP</td>\n",
       "      <td>UNK</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>9.34</td>\n",
       "      <td>109.34</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.206227</td>\n",
       "      <td>22.077870</td>\n",
       "      <td>0</td>\n",
       "      <td>SP</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_items  total_price  total_freight  payment_value_sum  \\\n",
       "0          2.0        72.89          63.34             136.23   \n",
       "1          1.0        59.50          15.56              75.06   \n",
       "2          1.0        86.90          17.16              40.95   \n",
       "3          3.0       134.97           8.49             105.28   \n",
       "4          1.0       100.00           9.34             109.34   \n",
       "\n",
       "   payment_installments_max  delivery_time_days  estimated_time_days  \\\n",
       "0                       1.0           10.206227            45.114363   \n",
       "1                       3.0           10.206227            52.989190   \n",
       "2                       2.0           10.206227            16.358113   \n",
       "3                       2.0           54.813194            18.488449   \n",
       "4                       1.0           10.206227            22.077870   \n",
       "\n",
       "   delivered_late customer_state payment_types  label_satisfied  \n",
       "0               0             RR   credit_card                0  \n",
       "1               0             RS   credit_card                0  \n",
       "2               0             SP   credit_card                0  \n",
       "3               1             SP           UNK                0  \n",
       "4               0             SP   credit_card                0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature configuration \n",
    "num_features = [\n",
    "    \"total_items\",\n",
    "    \"total_price\",\n",
    "    \"total_freight\",\n",
    "    \"payment_value_sum\",\n",
    "    \"payment_installments_max\",\n",
    "    \"delivery_time_days\",\n",
    "    \"estimated_time_days\",\n",
    "    \"delivered_late\",\n",
    "]\n",
    "cat_features = [\"customer_state\", \"payment_types\"]\n",
    "\n",
    "# Keep only existing columns \n",
    "num_features = [c for c in num_features if c in df_train.columns]\n",
    "cat_features = [c for c in cat_features if c in df_train.columns]\n",
    "\n",
    "print(\"Numeric features:\", num_features)\n",
    "print(\"Categorical features:\", cat_features)\n",
    "\n",
    "def make_model_frame(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    cols = num_features + cat_features + [label_col]\n",
    "    out = df[cols].copy()\n",
    "    # Ensure types\n",
    "    for c in num_features:\n",
    "        out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "    out[num_features] = out[num_features].fillna(out[num_features].median(numeric_only=True))\n",
    "    for c in cat_features:\n",
    "        out[c] = out[c].fillna(\"UNK\").astype(str)\n",
    "    out[label_col] = out[label_col].astype(int)\n",
    "    return out\n",
    "\n",
    "train_df = make_model_frame(df_train)\n",
    "test_df  = make_model_frame(df_test)\n",
    "val_df   = make_model_frame(df_val)\n",
    "\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f11b1c",
   "metadata": {},
   "source": [
    "### Baseline Model\n",
    "As a benchmark, we implemented a simple heuristic model that always predicts an order will be **Satisfied** (Class 1). This reflects the majority class (~77%) in the dataset.\n",
    "\n",
    "**Performance Note:**\n",
    "- Since the model always predicts the positive class (1), it achieves **perfect Recall (1.0)** for that class.\n",
    "- However, it **completely fails to identify any Late deliveries (Class 0)**, which is the critical minority case for this business problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d9b4dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark A — DummyClassifier: {'accuracy': 0.7559541907367995, 'precision': 0.7559541907367995, 'recall': 1.0, 'f1': 0.8610181230520605}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "X_train = train_df.drop(columns=[label_col])\n",
    "y_train = train_df[label_col].values\n",
    "\n",
    "X_test = test_df.drop(columns=[label_col])\n",
    "y_test = test_df[label_col].values\n",
    "\n",
    "# Benchmark A: majority-class baseline\n",
    "dummy = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy.fit(X_train, y_train)\n",
    "pred_dummy = dummy.predict(X_test)\n",
    "\n",
    "def classification_metrics(y_true, y_pred, y_score=None):\n",
    "    out = {\n",
    "        \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
    "        \"precision\": float(precision_score(y_true, y_pred, zero_division=0)),\n",
    "        \"recall\": float(recall_score(y_true, y_pred, zero_division=0)),\n",
    "        \"f1\": float(f1_score(y_true, y_pred, zero_division=0)),\n",
    "    }\n",
    "    if y_score is not None:\n",
    "        try:\n",
    "            out[\"roc_auc\"] = float(roc_auc_score(y_true, y_score))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return out\n",
    "\n",
    "metrics_dummy = classification_metrics(y_test, pred_dummy)\n",
    "print(\"Benchmark A — DummyClassifier:\", metrics_dummy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a33ff44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark B — Tiny LogisticRegression: {'accuracy': 0.7876760920239181, 'precision': 0.7911419887103778, 'recall': 0.9770746748893954, 'f1': 0.8743326735048887, 'roc_auc': 0.639013567635967}\n"
     ]
    }
   ],
   "source": [
    "# Benchmark B: logistic regression on features\n",
    "# delivered_late + delivery_time_days + total_price\n",
    "tiny_feats = [c for c in [\"delivered_late\", \"delivery_time_days\", \"total_price\"] if c in X_train.columns]\n",
    "assert len(tiny_feats) >= 1, \"No tiny benchmark features found; adjust tiny_feats list.\"\n",
    "\n",
    "pre = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", \"passthrough\", tiny_feats),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "bench_lr = Pipeline(steps=[\n",
    "    (\"pre\", pre),\n",
    "    (\"clf\", LogisticRegression(max_iter=200, n_jobs=None)),\n",
    "])\n",
    "bench_lr.fit(X_train, y_train)\n",
    "\n",
    "pred_lr = bench_lr.predict(X_test)\n",
    "proba_lr = None\n",
    "if hasattr(bench_lr.named_steps[\"clf\"], \"predict_proba\"):\n",
    "    proba_lr = bench_lr.predict_proba(X_test)[:, 1]\n",
    "\n",
    "metrics_lr = classification_metrics(y_test, pred_lr, y_score=proba_lr)\n",
    "print(\"Benchmark B — Tiny LogisticRegression:\", metrics_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b421af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((39469, 42), (9867, 42), (9867, 42))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Full preprocessing for model training \n",
    "ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", \"passthrough\", num_features),\n",
    "        (\"cat\", ohe, cat_features),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "# Fit on train only\n",
    "X_train_mat = preprocess.fit_transform(X_train)\n",
    "X_val_mat   = preprocess.transform(val_df.drop(columns=[label_col]))\n",
    "X_test_mat  = preprocess.transform(X_test)\n",
    "\n",
    "y_val = val_df[label_col].values\n",
    "\n",
    "# Helper to create XGBoost CSV \n",
    "def to_xgb_csv(X_mat, y_vec) -> pd.DataFrame:\n",
    "    y_vec = np.asarray(y_vec).reshape(-1, 1)\n",
    "    arr = np.hstack([y_vec, X_mat])\n",
    "    return pd.DataFrame(arr)\n",
    "\n",
    "train_xgb = to_xgb_csv(X_train_mat, y_train)\n",
    "val_xgb   = to_xgb_csv(X_val_mat, y_val)\n",
    "test_xgb  = to_xgb_csv(X_test_mat, y_test)\n",
    "\n",
    "train_xgb.shape, val_xgb.shape, test_xgb.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7b92afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded dataset prefixes:\n",
      "  train: s3://sagemaker-us-east-1-587322031938/modeling/xgb-baseline/train/\n",
      "  val  : s3://sagemaker-us-east-1-587322031938/modeling/xgb-baseline/val/\n",
      "  test : s3://sagemaker-us-east-1-587322031938/modeling/xgb-baseline/test/\n"
     ]
    }
   ],
   "source": [
    "import awswrangler as wr\n",
    "\n",
    "if \"DATALAKE_BUCKET\" in globals() and isinstance(DATALAKE_BUCKET, str) and len(DATALAKE_BUCKET) > 0:\n",
    "    bucket = DATALAKE_BUCKET\n",
    "else:\n",
    "    bucket = sm_sess.default_bucket()\n",
    "\n",
    "base_prefix = f\"s3://{bucket}/modeling/xgb-baseline/\"\n",
    "\n",
    "train_prefix = base_prefix + \"train/\"\n",
    "val_prefix   = base_prefix + \"val/\"\n",
    "test_prefix  = base_prefix + \"test/\"\n",
    "\n",
    "# Writes one or more CSV files under each prefix, overwriting existing data\n",
    "wr.s3.to_csv(train_xgb, path=train_prefix, index=False, header=False, dataset=True, mode=\"overwrite\")\n",
    "wr.s3.to_csv(val_xgb,   path=val_prefix,   index=False, header=False, dataset=True, mode=\"overwrite\")\n",
    "wr.s3.to_csv(test_xgb,  path=test_prefix,  index=False, header=False, dataset=True, mode=\"overwrite\")\n",
    "\n",
    "print(\"Uploaded dataset prefixes:\")\n",
    "print(\"  train:\", train_prefix)\n",
    "print(\"  val  :\", val_prefix)\n",
    "print(\"  test :\", test_prefix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bf9545",
   "metadata": {},
   "source": [
    "### First Iteration Model (XGBoost v1)\n",
    "We trained a first-pass XGBoost binary classifier in Amazon SageMaker using a limited set of engineered features related to order size, payment behavior, and purchase timing.\n",
    "\n",
    "The model was evaluated using SageMaker Batch Transform on the held-out test dataset. Batch Transform was selected over a real-time endpoint to minimize cost and ensure automatic resource cleanup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fix_missing_defs",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n"
     ]
    }
   ],
   "source": [
    "# Define paths used in training/validation\n",
    "s3_train = train_prefix\n",
    "s3_val   = val_prefix\n",
    "s3_test  = test_prefix\n",
    "\n",
    "output_path = f\"s3://{bucket}/modeling/output\"\n",
    "transform_output = base_prefix + \"transform-output/\"\n",
    "\n",
    "# Image URI\n",
    "xgb_image = image_uris.retrieve(\n",
    "    framework=\"xgboost\",\n",
    "    region=region,\n",
    "    version=\"1.5-1\"\n",
    ")\n",
    "\n",
    "# Estimator Definition\n",
    "xgb = sagemaker.estimator.Estimator(\n",
    "    image_uri=xgb_image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    output_path=output_path,\n",
    "    sagemaker_session=sm_sess\n",
    ")\n",
    "\n",
    "xgb.set_hyperparameters(\n",
    "    max_depth=5,\n",
    "    eta=0.2,\n",
    "    gamma=4,\n",
    "    min_child_weight=6,\n",
    "    subsample=0.8,\n",
    "    objective=\"binary:logistic\",\n",
    "    num_round=50\n",
    ")\n",
    "\n",
    "# Transformer Definition\n",
    "transformer = xgb.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    output_path=transform_output\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e70aa4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2026-02-17-19-39-09-126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing training artifacts found. Starting Training...\n",
      "\n",
      "2026-02-17 19:39:10 Starting - Starting the training job..\n",
      "2026-02-17 19:39:25 Starting - Preparing the instances for training..\n",
      "2026-02-17 19:39:42 Downloading - Downloading input data....\n",
      "2026-02-17 19:40:08 Downloading - Downloading the training image........\n",
      "2026-02-17 19:40:53 Training - Training image download completed. Training in progress...\n",
      "2026-02-17 19:41:09 Uploading - Uploading generated training model..\n",
      "2026-02-17 19:41:22 Completed - Training job completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-xgboost-2026-02-17-19-41-25-768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Re-created transformer object linked to current model.\n"
     ]
    }
   ],
   "source": [
    "train_input = TrainingInput(s3_data=os.path.dirname(s3_train) + \"/\", content_type=\"text/csv\")\n",
    "val_input   = TrainingInput(s3_data=os.path.dirname(s3_val) + \"/\",   content_type=\"text/csv\")\n",
    "\n",
    "# --- COST SAFETY CHECK ---\n",
    "import boto3\n",
    "import time\n",
    "from urllib.parse import urlparse\n",
    "from sagemaker.model import Model\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "def check_s3_prefix_has_contents(bucket_name, prefix):\n",
    "    resp = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "    return resp.get('KeyCount', 0) > 0\n",
    "\n",
    "# Parse the output path defined in previous cells\n",
    "p = urlparse(output_path)\n",
    "out_bucket = p.netloc\n",
    "out_key_prefix = p.path.lstrip('/')\n",
    "\n",
    "if check_s3_prefix_has_contents(out_bucket, out_key_prefix):\n",
    "    print(f'Found existing training artifacts in {output_path}. Skipping Training to save cost.')\n",
    "    # Find latest model artifact\n",
    "    resp = s3_client.list_objects_v2(Bucket=out_bucket, Prefix=out_key_prefix)\n",
    "    contents = sorted(resp.get('Contents', []), key=lambda x: x['LastModified'], reverse=True)\n",
    "    model_uri = None\n",
    "    for c in contents:\n",
    "        if c['Key'].endswith('/output/model.tar.gz'):\n",
    "            model_uri = f's3://{out_bucket}/{c[\"Key\"]}'\n",
    "            break\n",
    "    if model_uri:\n",
    "        print(f'   Using latest model artifact: {model_uri}')\n",
    "        # Recreate Estimator/Model so next cells work\n",
    "        xgb_model = Model(\n",
    "            image_uri=xgb_image,\n",
    "            model_data=model_uri,\n",
    "            role=role,\n",
    "            sagemaker_session=sm_sess\n",
    "        )\n",
    "        xgb_model.create()\n",
    "        # Swap xgb (Estimator) to xgb_model (Model) for transformer usage\n",
    "        xgb = xgb_model\n",
    "    else:\n",
    "        print('   Output dir exists but no model found. Retraining...')\n",
    "        xgb.fit({'train': train_input, 'validation': val_input}, logs=False)\n",
    "else:\n",
    "    print('No existing training artifacts found. Starting Training...')\n",
    "    xgb.fit({'train': train_input, 'validation': val_input}, logs=False)\n",
    "\n",
    "# --- FIX: Re-instantiate Transformer ---\n",
    "# Ensure transformer uses the correct model (whether trained now or loaded from S3)\n",
    "transformer = xgb.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    output_path=transform_output\n",
    ")\n",
    "print(\"Re-created transformer object linked to current model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c1829fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating transform job with name: sagemaker-xgboost-2026-02-17-19-48-56-610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform input prefix (features): s3://sagemaker-us-east-1-587322031938/modeling/xgb-baseline/test/features_only/\n",
      "Transform output path            : s3://sagemaker-us-east-1-587322031938/modeling/xgb-baseline/transform-output/\n",
      "No existing transform output. Starting Batch Transform...\n",
      ".............................\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[34m[2026-02-17:19:53:38:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-02-17:19:53:38:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-02-17:19:53:38:INFO] nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[35m[2026-02-17:19:53:38:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2026-02-17:19:53:38:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2026-02-17:19:53:38:INFO] nginx config: \u001b[0m\n",
      "\u001b[35mworker_processes auto;\u001b[0m\n",
      "\u001b[35mdaemon off;\u001b[0m\n",
      "\u001b[35mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[35merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[35mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[35mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2026-02-17 19:53:38 +0000] [20] [INFO] Starting gunicorn 19.10.0\u001b[0m\n",
      "\u001b[34m[2026-02-17 19:53:38 +0000] [20] [INFO] Listening at: unix:/tmp/gunicorn.sock (20)\u001b[0m\n",
      "\u001b[34m[2026-02-17 19:53:38 +0000] [20] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.8/os.py:1023: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  return io.open(fd, *args, **kwargs)\u001b[0m\n",
      "\u001b[34m[2026-02-17 19:53:38 +0000] [26] [INFO] Booting worker with pid: 26\u001b[0m\n",
      "\u001b[34m[2026-02-17 19:53:38 +0000] [27] [INFO] Booting worker with pid: 27\u001b[0m\n",
      "\u001b[34m[2026-02-17 19:53:38 +0000] [28] [INFO] Booting worker with pid: 28\u001b[0m\n",
      "\u001b[34m[2026-02-17 19:53:38 +0000] [29] [INFO] Booting worker with pid: 29\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[34m[2026-02-17:19:53:41:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-02-17:19:53:41:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2026-02-17:19:53:41:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[34m[2026-02-17:19:53:41:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-02-17:19:53:41:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2026-02-17:19:53:41:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[35mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35m[2026-02-17 19:53:38 +0000] [20] [INFO] Starting gunicorn 19.10.0\u001b[0m\n",
      "\u001b[35m[2026-02-17 19:53:38 +0000] [20] [INFO] Listening at: unix:/tmp/gunicorn.sock (20)\u001b[0m\n",
      "\u001b[35m[2026-02-17 19:53:38 +0000] [20] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.8/os.py:1023: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  return io.open(fd, *args, **kwargs)\u001b[0m\n",
      "\u001b[35m[2026-02-17 19:53:38 +0000] [26] [INFO] Booting worker with pid: 26\u001b[0m\n",
      "\u001b[35m[2026-02-17 19:53:38 +0000] [27] [INFO] Booting worker with pid: 27\u001b[0m\n",
      "\u001b[35m[2026-02-17 19:53:38 +0000] [28] [INFO] Booting worker with pid: 28\u001b[0m\n",
      "\u001b[35m[2026-02-17 19:53:38 +0000] [29] [INFO] Booting worker with pid: 29\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[35m[2026-02-17:19:53:41:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2026-02-17:19:53:41:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[35m[2026-02-17:19:53:41:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[35m[2026-02-17:19:53:41:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2026-02-17:19:53:41:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[35m[2026-02-17:19:53:41:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[34m[2026-02-17:19:53:41:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-02-17:19:53:41:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2026-02-17:19:53:41:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[34m[2026-02-17:19:53:41:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-02-17:19:53:41:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2026-02-17:19:53:41:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[35m[2026-02-17:19:53:41:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2026-02-17:19:53:41:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[35m[2026-02-17:19:53:41:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[35m[2026-02-17:19:53:41:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2026-02-17:19:53:41:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[35m[2026-02-17:19:53:41:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[34m[2026-02-17:19:53:44:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [17/Feb/2026:19:53:44 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [17/Feb/2026:19:53:44 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2026-02-17:19:53:44:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-02-17:19:53:44:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2026-02-17:19:53:44:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [17/Feb/2026:19:53:44 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [17/Feb/2026:19:53:44 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2026-02-17:19:53:44:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2026-02-17:19:53:44:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/core.py:105: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [17/Feb/2026:19:53:44 +0000] \"POST /invocations HTTP/1.1\" 200 186889 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.8/site-packages/xgboost/core.py:105: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [17/Feb/2026:19:53:44 +0000] \"POST /invocations HTTP/1.1\" 200 186889 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2026-02-17T19:53:44.071:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\n",
      "\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[34m[2026-02-17:19:53:38:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-02-17:19:53:38:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-02-17:19:53:38:INFO] nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[35m[2026-02-17:19:53:38:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2026-02-17:19:53:38:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2026-02-17:19:53:38:INFO] nginx config: \u001b[0m\n",
      "\u001b[35mworker_processes auto;\u001b[0m\n",
      "\u001b[35mdaemon off;\u001b[0m\n",
      "\u001b[35mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[35merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[35mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[35mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2026-02-17 19:53:38 +0000] [20] [INFO] Starting gunicorn 19.10.0\u001b[0m\n",
      "\u001b[34m[2026-02-17 19:53:38 +0000] [20] [INFO] Listening at: unix:/tmp/gunicorn.sock (20)\u001b[0m\n",
      "\u001b[34m[2026-02-17 19:53:38 +0000] [20] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.8/os.py:1023: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  return io.open(fd, *args, **kwargs)\u001b[0m\n",
      "\u001b[34m[2026-02-17 19:53:38 +0000] [26] [INFO] Booting worker with pid: 26\u001b[0m\n",
      "\u001b[34m[2026-02-17 19:53:38 +0000] [27] [INFO] Booting worker with pid: 27\u001b[0m\n",
      "\u001b[34m[2026-02-17 19:53:38 +0000] [28] [INFO] Booting worker with pid: 28\u001b[0m\n",
      "\u001b[34m[2026-02-17 19:53:38 +0000] [29] [INFO] Booting worker with pid: 29\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[34m[2026-02-17:19:53:41:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-02-17:19:53:41:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2026-02-17:19:53:41:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[34m[2026-02-17:19:53:41:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-02-17:19:53:41:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2026-02-17:19:53:41:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[35mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35m[2026-02-17 19:53:38 +0000] [20] [INFO] Starting gunicorn 19.10.0\u001b[0m\n",
      "\u001b[35m[2026-02-17 19:53:38 +0000] [20] [INFO] Listening at: unix:/tmp/gunicorn.sock (20)\u001b[0m\n",
      "\u001b[35m[2026-02-17 19:53:38 +0000] [20] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.8/os.py:1023: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  return io.open(fd, *args, **kwargs)\u001b[0m\n",
      "\u001b[35m[2026-02-17 19:53:38 +0000] [26] [INFO] Booting worker with pid: 26\u001b[0m\n",
      "\u001b[35m[2026-02-17 19:53:38 +0000] [27] [INFO] Booting worker with pid: 27\u001b[0m\n",
      "\u001b[35m[2026-02-17 19:53:38 +0000] [28] [INFO] Booting worker with pid: 28\u001b[0m\n",
      "\u001b[35m[2026-02-17 19:53:38 +0000] [29] [INFO] Booting worker with pid: 29\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[35m[2026-02-17:19:53:41:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2026-02-17:19:53:41:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[35m[2026-02-17:19:53:41:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[35m[2026-02-17:19:53:41:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2026-02-17:19:53:41:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[35m[2026-02-17:19:53:41:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[34m[2026-02-17:19:53:41:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-02-17:19:53:41:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2026-02-17:19:53:41:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[34m[2026-02-17:19:53:41:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-02-17:19:53:41:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2026-02-17:19:53:41:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[35m[2026-02-17:19:53:41:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2026-02-17:19:53:41:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[35m[2026-02-17:19:53:41:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[35m[2026-02-17:19:53:41:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2026-02-17:19:53:41:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[35m[2026-02-17:19:53:41:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[34m[2026-02-17:19:53:44:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [17/Feb/2026:19:53:44 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [17/Feb/2026:19:53:44 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2026-02-17:19:53:44:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-02-17:19:53:44:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2026-02-17:19:53:44:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [17/Feb/2026:19:53:44 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [17/Feb/2026:19:53:44 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2026-02-17:19:53:44:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2026-02-17:19:53:44:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/core.py:105: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [17/Feb/2026:19:53:44 +0000] \"POST /invocations HTTP/1.1\" 200 186889 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.8/site-packages/xgboost/core.py:105: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [17/Feb/2026:19:53:44 +0000] \"POST /invocations HTTP/1.1\" 200 186889 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2026-02-17T19:53:44.071:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "Batch transform complete.\n"
     ]
    }
   ],
   "source": [
    "# For transform, we provide features only\n",
    "test_features_only = test_xgb.drop(columns=[0])  \n",
    "\n",
    "# Write as a dataset under a prefix \n",
    "test_features_prefix = f\"{base_prefix}test/features_only/\"\n",
    "\n",
    "wr.s3.to_csv(\n",
    "    test_features_only,\n",
    "    path=test_features_prefix,\n",
    "    index=False,\n",
    "    header=False,\n",
    "    dataset=True,\n",
    "    mode=\"overwrite\",\n",
    ")\n",
    "\n",
    "print(\"Transform input prefix (features):\", test_features_prefix)\n",
    "print(\"Transform output path            :\", transform_output)\n",
    "\n",
    "# --- COST SAFETY CHECK ---\n",
    "t_parse = urlparse(transform_output)\n",
    "t_bucket = t_parse.netloc\n",
    "t_prefix = t_parse.path.lstrip('/')\n",
    "\n",
    "if check_s3_prefix_has_contents(t_bucket, t_prefix):\n",
    "    print(f'Found existing transform output in {transform_output}. Skipping Transform.')\n",
    "else:\n",
    "    print('No existing transform output. Starting Batch Transform...')\n",
    "    transformer.transform(\n",
    "        data=test_features_prefix,\n",
    "        content_type='text/csv',\n",
    "        split_type='Line',\n",
    "    )\n",
    "    transformer.wait()\n",
    "    print('Batch transform complete.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09f60821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output objects: ['modeling/xgb-baseline/transform-output/6fd3245b847643078fcadf0d7767f540.csv.out']\n",
      "Using output file: modeling/xgb-baseline/transform-output/6fd3245b847643078fcadf0d7767f540.csv.out\n",
      "SageMaker XGBoost metrics: {'accuracy': 0.8098712881321577, 'precision': 0.8079426365140651, 'recall': 0.9819010591232069, 'f1': 0.8864681675139191, 'roc_auc': 0.7298518167310201}\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8317    0.2770    0.4156      2408\n",
      "           1     0.8079    0.9819    0.8865      7459\n",
      "\n",
      "    accuracy                         0.8099      9867\n",
      "   macro avg     0.8198    0.6294    0.6510      9867\n",
      "weighted avg     0.8137    0.8099    0.7715      9867\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "# List objects under output prefix to find the output file\n",
    "s3 = boto3.client(\"s3\")\n",
    "# Fix: Use the actual transform output path\n",
    "parsed_out = urlparse(transform_output)\n",
    "out_prefix = parsed_out.path.lstrip('/')\n",
    "\n",
    "resp = s3.list_objects_v2(Bucket=bucket, Prefix=out_prefix)\n",
    "keys = [obj[\"Key\"] for obj in resp.get(\"Contents\", [])]\n",
    "print(\"Output objects:\", keys)\n",
    "\n",
    "out_files = [k for k in keys if k.endswith(\".out\") or k.endswith(\".csv\") or \"test_features\" in k]\n",
    "\n",
    "candidate = None\n",
    "for k in keys:\n",
    "    if k.endswith(\".out\"):\n",
    "        candidate = k\n",
    "        break\n",
    "if candidate is None:\n",
    "    raise RuntimeError(\"Could not find batch transform output .out file. Check S3 output prefix listing above.\")\n",
    "\n",
    "print(\"Using output file:\", candidate)\n",
    "\n",
    "obj = s3.get_object(Bucket=bucket, Key=candidate)\n",
    "raw = obj[\"Body\"].read().decode(\"utf-8\").strip().splitlines()\n",
    "\n",
    "# Each line is a probability \n",
    "y_score = np.array([float(x.strip().split(\",\")[0]) for x in raw])\n",
    "y_pred = (y_score >= 0.5).astype(int)\n",
    "\n",
    "metrics_xgb = classification_metrics(y_test, y_pred, y_score=y_score)\n",
    "\n",
    "print(\"SageMaker XGBoost metrics:\", metrics_xgb)\n",
    "print()\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a84a50a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Benchmark A: Dummy (most_frequent)</td>\n",
       "      <td>0.755954</td>\n",
       "      <td>0.755954</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.861018</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Benchmark B: Tiny LR (delivered_late, delivery...</td>\n",
       "      <td>0.787676</td>\n",
       "      <td>0.791142</td>\n",
       "      <td>0.977075</td>\n",
       "      <td>0.874333</td>\n",
       "      <td>0.639014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SageMaker: XGBoost (batch transform)</td>\n",
       "      <td>0.809871</td>\n",
       "      <td>0.807943</td>\n",
       "      <td>0.981901</td>\n",
       "      <td>0.886468</td>\n",
       "      <td>0.729852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               model  accuracy  precision  \\\n",
       "0                 Benchmark A: Dummy (most_frequent)  0.755954   0.755954   \n",
       "1  Benchmark B: Tiny LR (delivered_late, delivery...  0.787676   0.791142   \n",
       "2               SageMaker: XGBoost (batch transform)  0.809871   0.807943   \n",
       "\n",
       "     recall        f1   roc_auc  \n",
       "0  1.000000  0.861018       NaN  \n",
       "1  0.977075  0.874333  0.639014  \n",
       "2  0.981901  0.886468  0.729852  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "*** SIGTERM received at time=1771358158 on cpu 1 ***\n",
      "PC: @     0x7f8938ab1e9e  (unknown)  epoll_wait\n",
      "    @     0x7f88e0245b0d         64  absl::lts_20240722::AbslFailureSignalHandler()\n",
      "    @     0x7f89389ce520  (unknown)  (unknown)\n",
      "[2026-02-17 19:55:58,328 E 2706 2706] logging.cc:497: *** SIGTERM received at time=1771358158 on cpu 1 ***\n",
      "[2026-02-17 19:55:58,328 E 2706 2706] logging.cc:497: PC: @     0x7f8938ab1e9e  (unknown)  epoll_wait\n",
      "[2026-02-17 19:55:58,329 E 2706 2706] logging.cc:497:     @     0x7f88e0245b39         64  absl::lts_20240722::AbslFailureSignalHandler()\n",
      "[2026-02-17 19:55:58,329 E 2706 2706] logging.cc:497:     @     0x7f89389ce520  (unknown)  (unknown)\n"
     ]
    }
   ],
   "source": [
    "# Side-by-side comparison\n",
    "compare = pd.DataFrame([\n",
    "    {\"model\": \"Benchmark A: Dummy (most_frequent)\", **metrics_dummy},\n",
    "    {\"model\": f\"Benchmark B: Tiny LR ({', '.join(tiny_feats)})\", **metrics_lr},\n",
    "    {\"model\": \"SageMaker: XGBoost (batch transform)\", **metrics_xgb},\n",
    "])\n",
    "\n",
    "# Reorder columns\n",
    "cols = [\"model\"] + [c for c in [\"accuracy\",\"precision\",\"recall\",\"f1\",\"roc_auc\"] if c in compare.columns]\n",
    "compare = compare[cols]\n",
    "compare\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b90448",
   "metadata": {},
   "source": [
    "### Results Summary\n",
    "- The XGBoost model achieved an **AUC of approximately 0.73**, indicating it learned discriminative patterns significantly better than the baseline.\n",
    "- **Accuracy (0.81)** outperformed the majority-class baseline (0.76), demonstrating real predictive power.\n",
    "- **Precision (~0.81)** and **Recall (~0.98)** were strong, showing the model effectively identifies positive cases while maintaining reasonable correctness.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
