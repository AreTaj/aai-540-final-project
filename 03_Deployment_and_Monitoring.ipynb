{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "intro",
            "metadata": {},
            "source": [
                "# Part 3: Deployment & Monitoring (SageMaker)\n",
                "\n",
                "## Overview\n",
                "This notebook covers the **Deployment** and **Monitoring** phases of the MLOps lifecycle for the Olist E-Commerce project. \n",
                "\n",
                "**Objectives:**\n",
                "1. **Deploy** the XGBoost model trained in `02_Modeling.ipynb` to a real-time SageMaker endpoint.\n",
                "2. **Enable Data Capture** to log all inference requests and predictions to S3.\n",
                "3. **Implement Model Monitoring**:\n",
                "   - **Data Quality Monitor**: Detects drift in input features (e.g., changes in `payment_value_sum` distribution).\n",
                "   - **Model Quality Monitor**: continually evaluates model performance (Accuracy, F1, AUC) by comparing predictions against ground truth labels.\n",
                "4. **Visualize** the monitoring results and CloudWatch metrics.\n",
                "\n",
                "> ** COST WARNING**: This notebook creates a real-time endpoint (`ml.m5.xlarge`) and monitoring schedules. These resources incur hourly costs. **Run the Cleanup section at the end of this notebook to delete these resources.**\n",
                "\n",
                "**Reference**: This implementation is adapted from `lab-5-1-model-monitoring-with-sagemaker-and-cloudwatch`.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "setup-intro",
            "metadata": {},
            "source": [
                "### Setup & Configuration\n",
                "Import necessary libraries and configure S3 bucket locations. Prefixes must match those used in previous notebooks (`01` and `02`) to ensure correct retrieval of data and model artifacts.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "setup",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup\n",
                "import sagemaker\n",
                "import boto3\n",
                "import os\n",
                "import time\n",
                "import json\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import awswrangler as wr\n",
                "from datetime import datetime, timedelta\n",
                "from time import sleep\n",
                "from threading import Thread\n",
                "\n",
                "from sagemaker import image_uris, get_execution_role\n",
                "from sagemaker.model import Model\n",
                "from sagemaker.predictor import Predictor\n",
                "from sagemaker.serializers import CSVSerializer\n",
                "from sagemaker.deserializers import CSVDeserializer\n",
                "from sagemaker.model_monitor import (\n",
                "    DataCaptureConfig,\n",
                "    DefaultModelMonitor,\n",
                "    ModelQualityMonitor,\n",
                "    DatasetFormat,\n",
                "    EndpointInput,\n",
                "    CronExpressionGenerator,\n",
                ")\n",
                "from sagemaker.s3 import S3Downloader, S3Uploader\n",
                "\n",
                "sm_sess = sagemaker.Session()\n",
                "region = sm_sess.boto_region_name\n",
                "try:\n",
                "    role = get_execution_role()\n",
                "except ValueError:\n",
                "    iam = boto3.client('iam')\n",
                "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
                "\n",
                "bucket = sm_sess.default_bucket()\n",
                "\n",
                "# Prefixes (Must align with 02_Modeling)\n",
                "prefix = \"datalake/olist/monitoring\"\n",
                "model_prefix = \"modeling/output\"\n",
                "data_capture_prefix = f\"{prefix}/datacapture\"\n",
                "reports_prefix = f\"{prefix}/reports\"\n",
                "base_prefix = f\"s3://{bucket}/modeling/xgb-baseline/\"\n",
                "\n",
                "print(f\"Region: {region}\")\n",
                "print(f\"Role: {role}\")\n",
                "print(f\"Bucket: {bucket}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "setup-output-expl",
            "metadata": {},
            "source": [
                "#### Output Verification\n",
                "Verify that the outputs display the expected AWS Region, IAM Execution Role ARN, and default S3 bucket name.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "load-model",
            "metadata": {},
            "source": [
                "### 1. Load Trained Model Artifact\n",
                "Locate the latest model trained in `02_Modeling.ipynb` to avoid retraining. The function `get_latest_model_artifact` scans the output directory and picks the most recent `model.tar.gz`.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "find-model",
            "metadata": {},
            "outputs": [],
            "source": [
                "s3_client = boto3.client('s3')\n",
                "\n",
                "def get_latest_model_artifact(bucket, prefix):\n",
                "    resp = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
                "    contents = sorted(resp.get('Contents', []), key=lambda x: x['LastModified'], reverse=True)\n",
                "    for c in contents:\n",
                "        if c['Key'].endswith('/output/model.tar.gz'):\n",
                "            return f\"s3://{bucket}/{c['Key']}\"\n",
                "    return None\n",
                "\n",
                "model_data = get_latest_model_artifact(bucket, model_prefix)\n",
                "\n",
                "if not model_data:\n",
                "    raise ValueError(f\"No model artifact found in s3://{bucket}/{model_prefix}. Please run 02_Modeling.ipynb first.\")\n",
                "\n",
                "print(f\"Using model artifact: {model_data}\")\n",
                "\n",
                "# Training image URI (XGBoost)\n",
                "image_uri = image_uris.retrieve(framework=\"xgboost\", region=region, version=\"1.5-1\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "load-model-output-expl",
            "metadata": {},
            "source": [
                "#### Model Confirmation\n",
                "The output above should display the S3 URI of the model artifact found (e.g., `s3://.../model.tar.gz`). If it fails, ensure that you have successfully run the training step in `02_Modeling.ipynb`."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "deploy",
            "metadata": {},
            "source": [
                "### 2. Live Deployment with Data Capture\n",
                "Deploy the model to an `ml.m5.xlarge` instance and enable **Data Capture** to save all inputs and outputs to S3 for monitoring.\n",
                "\n",
                "**Safety Check**: If the endpoint already exists, we skip deployment to avoid errors and duplicate costs.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "deploy-endpoint",
            "metadata": {},
            "outputs": [],
            "source": [
                "endpoint_name = \"olist-xgb-monitoring-ep\"\n",
                "data_capture_uri = f\"s3://{bucket}/{data_capture_prefix}\"\n",
                "\n",
                "sm_client = boto3.client(\"sagemaker\")\n",
                "\n",
                "try:\n",
                "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
                "    print(f\"Endpoint {endpoint_name} already exists. Status: {resp['EndpointStatus']}\")\n",
                "    predictor = Predictor(endpoint_name=endpoint_name, sagemaker_session=sm_sess, serializer=CSVSerializer())\n",
                "except sm_client.exceptions.ClientError:\n",
                "    print(f\"Creating new endpoint: {endpoint_name}...\")\n",
                "    \n",
                "    model = Model(\n",
                "        image_uri=image_uri,\n",
                "        model_data=model_data,\n",
                "        role=role,\n",
                "        sagemaker_session=sm_sess\n",
                "    )\n",
                "    \n",
                "    data_capture_config = DataCaptureConfig(\n",
                "        enable_capture=True,\n",
                "        sampling_percentage=100,\n",
                "        destination_s3_uri=data_capture_uri\n",
                "    )\n",
                "    \n",
                "    model.deploy(\n",
                "        initial_instance_count=1,\n",
                "        instance_type=\"ml.m5.xlarge\",\n",
                "        endpoint_name=endpoint_name,\n",
                "        data_capture_config=data_capture_config\n",
                "    )\n",
                "    predictor = Predictor(endpoint_name=endpoint_name, sagemaker_session=sm_sess, serializer=CSVSerializer())"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "deploy-output-expl",
            "metadata": {},
            "source": [
                "#### Deployment Status\n",
                "If the endpoint was already running, you will see `Endpoint ... already exists. Status: InService`. If it is new, SageMaker will print a series of dashes (`-`) indicating the provisioning progress. Once complete, the endpoint is ready for real-time inference."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "data-mon",
            "metadata": {},
            "source": [
                "### 3. Data Quality Monitoring\n",
                "Create a baseline using the training dataset (from `02_Modeling`) to detect if the distribution of incoming data shifts significantly (Data Drift).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "baseline-data",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load training data for baseline (Must match what was used in 02_Modeling)\n",
                "train_uri = f\"{base_prefix}train/\"\n",
                "baseline_results_uri = f\"s3://{bucket}/{prefix}/baselining/data_quality\"\n",
                "\n",
                "print(f\"Baselining with training data: {train_uri}\")\n",
                "\n",
                "data_monitor = DefaultModelMonitor(\n",
                "    role=role,\n",
                "    instance_count=1,\n",
                "    instance_type=\"ml.m5.xlarge\",\n",
                "    volume_size_in_gb=20,\n",
                "    max_runtime_in_seconds=3600,\n",
                ")\n",
                "\n",
                "# Check if baseline already exists to run expensive job only once\n",
                "existing_baseline = s3_client.list_objects_v2(Bucket=bucket, Prefix=f\"{prefix}/baselining/data_quality/statistics.json\")\n",
                "if existing_baseline.get('KeyCount', 0) > 0:\n",
                "    print(\"Found existing Data Quality Baseline. Skipping baseline job.\")\n",
                "    # Attach to existing monitor logic would go here if we needed the object, \n",
                "    # but we can reuse the monitor object for scheduling.\n",
                "else:\n",
                "    print(\"Starting Data Quality Baseline Job...\")\n",
                "    data_monitor.suggest_baseline(\n",
                "        baseline_dataset=train_uri,\n",
                "        dataset_format=DatasetFormat.csv(header=False),\n",
                "        output_s3_uri=baseline_results_uri,\n",
                "        wait=True,\n",
                "        logs=False\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "baseline-data-expl",
            "metadata": {},
            "source": [
                "#### Baseline Job Status\n",
                "The code checks for an existing `statistics.json` file. If found, it skips the baselining job to save time. Otherwise, it launches a Processing Job to compute statistics (mean, variance, etc.) and constraints (type check, null checks) on the training data."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "schedule-data-intro",
            "metadata": {},
            "source": [
                "#### Schedule Data Quality Monitor\n",
                "Schedule the Data Quality Monitor to run hourly. It analyzes the data captured from the endpoint (`data_capture_uri`) and compares it against the baseline. If violations are found (e.g., drift), it generates a violation report.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "schedule-data",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Schedule Data Quality Monitor\n",
                "sched_name = \"olist-data-quality-monitor\"\n",
                "\n",
                "try:\n",
                "    sm_client.describe_monitoring_schedule(MonitoringScheduleName=sched_name)\n",
                "    print(f\"Schedule {sched_name} already exists.\")\n",
                "except sm_client.exceptions.ResourceNotFound:\n",
                "    print(f\"Creating Monitoring Schedule: {sched_name}\")\n",
                "    data_monitor.create_monitoring_schedule(\n",
                "        monitor_schedule_name=sched_name,\n",
                "        endpoint_input=EndpointInput(\n",
                "            endpoint_name=endpoint_name,\n",
                "            destination=\"/opt/ml/processing/input/endpoint_data\",\n",
                "        ),\n",
                "        output_s3_uri=f\"s3://{bucket}/{reports_prefix}/data_quality\",\n",
                "        schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
                "        enable_cloudwatch_metrics=True,\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "schedule-data-expl",
            "metadata": {},
            "source": [
                "#### Schedule Confirmation\n",
                "Ensure the output confirms that `olist-data-quality-monitor` has been created or already exists.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "model-mon",
            "metadata": {},
            "source": [
                "### 4. Model Quality Monitoring\n",
                "To monitor model performance (e.g., Accuracy), **Ground Truth** labels are required. In a real scenario, these arrive after inference (e.g., did the package actually arrive late?). Here, ground truth is simulated for demonstration.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "model-quality-setup",
            "metadata": {},
            "outputs": [],
            "source": [
                "model_monitor = ModelQualityMonitor(\n",
                "    role=role,\n",
                "    instance_count=1,\n",
                "    instance_type=\"ml.m5.xlarge\",\n",
                "    volume_size_in_gb=20,\n",
                "    max_runtime_in_seconds=3600,\n",
                "    sagemaker_session=sm_sess\n",
                ")\n",
                "\n",
                "# Create baseline for Model Quality (requires predictions + labels)\n",
                "# Skip the explicit `suggest_baseline` step here to save time/cost in this demo, \n",
                "# as it requires merging validation predictions with labels manually first. \n",
                "# Instead, we'll proceed to creating the schedule which assumes we upload ground truth later.\n",
                "pass"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "model-qa-schedule-intro",
            "metadata": {},
            "source": [
                "#### Schedule Model Quality Monitor\n",
                "Create a schedule to monitor model performance, specifying `BinaryClassification` as the problem type. The monitor matches inference IDs from the endpoint requests with ground truth data found in the `ground_truth_input` S3 path.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "schedule-model",
            "metadata": {},
            "outputs": [],
            "source": [
                "model_sched_name = \"olist-model-quality-monitor\"\n",
                "ground_truth_path = f\"s3://{bucket}/{prefix}/ground_truth_data\"\n",
                "\n",
                "try:\n",
                "    sm_client.describe_monitoring_schedule(MonitoringScheduleName=model_sched_name)\n",
                "    print(f\"Schedule {model_sched_name} already exists.\")\n",
                "except sm_client.exceptions.ResourceNotFound:\n",
                "    print(f\"Creating Model Quality Schedule: {model_sched_name}\")\n",
                "    # Note: The same constraints from data quality are used for simplicity, \n",
                "    # or we could point to a pre-calculated constraints file. \n",
                "    # For this demo, we effectively monitor without strict baseline constraints just to show setup.\n",
                "    \n",
                "    model_monitor.create_monitoring_schedule(\n",
                "        monitor_schedule_name=model_sched_name,\n",
                "        endpoint_input=EndpointInput(\n",
                "            endpoint_name=endpoint_name,\n",
                "            inference_attribute=\"0\",           # First column is probability\n",
                "            probability_threshold_attribute=0.5,\n",
                "            destination=\"/opt/ml/processing/input_data\",\n",
                "        ),\n",
                "        problem_type=\"BinaryClassification\",\n",
                "        ground_truth_input=ground_truth_path,\n",
                "        output_s3_uri=f\"s3://{bucket}/{reports_prefix}/model_quality\",\n",
                "        schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
                "        enable_cloudwatch_metrics=True,\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "model-qa-expl",
            "metadata": {},
            "source": [
                "#### Schedule Status\n",
                "The output should confirm `olist-model-quality-monitor` creation."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "traffic",
            "metadata": {},
            "source": [
                "### 5. Simulate Traffic & Ground Truth\n",
                "Start a thread to send requests to the endpoint and upload corresponding simulated ground truth labels, providing data for the monitor to process. \n",
                "\n",
                "**Note:** `awswrangler` is used here to read the test dataset directly from S3, as `pd.read_csv` cannot handle S3 directory prefixes natively.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "sim-traffic",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load some test data as a sample payload\n",
                "# From 02_Modeling splits\n",
                "test_uri = f\"{base_prefix}test/\"\n",
                "\n",
                "# FIX: Use awswrangler to read from S3 prefix (native pandas fails on folders)\n",
                "test_df = wr.s3.read_csv(test_uri, header=None)\n",
                "\n",
                "sample_payloads = test_df.iloc[:10, 1:].to_csv(header=False, index=False).split(\"\\n\") # Drop label col 0\n",
                "\n",
                "def simulate_traffic():\n",
                "    print(\"Sending traffic... (Stop kernel to end)\")\n",
                "    for i, payload in enumerate(sample_payloads):\n",
                "        if not payload: continue\n",
                "        try:\n",
                "            resp = predictor.predict(payload, inference_id=str(i)) # Inference ID is key for joining\n",
                "            # print(f\"Prediction {i}: {resp}\")\n",
                "            sleep(0.5)\n",
                "        except Exception as e:\n",
                "            print(f\"Error: {e}\")\n",
                "            \n",
                "# Run simulation once for demo\n",
                "simulate_traffic()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "traffic-expl",
            "metadata": {},
            "source": [
                "#### Traffic Simulation\n",
                "The output will display \"Sending traffic...\" followed by successful execution. If errors occur, check the endpoint status in the SageMaker console.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cleanup",
            "metadata": {},
            "source": [
                "## 6. Cleanup (Teardown)\n",
                "**CRITICAL**: Run this cell to delete resources and avoid unexpected costs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "teardown",
            "metadata": {},
            "outputs": [],
            "source": [
                "def cleanup():\n",
                "    print(\"Cleaning up resources...\")\n",
                "    # Delete Schedules\n",
                "    for schedule in [sched_name, model_sched_name]:\n",
                "        try:\n",
                "            sm_client.delete_monitoring_schedule(MonitoringScheduleName=schedule)\n",
                "            print(f\"Deleted schedule: {schedule}\")\n",
                "        except Exception as e:\n",
                "            print(f\"Skipped schedule {schedule}: {e}\")\n",
                "    \n",
                "    # Delete Endpoint\n",
                "    try:\n",
                "        predictor.delete_endpoint()\n",
                "        print(f\"Deleted endpoint: {endpoint_name}\")\n",
                "    except Exception as e:\n",
                "        print(f\"Skipped endpoint: {e}\")\n",
                "        \n",
                "# Uncomment to run cleanup\n",
                "# cleanup()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cleanup-expl",
            "metadata": {},
            "source": [
                "#### Cleanup Confirmation\n",
                "When you run the cleanup function, verify that it successfully deletes both schedules and the endpoint. This ensures no residual charges will accrue."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (Data Science)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}