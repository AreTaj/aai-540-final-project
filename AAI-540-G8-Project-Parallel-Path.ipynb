{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69584688-1464-4b46-be61-06a594249ce9",
   "metadata": {},
   "source": [
    "# MLOps Pipeline Workflow & Team Notes\n",
    "\n",
    "## Project Overview\n",
    "This notebook implements an end-to-end MLOps data pipeline using the **Olist Brazilian E-Commerce dataset**. The goal was to demonstrate a production-style workflow that covers data ingestion, cataloging, exploratory analysis, feature engineering, feature storage, and dataset splitting — all using AWS services in a cost-efficient way.\n",
    "\n",
    "The pipeline was intentionally built step-by-step to mirror MLOps practices rather than a one-off modeling notebook.\n",
    "\n",
    "---\n",
    "\n",
    "## High-Level Workflow\n",
    "\n",
    "### 1. Raw Data Ingestion (S3 Data Lake)\n",
    "- Created an S3 bucket to act as a data lake.\n",
    "- Uploaded all **9 raw CSV files** from the Kaggle Olist dataset.\n",
    "- Organized raw data under: s3:///raw/olist/ingest_date=YYYY-MM-DD/\n",
    "- Each dataset was placed into its own subfolder to support Athena’s directory-based table requirements.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Data Cataloging & Querying (Athena)\n",
    "- Created an Athena database (`olist_datalake`).\n",
    "- Defined **external tables** for each dataset directly from JupyterLab (no Glue crawler required).\n",
    "- Verified schemas and row counts using Athena queries.\n",
    "- This step enabled SQL-based access to the data and served as the cataloging layer for downstream analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Exploratory Data Analysis (SageMaker + Pandas)\n",
    "- Loaded Athena tables into Pandas using `awswrangler`.\n",
    "- Performed sanity checks on row counts and joins.\n",
    "- Built an **order-level analytical view** by aggregating:\n",
    "- order items\n",
    "- payments\n",
    "- customer attributes\n",
    "- Engineered a target variable (`is_late`) based on delivery vs. estimated delivery dates.\n",
    "- Observed class imbalance (~8% late deliveries), motivating careful splitting and evaluation later.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Feature Engineering\n",
    "- Created leakage-safe, order-level features using only information available at purchase time:\n",
    "- pricing, freight, number of items/sellers\n",
    "- payment information\n",
    "- time-based features (day of week, hour of day)\n",
    "- customer state\n",
    "- Maintained a **canonical feature dataset** for analysis and splitting.\n",
    "- Created a **Feature Store–compatible version** with strict data types.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. SageMaker Feature Store (Offline Store)\n",
    "- Created a **SageMaker Feature Group** (offline store only to control cost).\n",
    "- Used `order_id` as the record identifier.\n",
    "- Used a strictly formatted ISO-8601 `event_time` with UTC (`Z`) as the event time feature.\n",
    "- Successfully ingested ~99k feature records into Feature Store.\n",
    "- Offline store data is persisted in S3 for training and future reuse.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Dataset Splitting (Time-Based)\n",
    "- Performed a **time-based split** using `event_time` to avoid temporal leakage:\n",
    "- Train: ~40%\n",
    "- Validation: ~10%\n",
    "- Test: ~10%\n",
    "- Production reserve: ~40%\n",
    "- Persisted each split as Parquet files to: s3:///splits/olist/features/version=v1/\n",
    "- This mirrors a real production setup where recent data is reserved for inference.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Engineering Decisions & Lessons Learned\n",
    "\n",
    "- **Athena LOCATION must point to directories**, not individual files.\n",
    "- **Feature Store requires strict ISO-8601 timestamps with timezone** — missing the `Z` suffix causes ingestion failures.\n",
    "- Maintaining separate:\n",
    "- canonical feature data (analysis-friendly)\n",
    "- Feature Store–safe data (schema-restricted)\n",
    "is a best practice in real MLOps systems.\n",
    "- Time-based splitting is critical to avoid data leakage in temporal datasets.\n",
    "- Offline Feature Store provides the required functionality while minimizing cost.\n",
    "\n",
    "---\n",
    "\n",
    "## Cost Management Notes\n",
    "- SageMaker compute was stopped immediately after completion.\n",
    "- Feature Store **online store was intentionally disabled** to avoid ongoing charges.\n",
    "- S3 storage costs are minimal and safe to keep until final submission.\n",
    "- Cleanup (Feature Group deletion, S3 cleanup) should only be done **after submission**.\n",
    "\n",
    "---\n",
    "\n",
    "## For Teammates\n",
    "If you need to re-run or extend this work:\n",
    "1. Start at the Athena read step (no need to re-upload raw data).\n",
    "2. Do **not** re-run ingestion unless changing the feature schema.\n",
    "3. Always stop SageMaker compute when finished.\n",
    "\n",
    "This notebook represents a complete, MLOps data pipeline.\n",
    "\n",
    "## Model Benchmark and Evaluation (Module 4)\n",
    "\n",
    "### Baseline Model\n",
    "As a benchmark, we implemented a simple heuristic model that always predicts an order will be delivered on time. This reflects the majority class in the dataset and establishes a lower bound for model performance.\n",
    "\n",
    "Due to class imbalance (~86% of orders are not late), the baseline achieves high accuracy but fails to identify late deliveries, resulting in zero precision, recall, and F1-score.\n",
    "\n",
    "### First Iteration Model (XGBoost v1)\n",
    "We trained a first-pass XGBoost binary classifier in Amazon SageMaker using a limited set of engineered features related to order size, payment behavior, and purchase timing.\n",
    "\n",
    "The model was evaluated using SageMaker Batch Transform on the held-out test dataset. Batch Transform was selected over a real-time endpoint to minimize cost and ensure automatic resource cleanup.\n",
    "\n",
    "### Results Summary\n",
    "- The XGBoost model achieved an AUC of approximately **0.56**, indicating it learned some discriminative signal beyond random chance.\n",
    "- Overall accuracy matched the baseline model due to class imbalance and use of a default classification threshold.\n",
    "- Precision, recall, and F1-score remained low, highlighting the need for future improvements such as class weighting, threshold tuning, and feature expansion.\n",
    "\n",
    "### Key Takeaways\n",
    "This iteration establishes a complete, cost-aware MLOps workflow including data ingestion, feature engineering, model training, evaluation, and deployment via batch inference. While performance improvements are needed, this version serves as a strong baseline for future model iterations and CI/CD integration in later modules.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a684ab4-5185-4603-9f30-358b125dcba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found files: 10\n",
      "raw/olist/ingest_date=2026-01-25/\n",
      "raw/olist/ingest_date=2026-01-25/olist_customers_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_geolocation_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_order_items_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_order_payments_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_order_reviews_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_orders_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_products_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_sellers_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/product_category_name_translation.csv\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "bucket = \"aai540-olist-mlops-chris-7f3k2p\"\n",
    "prefix = \"raw/olist/ingest_date=2026-01-25/\"\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "resp = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "\n",
    "keys = [obj[\"Key\"] for obj in resp.get(\"Contents\", [])]\n",
    "print(\"Found files:\", len(keys))\n",
    "for k in keys:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9273a5f2-6a04-48e3-91fc-89dd28b3cd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Athena results prefix exists: s3://aai540-olist-mlops-chris-7f3k2p/athena-results/\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "bucket = \"aai540-olist-mlops-chris-7f3k2p\"\n",
    "results_prefix = \"athena-results/\"\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "resp = s3.list_objects_v2(Bucket=bucket, Prefix=results_prefix, MaxKeys=1)\n",
    "\n",
    "if \"Contents\" in resp:\n",
    "    print(\"✅ Athena results prefix exists:\", f\"s3://{bucket}/{results_prefix}\")\n",
    "else:\n",
    "    # create a zero-byte object so the prefix exists\n",
    "    s3.put_object(Bucket=bucket, Key=results_prefix)\n",
    "    print(\"✅ Created Athena results prefix:\", f\"s3://{bucket}/{results_prefix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ca03c11-54ff-441e-bf5e-fd9c05665d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Database ready: olist_datalake\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import boto3\n",
    "\n",
    "bucket = \"aai540-olist-mlops-chris-7f3k2p\"\n",
    "REGION = boto3.session.Session().region_name\n",
    "athena = boto3.client(\"athena\", region_name=REGION)\n",
    "\n",
    "ATHENA_OUTPUT = f\"s3://{bucket}/athena-results/\"\n",
    "DB = \"olist_datalake\"\n",
    "\n",
    "def run_athena(sql: str, database: str = \"default\"):\n",
    "    res = athena.start_query_execution(\n",
    "        QueryString=sql,\n",
    "        QueryExecutionContext={\"Database\": database},\n",
    "        ResultConfiguration={\"OutputLocation\": ATHENA_OUTPUT},\n",
    "    )\n",
    "    qid = res[\"QueryExecutionId\"]\n",
    "    while True:\n",
    "        q = athena.get_query_execution(QueryExecutionId=qid)\n",
    "        state = q[\"QueryExecution\"][\"Status\"][\"State\"]\n",
    "        if state in (\"SUCCEEDED\", \"FAILED\", \"CANCELLED\"):\n",
    "            break\n",
    "        time.sleep(1)\n",
    "    if state != \"SUCCEEDED\":\n",
    "        reason = q[\"QueryExecution\"][\"Status\"].get(\"StateChangeReason\", \"Unknown\")\n",
    "        raise RuntimeError(f\"Athena query failed: {state} - {reason}\\nSQL:\\n{sql}\")\n",
    "    return qid\n",
    "\n",
    "run_athena(f\"CREATE DATABASE IF NOT EXISTS {DB};\", database=\"default\")\n",
    "print(\"✅ Database ready:\", DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e977a0a3-98b4-4b4a-90b6-d6b93a85f392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Copied to: raw/olist/ingest_date=2026-01-25/olist_customers_dataset/olist_customers_dataset.csv\n",
      "✅ Copied to: raw/olist/ingest_date=2026-01-25/olist_geolocation_dataset/olist_geolocation_dataset.csv\n",
      "✅ Copied to: raw/olist/ingest_date=2026-01-25/olist_order_items_dataset/olist_order_items_dataset.csv\n",
      "✅ Copied to: raw/olist/ingest_date=2026-01-25/olist_order_payments_dataset/olist_order_payments_dataset.csv\n",
      "✅ Copied to: raw/olist/ingest_date=2026-01-25/olist_order_reviews_dataset/olist_order_reviews_dataset.csv\n",
      "✅ Copied to: raw/olist/ingest_date=2026-01-25/olist_orders_dataset/olist_orders_dataset.csv\n",
      "✅ Copied to: raw/olist/ingest_date=2026-01-25/olist_products_dataset/olist_products_dataset.csv\n",
      "✅ Copied to: raw/olist/ingest_date=2026-01-25/olist_sellers_dataset/olist_sellers_dataset.csv\n",
      "✅ Copied to: raw/olist/ingest_date=2026-01-25/product_category_name_translation/product_category_name_translation.csv\n",
      "\n",
      "Done. Next we’ll point Athena tables at these folders.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "bucket = \"aai540-olist-mlops-chris-7f3k2p\"\n",
    "base_prefix = \"raw/olist/ingest_date=2026-01-25/\"\n",
    "\n",
    "files = [\n",
    "    \"olist_customers_dataset.csv\",\n",
    "    \"olist_geolocation_dataset.csv\",\n",
    "    \"olist_order_items_dataset.csv\",\n",
    "    \"olist_order_payments_dataset.csv\",\n",
    "    \"olist_order_reviews_dataset.csv\",\n",
    "    \"olist_orders_dataset.csv\",\n",
    "    \"olist_products_dataset.csv\",\n",
    "    \"olist_sellers_dataset.csv\",\n",
    "    \"product_category_name_translation.csv\",\n",
    "]\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "for f in files:\n",
    "    src_key = base_prefix + f\n",
    "    folder = f.replace(\".csv\", \"\")  # folder name = file name without .csv\n",
    "    dst_key = f\"{base_prefix}{folder}/{f}\"\n",
    "    \n",
    "    # copy\n",
    "    s3.copy_object(\n",
    "        Bucket=bucket,\n",
    "        CopySource={\"Bucket\": bucket, \"Key\": src_key},\n",
    "        Key=dst_key\n",
    "    )\n",
    "    print(\"✅ Copied to:\", dst_key)\n",
    "\n",
    "print(\"\\nDone. Next we’ll point Athena tables at these folders.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "858ee701-16e0-4030-bd4c-c966fe47a717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw/olist/ingest_date=2026-01-25/\n",
      "raw/olist/ingest_date=2026-01-25/olist_customers_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_customers_dataset/olist_customers_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_geolocation_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_geolocation_dataset/olist_geolocation_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_order_items_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_order_items_dataset/olist_order_items_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_order_payments_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_order_payments_dataset/olist_order_payments_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_order_reviews_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_order_reviews_dataset/olist_order_reviews_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_orders_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_orders_dataset/olist_orders_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_products_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_products_dataset/olist_products_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_sellers_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_sellers_dataset/olist_sellers_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/product_category_name_translation.csv\n",
      "raw/olist/ingest_date=2026-01-25/product_category_name_translation/product_category_name_translation.csv\n"
     ]
    }
   ],
   "source": [
    "resp = s3.list_objects_v2(Bucket=bucket, Prefix=base_prefix, MaxKeys=50)\n",
    "for obj in resp.get(\"Contents\", []):\n",
    "    print(obj[\"Key\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c29e34fd-478d-486e-ab8e-bf2105b59b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created: olist_datalake.olist_customers_dataset\n",
      "✅ Created: olist_datalake.olist_geolocation_dataset\n",
      "✅ Created: olist_datalake.olist_order_items_dataset\n",
      "✅ Created: olist_datalake.olist_order_payments_dataset\n",
      "✅ Created: olist_datalake.olist_order_reviews_dataset\n",
      "✅ Created: olist_datalake.olist_orders_dataset\n",
      "✅ Created: olist_datalake.olist_products_dataset\n",
      "✅ Created: olist_datalake.olist_sellers_dataset\n",
      "✅ Created: olist_datalake.product_category_name_translation\n"
     ]
    }
   ],
   "source": [
    "RAW_BASE = f\"s3://{bucket}/{base_prefix}\"\n",
    "\n",
    "def create_csv_table(table_name: str, columns_ddl: str, folder_name: str):\n",
    "    sql = f\"\"\"\n",
    "    CREATE EXTERNAL TABLE IF NOT EXISTS {DB}.{table_name} (\n",
    "      {columns_ddl}\n",
    "    )\n",
    "    ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
    "    WITH SERDEPROPERTIES (\n",
    "      'separatorChar' = ',',\n",
    "      'quoteChar'     = '\\\"',\n",
    "      'escapeChar'    = '\\\\\\\\'\n",
    "    )\n",
    "    STORED AS TEXTFILE\n",
    "    LOCATION '{RAW_BASE}{folder_name}/'\n",
    "    TBLPROPERTIES ('skip.header.line.count'='1');\n",
    "    \"\"\"\n",
    "    run_athena(sql, database=DB)\n",
    "    print(f\"✅ Created: {DB}.{table_name}\")\n",
    "\n",
    "create_csv_table(\n",
    "    \"olist_customers_dataset\",\n",
    "    \"\"\"\n",
    "    customer_id string,\n",
    "    customer_unique_id string,\n",
    "    customer_zip_code_prefix int,\n",
    "    customer_city string,\n",
    "    customer_state string\n",
    "    \"\"\",\n",
    "    \"olist_customers_dataset\"\n",
    ")\n",
    "\n",
    "create_csv_table(\n",
    "    \"olist_geolocation_dataset\",\n",
    "    \"\"\"\n",
    "    geolocation_zip_code_prefix int,\n",
    "    geolocation_lat double,\n",
    "    geolocation_lng double,\n",
    "    geolocation_city string,\n",
    "    geolocation_state string\n",
    "    \"\"\",\n",
    "    \"olist_geolocation_dataset\"\n",
    ")\n",
    "\n",
    "create_csv_table(\n",
    "    \"olist_order_items_dataset\",\n",
    "    \"\"\"\n",
    "    order_id string,\n",
    "    order_item_id int,\n",
    "    product_id string,\n",
    "    seller_id string,\n",
    "    shipping_limit_date string,\n",
    "    price double,\n",
    "    freight_value double\n",
    "    \"\"\",\n",
    "    \"olist_order_items_dataset\"\n",
    ")\n",
    "\n",
    "create_csv_table(\n",
    "    \"olist_order_payments_dataset\",\n",
    "    \"\"\"\n",
    "    order_id string,\n",
    "    payment_sequential int,\n",
    "    payment_type string,\n",
    "    payment_installments int,\n",
    "    payment_value double\n",
    "    \"\"\",\n",
    "    \"olist_order_payments_dataset\"\n",
    ")\n",
    "\n",
    "create_csv_table(\n",
    "    \"olist_order_reviews_dataset\",\n",
    "    \"\"\"\n",
    "    review_id string,\n",
    "    order_id string,\n",
    "    review_score int,\n",
    "    review_comment_title string,\n",
    "    review_comment_message string,\n",
    "    review_creation_date string,\n",
    "    review_answer_timestamp string\n",
    "    \"\"\",\n",
    "    \"olist_order_reviews_dataset\"\n",
    ")\n",
    "\n",
    "create_csv_table(\n",
    "    \"olist_orders_dataset\",\n",
    "    \"\"\"\n",
    "    order_id string,\n",
    "    customer_id string,\n",
    "    order_status string,\n",
    "    order_purchase_timestamp string,\n",
    "    order_approved_at string,\n",
    "    order_delivered_carrier_date string,\n",
    "    order_delivered_customer_date string,\n",
    "    order_estimated_delivery_date string\n",
    "    \"\"\",\n",
    "    \"olist_orders_dataset\"\n",
    ")\n",
    "\n",
    "create_csv_table(\n",
    "    \"olist_products_dataset\",\n",
    "    \"\"\"\n",
    "    product_id string,\n",
    "    product_category_name string,\n",
    "    product_name_lenght int,\n",
    "    product_description_lenght int,\n",
    "    product_photos_qty int,\n",
    "    product_weight_g int,\n",
    "    product_length_cm int,\n",
    "    product_height_cm int,\n",
    "    product_width_cm int\n",
    "    \"\"\",\n",
    "    \"olist_products_dataset\"\n",
    ")\n",
    "\n",
    "create_csv_table(\n",
    "    \"olist_sellers_dataset\",\n",
    "    \"\"\"\n",
    "    seller_id string,\n",
    "    seller_zip_code_prefix int,\n",
    "    seller_city string,\n",
    "    seller_state string\n",
    "    \"\"\",\n",
    "    \"olist_sellers_dataset\"\n",
    ")\n",
    "\n",
    "create_csv_table(\n",
    "    \"product_category_name_translation\",\n",
    "    \"\"\"\n",
    "    product_category_name string,\n",
    "    product_category_name_english string\n",
    "    \"\"\",\n",
    "    \"product_category_name_translation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "393bfe94-4366-42c4-b48c-8a556f415750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SHOW TABLES succeeded\n",
      "✅ COUNT orders succeeded\n",
      "✅ GROUP BY order_status succeeded\n"
     ]
    }
   ],
   "source": [
    "run_athena(f\"SHOW TABLES IN {DB};\", database=DB)\n",
    "print(\"✅ SHOW TABLES succeeded\")\n",
    "\n",
    "run_athena(f\"SELECT COUNT(*) FROM {DB}.olist_orders_dataset;\", database=DB)\n",
    "print(\"✅ COUNT orders succeeded\")\n",
    "\n",
    "run_athena(f\"SELECT order_status, COUNT(*) c FROM {DB}.olist_orders_dataset GROUP BY 1 ORDER BY c DESC;\", database=DB)\n",
    "print(\"✅ GROUP BY order_status succeeded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d48e162-12df-4319-ba71-f3cf7f1412a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-25 19:16:46,584\tWARNING services.py:2070 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 1909432320 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=4.64gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2026-01-25 19:16:46,747\tINFO worker.py:1852 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orders: (99441, 8)\n",
      "order_items: (112650, 7)\n",
      "payments: (103886, 5)\n",
      "customers: (99441, 5)\n"
     ]
    }
   ],
   "source": [
    "#6.1\n",
    "import awswrangler as wr\n",
    "import pandas as pd\n",
    "\n",
    "DB = \"olist_datalake\"\n",
    "\n",
    "orders = wr.athena.read_sql_query(\n",
    "    sql=f\"SELECT * FROM {DB}.olist_orders_dataset\",\n",
    "    database=DB,\n",
    "    ctas_approach=False\n",
    ")\n",
    "\n",
    "order_items = wr.athena.read_sql_query(\n",
    "    sql=f\"SELECT * FROM {DB}.olist_order_items_dataset\",\n",
    "    database=DB,\n",
    "    ctas_approach=False\n",
    ")\n",
    "\n",
    "payments = wr.athena.read_sql_query(\n",
    "    sql=f\"SELECT * FROM {DB}.olist_order_payments_dataset\",\n",
    "    database=DB,\n",
    "    ctas_approach=False\n",
    ")\n",
    "\n",
    "customers = wr.athena.read_sql_query(\n",
    "    sql=f\"SELECT * FROM {DB}.olist_customers_dataset\",\n",
    "    database=DB,\n",
    "    ctas_approach=False\n",
    ")\n",
    "\n",
    "print(\"orders:\", orders.shape)\n",
    "print(\"order_items:\", order_items.shape)\n",
    "print(\"payments:\", payments.shape)\n",
    "print(\"customers:\", customers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cfa6170-a17c-4f5c-8cf7-8770b8b46166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orders rows: 99441\n",
      "EDA rows: 99441\n",
      "Row loss: 0\n",
      "Late rate:\n",
      " is_late\n",
      "0    0.92129\n",
      "1    0.07871\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#6.2\n",
    "# Parse timestamps\n",
    "timestamp_cols = [\n",
    "    \"order_purchase_timestamp\",\n",
    "    \"order_approved_at\",\n",
    "    \"order_delivered_carrier_date\",\n",
    "    \"order_delivered_customer_date\",\n",
    "    \"order_estimated_delivery_date\",\n",
    "]\n",
    "for col in timestamp_cols:\n",
    "    orders[col] = pd.to_datetime(orders[col], errors=\"coerce\")\n",
    "\n",
    "# Aggregations\n",
    "items_agg = (\n",
    "    order_items.groupby(\"order_id\")\n",
    "    .agg(\n",
    "        num_items=(\"order_item_id\", \"count\"),\n",
    "        total_price=(\"price\", \"sum\"),\n",
    "        total_freight_value=(\"freight_value\", \"sum\"),\n",
    "        num_sellers=(\"seller_id\", \"nunique\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "payments_agg = (\n",
    "    payments.groupby(\"order_id\")\n",
    "    .agg(\n",
    "        payment_value=(\"payment_value\", \"sum\"),\n",
    "        payment_installments=(\"payment_installments\", \"max\"),\n",
    "        payment_type=(\"payment_type\", lambda x: x.value_counts().index[0]),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "eda_df = (\n",
    "    orders\n",
    "    .merge(items_agg, on=\"order_id\", how=\"left\")\n",
    "    .merge(payments_agg, on=\"order_id\", how=\"left\")\n",
    "    .merge(customers[[\"customer_id\", \"customer_state\"]], on=\"customer_id\", how=\"left\")\n",
    ")\n",
    "\n",
    "# Time features\n",
    "eda_df[\"purchase_dow\"] = eda_df[\"order_purchase_timestamp\"].dt.dayofweek\n",
    "eda_df[\"purchase_hour\"] = eda_df[\"order_purchase_timestamp\"].dt.hour\n",
    "\n",
    "# Label: late delivery\n",
    "eda_df[\"is_late\"] = (\n",
    "    (eda_df[\"order_delivered_customer_date\"].notna()) &\n",
    "    (eda_df[\"order_estimated_delivery_date\"].notna()) &\n",
    "    (eda_df[\"order_delivered_customer_date\"] > eda_df[\"order_estimated_delivery_date\"])\n",
    ").astype(int)\n",
    "\n",
    "print(\"Orders rows:\", len(orders))\n",
    "print(\"EDA rows:\", len(eda_df))\n",
    "print(\"Row loss:\", len(orders) - len(eda_df))\n",
    "print(\"Late rate:\\n\", eda_df[\"is_late\"].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6c44e50-a27a-4052-b192-82f344558f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ feat shape: (99441, 14)\n",
      "✅ feat_fs shape: (99441, 13)\n",
      "event_time sample: ['2017-10-02T10:56:33Z', '2018-07-24T20:41:37Z', '2018-08-08T08:38:49Z', '2017-11-18T19:28:06Z', '2018-02-13T21:18:39Z']\n"
     ]
    }
   ],
   "source": [
    "#7.0 + 7B\n",
    "# Canonical features (with purchase timestamp)\n",
    "feat = eda_df[[\n",
    "    \"order_id\",\n",
    "    \"order_purchase_timestamp\",\n",
    "    \"customer_state\",\n",
    "    \"num_items\",\n",
    "    \"total_price\",\n",
    "    \"total_freight_value\",\n",
    "    \"num_sellers\",\n",
    "    \"payment_value\",\n",
    "    \"payment_installments\",\n",
    "    \"payment_type\",\n",
    "    \"purchase_dow\",\n",
    "    \"purchase_hour\",\n",
    "    \"is_late\"\n",
    "]].copy()\n",
    "\n",
    "feat[\"customer_state\"] = feat[\"customer_state\"].fillna(\"unknown\").astype(str)\n",
    "feat[\"payment_type\"] = feat[\"payment_type\"].fillna(\"unknown\").astype(str)\n",
    "\n",
    "for c in [\"num_items\", \"num_sellers\", \"payment_installments\", \"purchase_dow\", \"purchase_hour\", \"is_late\"]:\n",
    "    feat[c] = feat[c].fillna(0).astype(int)\n",
    "\n",
    "for c in [\"total_price\", \"total_freight_value\", \"payment_value\"]:\n",
    "    feat[c] = feat[c].fillna(0.0).astype(float)\n",
    "\n",
    "# Create strict ISO-8601 event time WITH timezone \"Z\"\n",
    "feat[\"event_time\"] = (\n",
    "    pd.to_datetime(feat[\"order_purchase_timestamp\"], errors=\"coerce\")\n",
    "    .dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    ")\n",
    "\n",
    "feat = feat.dropna(subset=[\"order_id\", \"event_time\"]).reset_index(drop=True)\n",
    "\n",
    "# FeatureStore-safe version (remove datetime64 column)\n",
    "feat_fs = feat.drop(columns=[\"order_purchase_timestamp\"]).copy()\n",
    "feat_fs[\"event_time\"] = feat_fs[\"event_time\"].astype(str)\n",
    "\n",
    "print(\"✅ feat shape:\", feat.shape)\n",
    "print(\"✅ feat_fs shape:\", feat_fs.shape)\n",
    "print(\"event_time sample:\", feat_fs[\"event_time\"].head().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f15a681-f32e-4103-be35-fab9b0646be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "Region: us-east-1\n",
      "Role: arn:aws:iam::758289042916:role/LabRole\n",
      "Offline store URI: s3://aai540-olist-mlops-chris-7f3k2p/feature-store/olist-order-features-v1/\n"
     ]
    }
   ],
   "source": [
    "#7.1\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "\n",
    "bucket = \"aai540-olist-mlops-chris-7f3k2p\"\n",
    "region = boto3.session.Session().region_name\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "feature_group_name = \"olist-order-features-v1\"\n",
    "offline_store_s3_uri = f\"s3://{bucket}/feature-store/{feature_group_name}/\"\n",
    "\n",
    "fg = FeatureGroup(name=feature_group_name, sagemaker_session=sess)\n",
    "\n",
    "print(\"Region:\", region)\n",
    "print(\"Role:\", role)\n",
    "print(\"Offline store URI:\", offline_store_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b652859d-e72d-4a6b-8af1-72af458ed97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Feature Group already exists: olist-order-features-v1\n",
      "Status=Created, OfflineStoreStatus=UNKNOWN\n",
      "✅ Feature Group ready\n"
     ]
    }
   ],
   "source": [
    "#7.2\n",
    "import time\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "sm = boto3.client(\"sagemaker\", region_name=region)\n",
    "\n",
    "def feature_group_exists(name: str) -> bool:\n",
    "    try:\n",
    "        sm.describe_feature_group(FeatureGroupName=name)\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        if \"ResourceNotFound\" in str(e):\n",
    "            return False\n",
    "        raise\n",
    "\n",
    "def wait_for_fg_created(name: str, timeout_sec: int = 600, poll_sec: int = 10):\n",
    "    start = time.time()\n",
    "    while True:\n",
    "        desc = sm.describe_feature_group(FeatureGroupName=name)\n",
    "        status = desc.get(\"FeatureGroupStatus\")\n",
    "        offline_status = desc.get(\"OfflineStoreStatus\", {}).get(\"Status\", \"UNKNOWN\")\n",
    "        print(f\"Status={status}, OfflineStoreStatus={offline_status}\")\n",
    "        if status == \"Created\" and offline_status in (\"Active\", \"UNKNOWN\"):\n",
    "            return desc\n",
    "        if status in (\"CreateFailed\", \"DeleteFailed\"):\n",
    "            raise RuntimeError(f\"Feature Group failed with status={status}. Details: {desc}\")\n",
    "        if time.time() - start > timeout_sec:\n",
    "            raise TimeoutError(f\"Timed out waiting for Feature Group to be Created: {name}\")\n",
    "        time.sleep(poll_sec)\n",
    "\n",
    "if feature_group_exists(feature_group_name):\n",
    "    print(f\"✅ Feature Group already exists: {feature_group_name}\")\n",
    "else:\n",
    "    fg.load_feature_definitions(data_frame=feat_fs)\n",
    "    fg.create(\n",
    "        s3_uri=offline_store_s3_uri,\n",
    "        record_identifier_name=\"order_id\",\n",
    "        event_time_feature_name=\"event_time\",\n",
    "        role_arn=role,\n",
    "        enable_online_store=False,\n",
    "    )\n",
    "    print(\"⏳ Create request submitted\")\n",
    "\n",
    "wait_for_fg_created(feature_group_name)\n",
    "print(\"✅ Feature Group ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2460fd33-5601-4e33-a636-da6a6ce54fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ingest complete\n"
     ]
    }
   ],
   "source": [
    "#7.3\n",
    "ingest_response = fg.ingest(data_frame=feat_fs, max_workers=2, wait=True)\n",
    "print(\"✅ Ingest complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1afae01-1c16-48a2-ab19-d0115ae85d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureGroupStatus: Created\n",
      "OfflineStoreStatus: Active\n",
      "S3 Offline Store Uri: s3://aai540-olist-mlops-chris-7f3k2p/feature-store/olist-order-features-v1/\n"
     ]
    }
   ],
   "source": [
    "#7.4\n",
    "import boto3\n",
    "\n",
    "sm = boto3.client(\"sagemaker\", region_name=region)\n",
    "desc = sm.describe_feature_group(FeatureGroupName=feature_group_name)\n",
    "\n",
    "print(\"FeatureGroupStatus:\", desc[\"FeatureGroupStatus\"])\n",
    "print(\"OfflineStoreStatus:\", desc[\"OfflineStoreStatus\"][\"Status\"])\n",
    "print(\"S3 Offline Store Uri:\", desc[\"OfflineStoreConfig\"][\"S3StorageConfig\"][\"S3Uri\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68adb030-ec45-4e9a-bd5a-abbbce5622ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Split sizes\n",
      "train: 39776\n",
      "val:   9944\n",
      "test:  9944\n",
      "prod:  39777\n",
      "✅ Wrote splits to: s3://aai540-olist-mlops-chris-7f3k2p/splits/olist/features/version=v1/\n"
     ]
    }
   ],
   "source": [
    "#8.0\n",
    "import awswrangler as wr\n",
    "\n",
    "bucket = \"aai540-olist-mlops-chris-7f3k2p\"\n",
    "\n",
    "# Use feat_fs for splits (Feature Store compatible)\n",
    "feat_sorted = feat_fs.sort_values(\"event_time\").reset_index(drop=True)\n",
    "n = len(feat_sorted)\n",
    "\n",
    "train_end = int(n * 0.40)\n",
    "val_end   = int(n * 0.50)\n",
    "test_end  = int(n * 0.60)\n",
    "\n",
    "train_df = feat_sorted.iloc[:train_end]\n",
    "val_df   = feat_sorted.iloc[train_end:val_end]\n",
    "test_df  = feat_sorted.iloc[val_end:test_end]\n",
    "prod_df  = feat_sorted.iloc[test_end:]\n",
    "\n",
    "print(\"✅ Split sizes\")\n",
    "print(\"train:\", len(train_df))\n",
    "print(\"val:  \", len(val_df))\n",
    "print(\"test: \", len(test_df))\n",
    "print(\"prod: \", len(prod_df))\n",
    "\n",
    "split_base = f\"s3://{bucket}/splits/olist/features/version=v1/\"\n",
    "wr.s3.to_parquet(train_df, f\"{split_base}train/\", dataset=True, mode=\"overwrite\")\n",
    "wr.s3.to_parquet(val_df,   f\"{split_base}val/\",   dataset=True, mode=\"overwrite\")\n",
    "wr.s3.to_parquet(test_df,  f\"{split_base}test/\",  dataset=True, mode=\"overwrite\")\n",
    "wr.s3.to_parquet(prod_df,  f\"{split_base}prod/\",  dataset=True, mode=\"overwrite\")\n",
    "\n",
    "print(\"✅ Wrote splits to:\", split_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8ab440e-a4ea-477e-bad5-fc372615b00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5822e971-6f0e-4ddb-acb7-41898a947a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objects found: 4\n",
      "splits/olist/features/version=v1/prod/a8cdd289b48b495ba34f072d8dfa9932.snappy.parquet\n",
      "splits/olist/features/version=v1/test/ba691db52493437ab3c663c8b98fe955.snappy.parquet\n",
      "splits/olist/features/version=v1/train/33879ef6e41a44e384d5255caa4ffa7f.snappy.parquet\n",
      "splits/olist/features/version=v1/val/6fb7a4263ec04acb8f57f8ad0050cfb0.snappy.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1769369955.066884   28608 chttp2_transport.cc:1182] ipv4:169.255.255.2:57431: Got goaway [2] err=UNAVAILABLE:GOAWAY received; Error code: 2; Debug Text: Cancelling all calls {grpc_status:14, http2_error:2, created_time:\"2026-01-25T19:39:15.064991581+00:00\"}\n",
      "*** SIGTERM received at time=1769369957 on cpu 0 ***\n",
      "PC: @     0x7f13accaae9e  (unknown)  epoll_wait\n",
      "    @     0x7f1357624b0d         64  absl::lts_20240722::AbslFailureSignalHandler()\n",
      "    @     0x7f13acbc7520  (unknown)  (unknown)\n",
      "[2026-01-25 19:39:17,560 E 28266 28266] logging.cc:497: *** SIGTERM received at time=1769369957 on cpu 0 ***\n",
      "[2026-01-25 19:39:17,560 E 28266 28266] logging.cc:497: PC: @     0x7f13accaae9e  (unknown)  epoll_wait\n",
      "[2026-01-25 19:39:17,561 E 28266 28266] logging.cc:497:     @     0x7f1357624b39         64  absl::lts_20240722::AbslFailureSignalHandler()\n",
      "[2026-01-25 19:39:17,561 E 28266 28266] logging.cc:497:     @     0x7f13acbc7520  (unknown)  (unknown)\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "prefix = \"splits/olist/features/version=v1/\"\n",
    "\n",
    "resp = s3.list_objects_v2(Bucket=bucket, Prefix=prefix, MaxKeys=50)\n",
    "print(\"Objects found:\", resp.get(\"KeyCount\", 0))\n",
    "for obj in resp.get(\"Contents\", [])[:20]:\n",
    "    print(obj[\"Key\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bab04fb0-989f-4744-bbda-b33cb55b73ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Kernel is fresh — variables not loaded\n"
     ]
    }
   ],
   "source": [
    "#M4 Start\n",
    "try:\n",
    "    print(len(train))\n",
    "except NameError:\n",
    "    print(\"❌ Kernel is fresh — variables not loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0034099-aa6f-4045-a5b2-4375191fdacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import awswrangler as wr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "bucket = \"aai540-olist-mlops-chris-7f3k2p\"\n",
    "split_base = f\"s3://{bucket}/splits/olist/features/version=v1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae9faaf5-2b66-4cc1-b3e1-7d0e8e541b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-31 19:18:39,008\tWARNING services.py:2070 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 1938792448 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=4.61gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2026-01-31 19:18:40,252\tINFO worker.py:1852 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39776, 13) (9944, 13) (9944, 13)\n"
     ]
    }
   ],
   "source": [
    "train = wr.s3.read_parquet(f\"{split_base}train/\")\n",
    "val   = wr.s3.read_parquet(f\"{split_base}val/\")\n",
    "test  = wr.s3.read_parquet(f\"{split_base}test/\")\n",
    "\n",
    "print(train.shape, val.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d4945ad-cf8d-4724-a7a2-9a62e20ad344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8621279163314561, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#M4-1 Benchmark Model Baseline\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Ground truth\n",
    "y_test = test[\"is_late\"].astype(int)\n",
    "\n",
    "# Baseline prediction: always predict NOT late\n",
    "y_pred_baseline = pd.Series(0, index=test.index)\n",
    "\n",
    "baseline_metrics = {\n",
    "    \"accuracy\": accuracy_score(y_test, y_pred_baseline),\n",
    "    \"precision\": precision_score(y_test, y_pred_baseline, zero_division=0),\n",
    "    \"recall\": recall_score(y_test, y_pred_baseline, zero_division=0),\n",
    "    \"f1\": f1_score(y_test, y_pred_baseline, zero_division=0),\n",
    "}\n",
    "\n",
    "baseline_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30e4bb0b-3070-47b1-9036-21f7f01bbbb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_late</th>\n",
       "      <th>num_items</th>\n",
       "      <th>total_price</th>\n",
       "      <th>total_freight_value</th>\n",
       "      <th>num_sellers</th>\n",
       "      <th>payment_value</th>\n",
       "      <th>payment_installments</th>\n",
       "      <th>purchase_dow</th>\n",
       "      <th>purchase_hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>72.89</td>\n",
       "      <td>63.34</td>\n",
       "      <td>1</td>\n",
       "      <td>136.23</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>59.50</td>\n",
       "      <td>15.56</td>\n",
       "      <td>1</td>\n",
       "      <td>75.06</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>40.95</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>134.97</td>\n",
       "      <td>8.49</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100.00</td>\n",
       "      <td>9.34</td>\n",
       "      <td>1</td>\n",
       "      <td>109.34</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_late  num_items  total_price  total_freight_value  num_sellers  \\\n",
       "0        0          2        72.89                63.34            1   \n",
       "1        0          1        59.50                15.56            1   \n",
       "2        0          0         0.00                 0.00            0   \n",
       "3        1          3       134.97                 8.49            1   \n",
       "4        0          1       100.00                 9.34            1   \n",
       "\n",
       "   payment_value  payment_installments  purchase_dow  purchase_hour  \n",
       "0         136.23                     1             6             21  \n",
       "1          75.06                     3             0              0  \n",
       "2          40.95                     2             1             15  \n",
       "3           0.00                     0             3             12  \n",
       "4         109.34                     1             6             22  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#M4-2 1st Model Sage Maker\n",
    "\n",
    "FEATURES = [\n",
    "    \"num_items\",\n",
    "    \"total_price\",\n",
    "    \"total_freight_value\",\n",
    "    \"num_sellers\",\n",
    "    \"payment_value\",\n",
    "    \"payment_installments\",\n",
    "    \"purchase_dow\",\n",
    "    \"purchase_hour\",\n",
    "]\n",
    "\n",
    "def to_xgb_matrix(df):\n",
    "    out = df[[\"is_late\"] + FEATURES].copy()\n",
    "    for c in FEATURES:\n",
    "        out[c] = pd.to_numeric(out[c], errors=\"coerce\").fillna(0.0)\n",
    "    out[\"is_late\"] = out[\"is_late\"].astype(int)\n",
    "    return out\n",
    "\n",
    "train_xgb = to_xgb_matrix(train)\n",
    "val_xgb   = to_xgb_matrix(val)\n",
    "test_xgb  = to_xgb_matrix(test)\n",
    "\n",
    "train_xgb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a07fbf40-aeb6-4aa5-ab61-cdb2068dba2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: s3://aai540-olist-mlops-chris-7f3k2p/modeling/xgb_v1/train/train.csv\n",
      "Val:   s3://aai540-olist-mlops-chris-7f3k2p/modeling/xgb_v1/val/val.csv\n",
      "Test:  s3://aai540-olist-mlops-chris-7f3k2p/modeling/xgb_v1/test/test.csv\n"
     ]
    }
   ],
   "source": [
    "#M4-2-2\n",
    "## NOTE: CSVs already written prior to training — do not rerun\n",
    "xgb_prefix = f\"s3://{bucket}/modeling/xgb_v1/\"\n",
    "\n",
    "train_csv_uri = f\"{xgb_prefix}train/train.csv\"\n",
    "val_csv_uri   = f\"{xgb_prefix}val/val.csv\"\n",
    "test_csv_uri  = f\"{xgb_prefix}test/test.csv\"\n",
    "\n",
    "wr.s3.to_csv(train_xgb, train_csv_uri, index=False, header=False)\n",
    "wr.s3.to_csv(val_xgb,   val_csv_uri,   index=False, header=False)\n",
    "wr.s3.to_csv(test_xgb,  test_csv_uri,  index=False, header=False)\n",
    "\n",
    "print(\"Train:\", train_csv_uri)\n",
    "print(\"Val:  \", val_csv_uri)\n",
    "print(\"Test: \", test_csv_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db400ea4-a70a-457a-9441-2c4bb5b1e071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2026-01-31-18-00-53-460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-31 18:00:53 Starting - Starting the training job...\n",
      "2026-01-31 18:01:14 Starting - Preparing the instances for training...\n",
      "2026-01-31 18:01:38 Downloading - Downloading input data...\n",
      "2026-01-31 18:02:23 Downloading - Downloading the training image........\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m[2026-01-31 18:03:35.611 ip-10-2-90-203.ec2.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2026-01-31 18:03:35.683 ip-10-2-90-203.ec2.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2026-01-31:18:03:36:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2026-01-31:18:03:36:INFO] Failed to parse hyperparameter eval_metric value auc to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2026-01-31:18:03:36:INFO] Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2026-01-31:18:03:36:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-01-31:18:03:36:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34m[2026-01-31:18:03:36:INFO] Determined 0 GPU(s) available on the instance.\u001b[0m\n",
      "\u001b[34m[2026-01-31:18:03:36:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2026-01-31:18:03:36:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2026-01-31:18:03:36:INFO] File path /opt/ml/input/data/train of input files\u001b[0m\n",
      "\u001b[34m[2026-01-31:18:03:36:INFO] Making smlinks from folder /opt/ml/input/data/train to folder /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2026-01-31:18:03:36:INFO] creating symlink between Path /opt/ml/input/data/train/train.csv and destination /tmp/sagemaker_xgboost_input_data/train.csv-4925750691625951217\u001b[0m\n",
      "\u001b[34m[2026-01-31:18:03:36:INFO] files path: /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2026-01-31:18:03:36:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2026-01-31:18:03:36:INFO] File path /opt/ml/input/data/validation of input files\u001b[0m\n",
      "\u001b[34m[2026-01-31:18:03:36:INFO] Making smlinks from folder /opt/ml/input/data/validation to folder /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2026-01-31:18:03:36:INFO] creating symlink between Path /opt/ml/input/data/validation/val.csv and destination /tmp/sagemaker_xgboost_input_data/val.csv-493361961238117506\u001b[0m\n",
      "\u001b[34m[2026-01-31:18:03:36:INFO] files path: /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2026-01-31:18:03:36:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2026-01-31:18:03:36:INFO] Single node training.\u001b[0m\n",
      "\u001b[34m[2026-01-31:18:03:36:INFO] Train matrix has 39776 rows and 8 columns\u001b[0m\n",
      "\u001b[34m[2026-01-31:18:03:36:INFO] Validation matrix has 9944 rows\u001b[0m\n",
      "\u001b[34m[2026-01-31 18:03:36.084 ip-10-2-90-203.ec2.internal:7 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2026-01-31 18:03:36.085 ip-10-2-90-203.ec2.internal:7 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2026-01-31 18:03:36.085 ip-10-2-90-203.ec2.internal:7 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2026-01-31 18:03:36.086 ip-10-2-90-203.ec2.internal:7 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2026-01-31:18:03:36:INFO] Debug hook created from config\u001b[0m\n",
      "\u001b[34m[0]#011train-auc:0.57718#011validation-auc:0.60987\u001b[0m\n",
      "\u001b[34m[2026-01-31 18:03:36.133 ip-10-2-90-203.ec2.internal:7 INFO hook.py:427] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[34m[2026-01-31 18:03:36.138 ip-10-2-90-203.ec2.internal:7 INFO hook.py:491] Hook is writing from the hook with pid: 7\u001b[0m\n",
      "\u001b[34m[1]#011train-auc:0.58678#011validation-auc:0.61750\u001b[0m\n",
      "\u001b[34m[2]#011train-auc:0.58967#011validation-auc:0.61922\u001b[0m\n",
      "\u001b[34m[3]#011train-auc:0.59464#011validation-auc:0.60855\u001b[0m\n",
      "\u001b[34m[4]#011train-auc:0.60033#011validation-auc:0.61503\u001b[0m\n",
      "\u001b[34m[5]#011train-auc:0.60274#011validation-auc:0.61439\u001b[0m\n",
      "\u001b[34m[6]#011train-auc:0.60120#011validation-auc:0.61226\u001b[0m\n",
      "\u001b[34m[7]#011train-auc:0.60385#011validation-auc:0.61266\u001b[0m\n",
      "\u001b[34m[8]#011train-auc:0.60486#011validation-auc:0.61274\u001b[0m\n",
      "\u001b[34m[9]#011train-auc:0.60711#011validation-auc:0.61181\u001b[0m\n",
      "\u001b[34m[10]#011train-auc:0.60697#011validation-auc:0.61046\u001b[0m\n",
      "\u001b[34m[11]#011train-auc:0.60916#011validation-auc:0.60854\u001b[0m\n",
      "\u001b[34m[12]#011train-auc:0.61248#011validation-auc:0.60882\u001b[0m\n",
      "\u001b[34m[13]#011train-auc:0.61380#011validation-auc:0.60906\u001b[0m\n",
      "\u001b[34m[14]#011train-auc:0.61496#011validation-auc:0.60803\u001b[0m\n",
      "\u001b[34m[15]#011train-auc:0.61577#011validation-auc:0.60883\u001b[0m\n",
      "\u001b[34m[16]#011train-auc:0.61938#011validation-auc:0.60737\u001b[0m\n",
      "\u001b[34m[17]#011train-auc:0.62391#011validation-auc:0.60716\u001b[0m\n",
      "\u001b[34m[18]#011train-auc:0.62429#011validation-auc:0.60665\u001b[0m\n",
      "\u001b[34m[19]#011train-auc:0.62490#011validation-auc:0.60728\u001b[0m\n",
      "\u001b[34m[20]#011train-auc:0.62650#011validation-auc:0.60556\u001b[0m\n",
      "\u001b[34m[21]#011train-auc:0.62888#011validation-auc:0.60332\u001b[0m\n",
      "\u001b[34m[22]#011train-auc:0.63095#011validation-auc:0.60233\u001b[0m\n",
      "\u001b[34m[23]#011train-auc:0.63570#011validation-auc:0.59728\u001b[0m\n",
      "\u001b[34m[24]#011train-auc:0.63690#011validation-auc:0.59524\u001b[0m\n",
      "\u001b[34m[25]#011train-auc:0.63829#011validation-auc:0.59603\u001b[0m\n",
      "\u001b[34m[26]#011train-auc:0.64073#011validation-auc:0.59587\u001b[0m\n",
      "\u001b[34m[27]#011train-auc:0.64565#011validation-auc:0.59651\u001b[0m\n",
      "\u001b[34m[28]#011train-auc:0.64638#011validation-auc:0.59769\u001b[0m\n",
      "\u001b[34m[29]#011train-auc:0.64903#011validation-auc:0.59423\u001b[0m\n",
      "\u001b[34m[30]#011train-auc:0.65247#011validation-auc:0.59341\u001b[0m\n",
      "\u001b[34m[31]#011train-auc:0.65328#011validation-auc:0.59380\u001b[0m\n",
      "\u001b[34m[32]#011train-auc:0.65542#011validation-auc:0.59545\u001b[0m\n",
      "\u001b[34m[33]#011train-auc:0.65942#011validation-auc:0.59481\u001b[0m\n",
      "\u001b[34m[34]#011train-auc:0.66074#011validation-auc:0.59615\u001b[0m\n",
      "\u001b[34m[35]#011train-auc:0.66255#011validation-auc:0.59685\u001b[0m\n",
      "\u001b[34m[36]#011train-auc:0.66308#011validation-auc:0.59537\u001b[0m\n",
      "\u001b[34m[37]#011train-auc:0.66411#011validation-auc:0.59450\u001b[0m\n",
      "\u001b[34m[38]#011train-auc:0.66587#011validation-auc:0.59220\u001b[0m\n",
      "\u001b[34m[39]#011train-auc:0.66659#011validation-auc:0.59020\u001b[0m\n",
      "\u001b[34m[40]#011train-auc:0.67067#011validation-auc:0.58837\u001b[0m\n",
      "\u001b[34m[41]#011train-auc:0.67208#011validation-auc:0.59180\u001b[0m\n",
      "\u001b[34m[42]#011train-auc:0.67470#011validation-auc:0.59018\u001b[0m\n",
      "\u001b[34m[43]#011train-auc:0.67599#011validation-auc:0.58973\u001b[0m\n",
      "\u001b[34m[44]#011train-auc:0.67859#011validation-auc:0.59031\u001b[0m\n",
      "\u001b[34m[45]#011train-auc:0.68025#011validation-auc:0.59029\u001b[0m\n",
      "\u001b[34m[46]#011train-auc:0.68055#011validation-auc:0.58914\u001b[0m\n",
      "\u001b[34m[47]#011train-auc:0.68338#011validation-auc:0.58682\u001b[0m\n",
      "\u001b[34m[48]#011train-auc:0.68535#011validation-auc:0.58531\u001b[0m\n",
      "\u001b[34m[49]#011train-auc:0.68663#011validation-auc:0.58506\u001b[0m\n",
      "\u001b[34m[50]#011train-auc:0.68960#011validation-auc:0.58067\u001b[0m\n",
      "\u001b[34m[51]#011train-auc:0.69082#011validation-auc:0.57944\u001b[0m\n",
      "\u001b[34m[52]#011train-auc:0.69350#011validation-auc:0.57876\u001b[0m\n",
      "\u001b[34m[53]#011train-auc:0.69431#011validation-auc:0.57790\u001b[0m\n",
      "\u001b[34m[54]#011train-auc:0.69508#011validation-auc:0.57814\u001b[0m\n",
      "\u001b[34m[55]#011train-auc:0.69788#011validation-auc:0.57586\u001b[0m\n",
      "\u001b[34m[56]#011train-auc:0.69958#011validation-auc:0.57689\u001b[0m\n",
      "\u001b[34m[57]#011train-auc:0.69967#011validation-auc:0.57617\u001b[0m\n",
      "\u001b[34m[58]#011train-auc:0.70106#011validation-auc:0.57703\u001b[0m\n",
      "\u001b[34m[59]#011train-auc:0.70124#011validation-auc:0.57684\u001b[0m\n",
      "\u001b[34m[60]#011train-auc:0.70351#011validation-auc:0.57662\u001b[0m\n",
      "\u001b[34m[61]#011train-auc:0.70420#011validation-auc:0.57699\u001b[0m\n",
      "\u001b[34m[62]#011train-auc:0.70539#011validation-auc:0.57893\u001b[0m\n",
      "\u001b[34m[63]#011train-auc:0.70704#011validation-auc:0.57891\u001b[0m\n",
      "\u001b[34m[64]#011train-auc:0.70791#011validation-auc:0.57796\u001b[0m\n",
      "\u001b[34m[65]#011train-auc:0.71070#011validation-auc:0.57732\u001b[0m\n",
      "\u001b[34m[66]#011train-auc:0.71187#011validation-auc:0.57656\u001b[0m\n",
      "\u001b[34m[67]#011train-auc:0.71214#011validation-auc:0.57604\u001b[0m\n",
      "\u001b[34m[68]#011train-auc:0.71341#011validation-auc:0.57686\u001b[0m\n",
      "\u001b[34m[69]#011train-auc:0.71415#011validation-auc:0.57558\u001b[0m\n",
      "\u001b[34m[70]#011train-auc:0.71535#011validation-auc:0.57584\u001b[0m\n",
      "\u001b[34m[71]#011train-auc:0.71621#011validation-auc:0.57537\u001b[0m\n",
      "\u001b[34m[72]#011train-auc:0.71661#011validation-auc:0.57624\u001b[0m\n",
      "\u001b[34m[73]#011train-auc:0.71805#011validation-auc:0.57630\u001b[0m\n",
      "\u001b[34m[74]#011train-auc:0.71962#011validation-auc:0.57667\u001b[0m\n",
      "\u001b[34m[75]#011train-auc:0.72031#011validation-auc:0.57737\u001b[0m\n",
      "\u001b[34m[76]#011train-auc:0.72202#011validation-auc:0.57646\u001b[0m\n",
      "\u001b[34m[77]#011train-auc:0.72265#011validation-auc:0.57596\u001b[0m\n",
      "\u001b[34m[78]#011train-auc:0.72382#011validation-auc:0.57490\u001b[0m\n",
      "\u001b[34m[79]#011train-auc:0.72416#011validation-auc:0.57438\u001b[0m\n",
      "\u001b[34m[80]#011train-auc:0.72454#011validation-auc:0.57428\u001b[0m\n",
      "\u001b[34m[81]#011train-auc:0.72604#011validation-auc:0.57419\u001b[0m\n",
      "\u001b[34m[82]#011train-auc:0.72744#011validation-auc:0.57596\u001b[0m\n",
      "\u001b[34m[83]#011train-auc:0.72923#011validation-auc:0.57486\u001b[0m\n",
      "\u001b[34m[84]#011train-auc:0.73094#011validation-auc:0.57390\u001b[0m\n",
      "\u001b[34m[85]#011train-auc:0.73201#011validation-auc:0.57408\u001b[0m\n",
      "\u001b[34m[86]#011train-auc:0.73278#011validation-auc:0.57454\u001b[0m\n",
      "\u001b[34m[87]#011train-auc:0.73306#011validation-auc:0.57505\u001b[0m\n",
      "\u001b[34m[88]#011train-auc:0.73337#011validation-auc:0.57517\u001b[0m\n",
      "\u001b[34m[89]#011train-auc:0.73389#011validation-auc:0.57457\u001b[0m\n",
      "\u001b[34m[90]#011train-auc:0.73583#011validation-auc:0.57328\u001b[0m\n",
      "\u001b[34m[91]#011train-auc:0.73729#011validation-auc:0.57133\u001b[0m\n",
      "\u001b[34m[92]#011train-auc:0.73771#011validation-auc:0.57227\u001b[0m\n",
      "\u001b[34m[93]#011train-auc:0.73876#011validation-auc:0.57303\u001b[0m\n",
      "\u001b[34m[94]#011train-auc:0.74023#011validation-auc:0.57281\u001b[0m\n",
      "\u001b[34m[95]#011train-auc:0.74183#011validation-auc:0.57206\u001b[0m\n",
      "\u001b[34m[96]#011train-auc:0.74366#011validation-auc:0.57112\u001b[0m\n",
      "\u001b[34m[97]#011train-auc:0.74468#011validation-auc:0.57151\u001b[0m\n",
      "\u001b[34m[98]#011train-auc:0.74505#011validation-auc:0.57078\u001b[0m\n",
      "\u001b[34m[99]#011train-auc:0.74611#011validation-auc:0.57011\u001b[0m\n",
      "\n",
      "2026-01-31 18:03:42 Training - Training image download completed. Training in progress.\n",
      "2026-01-31 18:03:42 Uploading - Uploading generated training model\n",
      "2026-01-31 18:04:00 Completed - Training job completed\n",
      "Training seconds: 143\n",
      "Billable seconds: 143\n"
     ]
    }
   ],
   "source": [
    "#M4-2-3 Train Model\n",
    "# TRAINING CELL (DO NOT RERUN)\n",
    "# This cell was executed once to train the initial XGBoost model.\n",
    "# Re-running this cell will retrain the model and incur additional cost.\n",
    "# The trained model is reused below via attachment for evaluation and deployment.\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "# Built-in XGBoost container\n",
    "xgb_image = retrieve(\n",
    "    framework=\"xgboost\",\n",
    "    region=region,\n",
    "    version=\"1.7-1\"\n",
    ")\n",
    "\n",
    "output_path = f\"s3://{bucket}/modeling/xgb_v1/output/\"\n",
    "\n",
    "xgb = Estimator(\n",
    "    image_uri=xgb_image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",  # budget-friendly\n",
    "    output_path=output_path,\n",
    "    sagemaker_session=sess,\n",
    ")\n",
    "\n",
    "# Simple, reasonable first-pass hyperparameters\n",
    "xgb.set_hyperparameters(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\",\n",
    "    num_round=100,\n",
    "    max_depth=4,\n",
    "    eta=0.2,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    ")\n",
    "\n",
    "train_input = TrainingInput(train_csv_uri, content_type=\"text/csv\")\n",
    "val_input   = TrainingInput(val_csv_uri, content_type=\"text/csv\")\n",
    "\n",
    "#xgb.fit({\n",
    "#    \"train\": train_input,\n",
    "#    \"validation\": val_input\n",
    "#})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0151caee-82d9-4259-819d-bf0696a22481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "✅ Using training job: sagemaker-xgboost-2026-01-31-18-00-53-460\n",
      "✅ Attached. Ready for Batch Transform.\n"
     ]
    }
   ],
   "source": [
    "# M4-2-3b Attach to Existing Trained Model (No Retraining)\n",
    "import boto3, sagemaker\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.session.Session().region_name\n",
    "sm = boto3.client(\"sagemaker\", region_name=region)\n",
    "\n",
    "# Pick the most recent completed training job\n",
    "jobs = sm.list_training_jobs(SortBy=\"CreationTime\", SortOrder=\"Descending\", MaxResults=20)[\"TrainingJobSummaries\"]\n",
    "training_job_name = next(j[\"TrainingJobName\"] for j in jobs if j[\"TrainingJobStatus\"] == \"Completed\")\n",
    "print(\"✅ Using training job:\", training_job_name)\n",
    "\n",
    "# Recreate estimator and attach (no retraining)\n",
    "xgb_image = retrieve(framework=\"xgboost\", region=region, version=\"1.7-1\")\n",
    "xgb = Estimator(\n",
    "    image_uri=xgb_image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    output_path=f\"s3://{bucket}/modeling/xgb_v1/output/\",\n",
    "    sagemaker_session=sess,\n",
    ")\n",
    "xgb._current_job_name = training_job_name\n",
    "print(\"✅ Attached. Ready for Batch Transform.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "574dce64-f186-495e-a51a-7c63553268cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: s3://aai540-olist-mlops-chris-7f3k2p/modeling/xgb_v1/train/train.csv\n",
      "Val:   s3://aai540-olist-mlops-chris-7f3k2p/modeling/xgb_v1/val/val.csv\n",
      "Test:  s3://aai540-olist-mlops-chris-7f3k2p/modeling/xgb_v1/test/test.csv\n"
     ]
    }
   ],
   "source": [
    "# M4-2-2b Recreate CSV URIs\n",
    "\n",
    "xgb_prefix = f\"s3://{bucket}/modeling/xgb_v1/\"\n",
    "\n",
    "train_csv_uri = f\"{xgb_prefix}train/train.csv\"\n",
    "val_csv_uri   = f\"{xgb_prefix}val/val.csv\"\n",
    "test_csv_uri  = f\"{xgb_prefix}test/test.csv\"\n",
    "\n",
    "print(\"Train:\", train_csv_uri)\n",
    "print(\"Val:  \", val_csv_uri)\n",
    "print(\"Test: \", test_csv_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7bf01c7-3661-40d1-8cb6-9298677f125d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote inference CSV: s3://aai540-olist-mlops-chris-7f3k2p/modeling/xgb_v1/test/test_infer.csv\n",
      "Shape (should be 9944 x 8): (9944, 8)\n"
     ]
    }
   ],
   "source": [
    "# M4-3.0 Create inference-only TEST input (features only)\n",
    "# Code developed using ChatGPT (ChatGPT, 2024) as a paired programmer.\n",
    "\n",
    "# test_xgb currently has: is_late + 8 features\n",
    "test_infer = test_xgb.drop(columns=[\"is_late\"]).copy()\n",
    "\n",
    "test_infer_csv_uri = f\"{xgb_prefix}test/test_infer.csv\"\n",
    "\n",
    "# IMPORTANT: no header, no index\n",
    "wr.s3.to_csv(test_infer, test_infer_csv_uri, index=False, header=False)\n",
    "\n",
    "print(\"✅ Wrote inference CSV:\", test_infer_csv_uri)\n",
    "print(\"Shape (should be 9944 x 8):\", test_infer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93fd6908-b378-4c81-924d-062f65c4215a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating transform job with name: sagemaker-xgboost-2026-01-31-19-39-44-779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Starting batch transform (features-only input)...\n",
      ".................................\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:17:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:17:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:17:INFO] nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2026-01-31 19:45:17 +0000] [13] [INFO] Starting gunicorn 23.0.0\u001b[0m\n",
      "\u001b[34m[2026-01-31 19:45:17 +0000] [13] [INFO] Listening at: unix:/tmp/gunicorn.sock (13)\u001b[0m\n",
      "\u001b[34m[2026-01-31 19:45:17 +0000] [13] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2026-01-31 19:45:17 +0000] [16] [INFO] Booting worker with pid: 16\u001b[0m\n",
      "\u001b[34m[2026-01-31 19:45:17 +0000] [17] [INFO] Booting worker with pid: 17\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:19:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:19:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:19:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:19:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:19:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:19:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:26:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [31/Jan/2026:19:45:26 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:26:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [31/Jan/2026:19:45:26 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:26:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [31/Jan/2026:19:45:26 +0000] \"POST /invocations HTTP/1.1\" 200 200088 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2026-01-31:19:45:26:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [31/Jan/2026:19:45:26 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2026-01-31:19:45:26:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [31/Jan/2026:19:45:26 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2026-01-31:19:45:26:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [31/Jan/2026:19:45:26 +0000] \"POST /invocations HTTP/1.1\" 200 200088 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2026-01-31T19:45:26.181:[sagemaker logs]: MaxConcurrentTransforms=2, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:17:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:17:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:17:INFO] nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2026-01-31 19:45:17 +0000] [13] [INFO] Starting gunicorn 23.0.0\u001b[0m\n",
      "\u001b[34m[2026-01-31 19:45:17 +0000] [13] [INFO] Listening at: unix:/tmp/gunicorn.sock (13)\u001b[0m\n",
      "\u001b[34m[2026-01-31 19:45:17 +0000] [13] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2026-01-31 19:45:17 +0000] [16] [INFO] Booting worker with pid: 16\u001b[0m\n",
      "\u001b[35m[2026-01-31:19:45:17:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2026-01-31:19:45:17:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2026-01-31:19:45:17:INFO] nginx config: \u001b[0m\n",
      "\u001b[35mworker_processes auto;\u001b[0m\n",
      "\u001b[35mdaemon off;\u001b[0m\n",
      "\u001b[35mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[35merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[35mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[35mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35m[2026-01-31 19:45:17 +0000] [13] [INFO] Starting gunicorn 23.0.0\u001b[0m\n",
      "\u001b[35m[2026-01-31 19:45:17 +0000] [13] [INFO] Listening at: unix:/tmp/gunicorn.sock (13)\u001b[0m\n",
      "\u001b[35m[2026-01-31 19:45:17 +0000] [13] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[35m[2026-01-31 19:45:17 +0000] [16] [INFO] Booting worker with pid: 16\u001b[0m\n",
      "\u001b[34m[2026-01-31 19:45:17 +0000] [17] [INFO] Booting worker with pid: 17\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:19:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:19:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:19:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:19:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:19:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:19:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[35m[2026-01-31 19:45:17 +0000] [17] [INFO] Booting worker with pid: 17\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[35m[2026-01-31:19:45:19:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2026-01-31:19:45:19:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[35m[2026-01-31:19:45:19:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[35m[2026-01-31:19:45:19:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2026-01-31:19:45:19:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[35m[2026-01-31:19:45:19:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:26:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [31/Jan/2026:19:45:26 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:26:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [31/Jan/2026:19:45:26 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:26:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [31/Jan/2026:19:45:26 +0000] \"POST /invocations HTTP/1.1\" 200 200088 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2026-01-31:19:45:26:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [31/Jan/2026:19:45:26 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2026-01-31:19:45:26:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [31/Jan/2026:19:45:26 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2026-01-31:19:45:26:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [31/Jan/2026:19:45:26 +0000] \"POST /invocations HTTP/1.1\" 200 200088 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2026-01-31T19:45:26.181:[sagemaker logs]: MaxConcurrentTransforms=2, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "✅ Batch transform finished: s3://aai540-olist-mlops-chris-7f3k2p/modeling/xgb_v1/batch-out/test_v2/\n"
     ]
    }
   ],
   "source": [
    "# M4-3.1 Batch Transform on TEST (Evaluation) - v2 (features-only)\n",
    "from sagemaker.transformer import Transformer\n",
    "\n",
    "test_transform_output_v2 = f\"s3://{bucket}/modeling/xgb_v1/batch-out/test_v2/\"\n",
    "\n",
    "transformer = Transformer(\n",
    "    model_name=model_name,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    output_path=test_transform_output_v2,\n",
    "    assemble_with=\"Line\",\n",
    "    accept=\"text/csv\"\n",
    ")\n",
    "\n",
    "print(\"⏳ Starting batch transform (features-only input)...\")\n",
    "transformer.transform(\n",
    "    data=test_infer_csv_uri,\n",
    "    content_type=\"text/csv\",\n",
    "    split_type=\"Line\"\n",
    ")\n",
    "\n",
    "transformer.wait()\n",
    "print(\"✅ Batch transform finished:\", test_transform_output_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c23b185-e888-4cb8-a1bf-1c2d89f33e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading predictions from: s3://aai540-olist-mlops-chris-7f3k2p/modeling/xgb_v1/batch-out/test_v2/test_infer.csv.out\n",
      "✅ Model metrics: {'auc': 0.5635208855035949, 'accuracy': 0.8621279163314561, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline_always_on_time</th>\n",
       "      <td>0.862128</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb_v1</th>\n",
       "      <td>0.862128</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.563521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         accuracy  precision  recall   f1       auc\n",
       "baseline_always_on_time  0.862128        0.0     0.0  0.0       NaN\n",
       "xgb_v1                   0.862128        0.0     0.0  0.0  0.563521"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# M4-3.2 Load predictions and evaluate\n",
    "import boto3\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "resp = s3.list_objects_v2(Bucket=bucket, Prefix=\"modeling/xgb_v1/batch-out/test_v2/\")\n",
    "keys = [o[\"Key\"] for o in resp.get(\"Contents\", [])]\n",
    "out_files = [k for k in keys if k.endswith(\".out\")]\n",
    "\n",
    "if not out_files:\n",
    "    raise RuntimeError(f\"No .out files found in test_v2 output. Keys seen: {keys[:10]}\")\n",
    "\n",
    "out_key = sorted(out_files)[-1]\n",
    "out_uri = f\"s3://{bucket}/{out_key}\"\n",
    "print(\"Reading predictions from:\", out_uri)\n",
    "\n",
    "pred_df = wr.s3.read_csv(out_uri, header=None)\n",
    "y_prob = pred_df[0].astype(float).reset_index(drop=True)\n",
    "\n",
    "y_true = test_xgb[\"is_late\"].astype(int).reset_index(drop=True)\n",
    "y_hat = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "model_metrics = {\n",
    "    \"auc\": roc_auc_score(y_true, y_prob),\n",
    "    \"accuracy\": accuracy_score(y_true, y_hat),\n",
    "    \"precision\": precision_score(y_true, y_hat, zero_division=0),\n",
    "    \"recall\": recall_score(y_true, y_hat, zero_division=0),\n",
    "    \"f1\": f1_score(y_true, y_hat, zero_division=0),\n",
    "}\n",
    "\n",
    "comparison = pd.DataFrame(\n",
    "    [baseline_metrics, model_metrics],\n",
    "    index=[\"baseline_always_on_time\", \"xgb_v1\"]\n",
    ")\n",
    "\n",
    "print(\"✅ Model metrics:\", model_metrics)\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "907c66f6-bf25-49d8-ac61-ca93e7100c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Deleted model: xgb-v1-1769887733\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "sm = boto3.client(\"sagemaker\", region_name=region)\n",
    "sm.delete_model(ModelName=model_name)\n",
    "print(\"✅ Deleted model:\", model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80622011-15db-4d64-8218-fcea39c9552e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
