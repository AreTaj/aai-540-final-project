{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69584688-1464-4b46-be61-06a594249ce9",
   "metadata": {},
   "source": [
    "# MLOps Pipeline Workflow & Team Notes\n",
    "\n",
    "## Project Overview\n",
    "This notebook implements an end-to-end MLOps data pipeline using the **Olist Brazilian E-Commerce dataset**. The goal was to demonstrate a production-style workflow that covers data ingestion, cataloging, exploratory analysis, feature engineering, feature storage, and dataset splitting — all using AWS services in a cost-efficient way.\n",
    "\n",
    "The pipeline was intentionally built step-by-step to mirror MLOps practices rather than a one-off modeling notebook.\n",
    "\n",
    "---\n",
    "\n",
    "## High-Level Workflow\n",
    "\n",
    "### 1. Raw Data Ingestion (S3 Data Lake)\n",
    "- Created an S3 bucket to act as a data lake.\n",
    "- Uploaded all **9 raw CSV files** from the Kaggle Olist dataset.\n",
    "- Organized raw data under: s3:///raw/olist/ingest_date=YYYY-MM-DD/\n",
    "- Each dataset was placed into its own subfolder to support Athena’s directory-based table requirements.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Data Cataloging & Querying (Athena)\n",
    "- Created an Athena database (`olist_datalake`).\n",
    "- Defined **external tables** for each dataset directly from JupyterLab (no Glue crawler required).\n",
    "- Verified schemas and row counts using Athena queries.\n",
    "- This step enabled SQL-based access to the data and served as the cataloging layer for downstream analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Exploratory Data Analysis (SageMaker + Pandas)\n",
    "- Loaded Athena tables into Pandas using `awswrangler`.\n",
    "- Performed sanity checks on row counts and joins.\n",
    "- Built an **order-level analytical view** by aggregating:\n",
    "- order items\n",
    "- payments\n",
    "- customer attributes\n",
    "- Engineered a target variable (`is_late`) based on delivery vs. estimated delivery dates.\n",
    "- Observed class imbalance (~8% late deliveries), motivating careful splitting and evaluation later.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Feature Engineering\n",
    "- Created leakage-safe, order-level features using only information available at purchase time:\n",
    "- pricing, freight, number of items/sellers\n",
    "- payment information\n",
    "- time-based features (day of week, hour of day)\n",
    "- customer state\n",
    "- Maintained a **canonical feature dataset** for analysis and splitting.\n",
    "- Created a **Feature Store–compatible version** with strict data types.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. SageMaker Feature Store (Offline Store)\n",
    "- Created a **SageMaker Feature Group** (offline store only to control cost).\n",
    "- Used `order_id` as the record identifier.\n",
    "- Used a strictly formatted ISO-8601 `event_time` with UTC (`Z`) as the event time feature.\n",
    "- Successfully ingested ~99k feature records into Feature Store.\n",
    "- Offline store data is persisted in S3 for training and future reuse.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Dataset Splitting (Time-Based)\n",
    "- Performed a **time-based split** using `event_time` to avoid temporal leakage:\n",
    "- Train: ~40%\n",
    "- Validation: ~10%\n",
    "- Test: ~10%\n",
    "- Production reserve: ~40%\n",
    "- Persisted each split as Parquet files to: s3:///splits/olist/features/version=v1/\n",
    "- This mirrors a real production setup where recent data is reserved for inference.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Engineering Decisions & Lessons Learned\n",
    "\n",
    "- **Athena LOCATION must point to directories**, not individual files.\n",
    "- **Feature Store requires strict ISO-8601 timestamps with timezone** — missing the `Z` suffix causes ingestion failures.\n",
    "- Maintaining separate:\n",
    "- canonical feature data (analysis-friendly)\n",
    "- Feature Store–safe data (schema-restricted)\n",
    "is a best practice in real MLOps systems.\n",
    "- Time-based splitting is critical to avoid data leakage in temporal datasets.\n",
    "- Offline Feature Store provides the required functionality while minimizing cost.\n",
    "\n",
    "---\n",
    "\n",
    "## Cost Management Notes\n",
    "- SageMaker compute was stopped immediately after completion.\n",
    "- Feature Store **online store was intentionally disabled** to avoid ongoing charges.\n",
    "- S3 storage costs are minimal and safe to keep until final submission.\n",
    "- Cleanup (Feature Group deletion, S3 cleanup) should only be done **after submission**.\n",
    "\n",
    "---\n",
    "\n",
    "## For Teammates\n",
    "If you need to re-run or extend this work:\n",
    "1. Start at the Athena read step (no need to re-upload raw data).\n",
    "2. Do **not** re-run ingestion unless changing the feature schema.\n",
    "3. Always stop SageMaker compute when finished.\n",
    "\n",
    "This notebook represents a complete, MLOps data pipeline.\n",
    "\n",
    "## Model Benchmark and Evaluation (Module 4)\n",
    "\n",
    "### Baseline Model\n",
    "As a benchmark, we implemented a simple heuristic model that always predicts an order will be delivered on time. This reflects the majority class in the dataset and establishes a lower bound for model performance.\n",
    "\n",
    "Due to class imbalance (~86% of orders are not late), the baseline achieves high accuracy but fails to identify late deliveries, resulting in zero precision, recall, and F1-score.\n",
    "\n",
    "### First Iteration Model (XGBoost v1)\n",
    "We trained a first-pass XGBoost binary classifier in Amazon SageMaker using a limited set of engineered features related to order size, payment behavior, and purchase timing.\n",
    "\n",
    "The model was evaluated using SageMaker Batch Transform on the held-out test dataset. Batch Transform was selected over a real-time endpoint to minimize cost and ensure automatic resource cleanup.\n",
    "\n",
    "### Results Summary\n",
    "- The XGBoost model achieved an AUC of approximately **0.56**, indicating it learned some discriminative signal beyond random chance.\n",
    "- Overall accuracy matched the baseline model due to class imbalance and use of a default classification threshold.\n",
    "- Precision, recall, and F1-score remained low, highlighting the need for future improvements such as class weighting, threshold tuning, and feature expansion.\n",
    "\n",
    "### Key Takeaways\n",
    "This iteration establishes a complete, cost-aware MLOps workflow including data ingestion, feature engineering, model training, evaluation, and deployment via batch inference. While performance improvements are needed, this version serves as a strong baseline for future model iterations and CI/CD integration in later modules.\n",
    "\n",
    "## Monitoring, Evaluation, and Reporting Summary (Module 5)\n",
    "\n",
    "### Model Benchmarking and Evaluation\n",
    "We established a simple benchmark model that always predicts orders as on-time. While this baseline achieved high accuracy due to class imbalance, it had zero recall for late deliveries. We then trained an XGBoost classifier using a small, carefully selected feature set. Model performance was evaluated using batch inference, and metrics such as accuracy and AUC were compared against the benchmark to establish a minimum viable improvement baseline.\n",
    "\n",
    "### Data Monitoring\n",
    "Data monitoring was implemented using SageMaker Batch Transform with batch data capture enabled. Production inference inputs and corresponding model outputs were automatically captured to S3. Baseline statistics and constraints were generated using SageMaker Model Monitor and stored in S3. These artifacts enable offline drift detection and future scheduled monitoring without requiring a persistent real-time endpoint, aligning with cost-efficient MLOps best practices.\n",
    "\n",
    "### Infrastructure Monitoring\n",
    "Infrastructure-level monitoring was implemented using Amazon CloudWatch. Metrics for SageMaker training jobs, batch transform jobs, and processing jobs were collected automatically. A centralized CloudWatch dashboard was created to visualize job durations and failure counts, providing operational visibility into the ML system.\n",
    "\n",
    "### Reports and Artifacts\n",
    "The system produces the following monitoring artifacts:\n",
    "- Model training metrics in CloudWatch Logs\n",
    "- Baseline statistics and constraints in S3\n",
    "- Captured production input and output data in S3\n",
    "- CloudWatch dashboard for infrastructure monitoring\n",
    "\n",
    "This architecture supports scalable monitoring, auditability, and future CI/CD integration while remaining within budget constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a684ab4-5185-4603-9f30-358b125dcba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found files: 10\n",
      "raw/olist/ingest_date=2026-01-25/\n",
      "raw/olist/ingest_date=2026-01-25/olist_customers_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_geolocation_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_order_items_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_order_payments_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_order_reviews_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_orders_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_products_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_sellers_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/product_category_name_translation.csv\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "bucket = \"aai540-olist-mlops-chris-7f3k2p\"\n",
    "prefix = \"raw/olist/ingest_date=2026-01-25/\"\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "resp = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "\n",
    "keys = [obj[\"Key\"] for obj in resp.get(\"Contents\", [])]\n",
    "print(\"Found files:\", len(keys))\n",
    "for k in keys:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9273a5f2-6a04-48e3-91fc-89dd28b3cd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Athena results prefix exists: s3://aai540-olist-mlops-chris-7f3k2p/athena-results/\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "bucket = \"aai540-olist-mlops-chris-7f3k2p\"\n",
    "results_prefix = \"athena-results/\"\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "resp = s3.list_objects_v2(Bucket=bucket, Prefix=results_prefix, MaxKeys=1)\n",
    "\n",
    "if \"Contents\" in resp:\n",
    "    print(\"✅ Athena results prefix exists:\", f\"s3://{bucket}/{results_prefix}\")\n",
    "else:\n",
    "    # create a zero-byte object so the prefix exists\n",
    "    s3.put_object(Bucket=bucket, Key=results_prefix)\n",
    "    print(\"✅ Created Athena results prefix:\", f\"s3://{bucket}/{results_prefix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ca03c11-54ff-441e-bf5e-fd9c05665d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Database ready: olist_datalake\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import boto3\n",
    "\n",
    "bucket = \"aai540-olist-mlops-chris-7f3k2p\"\n",
    "REGION = boto3.session.Session().region_name\n",
    "athena = boto3.client(\"athena\", region_name=REGION)\n",
    "\n",
    "ATHENA_OUTPUT = f\"s3://{bucket}/athena-results/\"\n",
    "DB = \"olist_datalake\"\n",
    "\n",
    "def run_athena(sql: str, database: str = \"default\"):\n",
    "    res = athena.start_query_execution(\n",
    "        QueryString=sql,\n",
    "        QueryExecutionContext={\"Database\": database},\n",
    "        ResultConfiguration={\"OutputLocation\": ATHENA_OUTPUT},\n",
    "    )\n",
    "    qid = res[\"QueryExecutionId\"]\n",
    "    while True:\n",
    "        q = athena.get_query_execution(QueryExecutionId=qid)\n",
    "        state = q[\"QueryExecution\"][\"Status\"][\"State\"]\n",
    "        if state in (\"SUCCEEDED\", \"FAILED\", \"CANCELLED\"):\n",
    "            break\n",
    "        time.sleep(1)\n",
    "    if state != \"SUCCEEDED\":\n",
    "        reason = q[\"QueryExecution\"][\"Status\"].get(\"StateChangeReason\", \"Unknown\")\n",
    "        raise RuntimeError(f\"Athena query failed: {state} - {reason}\\nSQL:\\n{sql}\")\n",
    "    return qid\n",
    "\n",
    "run_athena(f\"CREATE DATABASE IF NOT EXISTS {DB};\", database=\"default\")\n",
    "print(\"✅ Database ready:\", DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e977a0a3-98b4-4b4a-90b6-d6b93a85f392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Copied to: raw/olist/ingest_date=2026-01-25/olist_customers_dataset/olist_customers_dataset.csv\n",
      "✅ Copied to: raw/olist/ingest_date=2026-01-25/olist_geolocation_dataset/olist_geolocation_dataset.csv\n",
      "✅ Copied to: raw/olist/ingest_date=2026-01-25/olist_order_items_dataset/olist_order_items_dataset.csv\n",
      "✅ Copied to: raw/olist/ingest_date=2026-01-25/olist_order_payments_dataset/olist_order_payments_dataset.csv\n",
      "✅ Copied to: raw/olist/ingest_date=2026-01-25/olist_order_reviews_dataset/olist_order_reviews_dataset.csv\n",
      "✅ Copied to: raw/olist/ingest_date=2026-01-25/olist_orders_dataset/olist_orders_dataset.csv\n",
      "✅ Copied to: raw/olist/ingest_date=2026-01-25/olist_products_dataset/olist_products_dataset.csv\n",
      "✅ Copied to: raw/olist/ingest_date=2026-01-25/olist_sellers_dataset/olist_sellers_dataset.csv\n",
      "✅ Copied to: raw/olist/ingest_date=2026-01-25/product_category_name_translation/product_category_name_translation.csv\n",
      "\n",
      "Done. Next we’ll point Athena tables at these folders.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "bucket = \"aai540-olist-mlops-chris-7f3k2p\"\n",
    "base_prefix = \"raw/olist/ingest_date=2026-01-25/\"\n",
    "\n",
    "files = [\n",
    "    \"olist_customers_dataset.csv\",\n",
    "    \"olist_geolocation_dataset.csv\",\n",
    "    \"olist_order_items_dataset.csv\",\n",
    "    \"olist_order_payments_dataset.csv\",\n",
    "    \"olist_order_reviews_dataset.csv\",\n",
    "    \"olist_orders_dataset.csv\",\n",
    "    \"olist_products_dataset.csv\",\n",
    "    \"olist_sellers_dataset.csv\",\n",
    "    \"product_category_name_translation.csv\",\n",
    "]\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "for f in files:\n",
    "    src_key = base_prefix + f\n",
    "    folder = f.replace(\".csv\", \"\")  # folder name = file name without .csv\n",
    "    dst_key = f\"{base_prefix}{folder}/{f}\"\n",
    "    \n",
    "    # copy\n",
    "    s3.copy_object(\n",
    "        Bucket=bucket,\n",
    "        CopySource={\"Bucket\": bucket, \"Key\": src_key},\n",
    "        Key=dst_key\n",
    "    )\n",
    "    print(\"✅ Copied to:\", dst_key)\n",
    "\n",
    "print(\"\\nDone. Next we’ll point Athena tables at these folders.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "858ee701-16e0-4030-bd4c-c966fe47a717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw/olist/ingest_date=2026-01-25/\n",
      "raw/olist/ingest_date=2026-01-25/olist_customers_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_customers_dataset/olist_customers_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_geolocation_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_geolocation_dataset/olist_geolocation_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_order_items_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_order_items_dataset/olist_order_items_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_order_payments_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_order_payments_dataset/olist_order_payments_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_order_reviews_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_order_reviews_dataset/olist_order_reviews_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_orders_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_orders_dataset/olist_orders_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_products_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_products_dataset/olist_products_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_sellers_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_sellers_dataset/olist_sellers_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/product_category_name_translation.csv\n",
      "raw/olist/ingest_date=2026-01-25/product_category_name_translation/product_category_name_translation.csv\n"
     ]
    }
   ],
   "source": [
    "resp = s3.list_objects_v2(Bucket=bucket, Prefix=base_prefix, MaxKeys=50)\n",
    "for obj in resp.get(\"Contents\", []):\n",
    "    print(obj[\"Key\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c29e34fd-478d-486e-ab8e-bf2105b59b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created: olist_datalake.olist_customers_dataset\n",
      "✅ Created: olist_datalake.olist_geolocation_dataset\n",
      "✅ Created: olist_datalake.olist_order_items_dataset\n",
      "✅ Created: olist_datalake.olist_order_payments_dataset\n",
      "✅ Created: olist_datalake.olist_order_reviews_dataset\n",
      "✅ Created: olist_datalake.olist_orders_dataset\n",
      "✅ Created: olist_datalake.olist_products_dataset\n",
      "✅ Created: olist_datalake.olist_sellers_dataset\n",
      "✅ Created: olist_datalake.product_category_name_translation\n"
     ]
    }
   ],
   "source": [
    "RAW_BASE = f\"s3://{bucket}/{base_prefix}\"\n",
    "\n",
    "def create_csv_table(table_name: str, columns_ddl: str, folder_name: str):\n",
    "    sql = f\"\"\"\n",
    "    CREATE EXTERNAL TABLE IF NOT EXISTS {DB}.{table_name} (\n",
    "      {columns_ddl}\n",
    "    )\n",
    "    ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
    "    WITH SERDEPROPERTIES (\n",
    "      'separatorChar' = ',',\n",
    "      'quoteChar'     = '\\\"',\n",
    "      'escapeChar'    = '\\\\\\\\'\n",
    "    )\n",
    "    STORED AS TEXTFILE\n",
    "    LOCATION '{RAW_BASE}{folder_name}/'\n",
    "    TBLPROPERTIES ('skip.header.line.count'='1');\n",
    "    \"\"\"\n",
    "    run_athena(sql, database=DB)\n",
    "    print(f\"✅ Created: {DB}.{table_name}\")\n",
    "\n",
    "create_csv_table(\n",
    "    \"olist_customers_dataset\",\n",
    "    \"\"\"\n",
    "    customer_id string,\n",
    "    customer_unique_id string,\n",
    "    customer_zip_code_prefix int,\n",
    "    customer_city string,\n",
    "    customer_state string\n",
    "    \"\"\",\n",
    "    \"olist_customers_dataset\"\n",
    ")\n",
    "\n",
    "create_csv_table(\n",
    "    \"olist_geolocation_dataset\",\n",
    "    \"\"\"\n",
    "    geolocation_zip_code_prefix int,\n",
    "    geolocation_lat double,\n",
    "    geolocation_lng double,\n",
    "    geolocation_city string,\n",
    "    geolocation_state string\n",
    "    \"\"\",\n",
    "    \"olist_geolocation_dataset\"\n",
    ")\n",
    "\n",
    "create_csv_table(\n",
    "    \"olist_order_items_dataset\",\n",
    "    \"\"\"\n",
    "    order_id string,\n",
    "    order_item_id int,\n",
    "    product_id string,\n",
    "    seller_id string,\n",
    "    shipping_limit_date string,\n",
    "    price double,\n",
    "    freight_value double\n",
    "    \"\"\",\n",
    "    \"olist_order_items_dataset\"\n",
    ")\n",
    "\n",
    "create_csv_table(\n",
    "    \"olist_order_payments_dataset\",\n",
    "    \"\"\"\n",
    "    order_id string,\n",
    "    payment_sequential int,\n",
    "    payment_type string,\n",
    "    payment_installments int,\n",
    "    payment_value double\n",
    "    \"\"\",\n",
    "    \"olist_order_payments_dataset\"\n",
    ")\n",
    "\n",
    "create_csv_table(\n",
    "    \"olist_order_reviews_dataset\",\n",
    "    \"\"\"\n",
    "    review_id string,\n",
    "    order_id string,\n",
    "    review_score int,\n",
    "    review_comment_title string,\n",
    "    review_comment_message string,\n",
    "    review_creation_date string,\n",
    "    review_answer_timestamp string\n",
    "    \"\"\",\n",
    "    \"olist_order_reviews_dataset\"\n",
    ")\n",
    "\n",
    "create_csv_table(\n",
    "    \"olist_orders_dataset\",\n",
    "    \"\"\"\n",
    "    order_id string,\n",
    "    customer_id string,\n",
    "    order_status string,\n",
    "    order_purchase_timestamp string,\n",
    "    order_approved_at string,\n",
    "    order_delivered_carrier_date string,\n",
    "    order_delivered_customer_date string,\n",
    "    order_estimated_delivery_date string\n",
    "    \"\"\",\n",
    "    \"olist_orders_dataset\"\n",
    ")\n",
    "\n",
    "create_csv_table(\n",
    "    \"olist_products_dataset\",\n",
    "    \"\"\"\n",
    "    product_id string,\n",
    "    product_category_name string,\n",
    "    product_name_lenght int,\n",
    "    product_description_lenght int,\n",
    "    product_photos_qty int,\n",
    "    product_weight_g int,\n",
    "    product_length_cm int,\n",
    "    product_height_cm int,\n",
    "    product_width_cm int\n",
    "    \"\"\",\n",
    "    \"olist_products_dataset\"\n",
    ")\n",
    "\n",
    "create_csv_table(\n",
    "    \"olist_sellers_dataset\",\n",
    "    \"\"\"\n",
    "    seller_id string,\n",
    "    seller_zip_code_prefix int,\n",
    "    seller_city string,\n",
    "    seller_state string\n",
    "    \"\"\",\n",
    "    \"olist_sellers_dataset\"\n",
    ")\n",
    "\n",
    "create_csv_table(\n",
    "    \"product_category_name_translation\",\n",
    "    \"\"\"\n",
    "    product_category_name string,\n",
    "    product_category_name_english string\n",
    "    \"\"\",\n",
    "    \"product_category_name_translation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "393bfe94-4366-42c4-b48c-8a556f415750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SHOW TABLES succeeded\n",
      "✅ COUNT orders succeeded\n",
      "✅ GROUP BY order_status succeeded\n"
     ]
    }
   ],
   "source": [
    "run_athena(f\"SHOW TABLES IN {DB};\", database=DB)\n",
    "print(\"✅ SHOW TABLES succeeded\")\n",
    "\n",
    "run_athena(f\"SELECT COUNT(*) FROM {DB}.olist_orders_dataset;\", database=DB)\n",
    "print(\"✅ COUNT orders succeeded\")\n",
    "\n",
    "run_athena(f\"SELECT order_status, COUNT(*) c FROM {DB}.olist_orders_dataset GROUP BY 1 ORDER BY c DESC;\", database=DB)\n",
    "print(\"✅ GROUP BY order_status succeeded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d48e162-12df-4319-ba71-f3cf7f1412a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-25 19:16:46,584\tWARNING services.py:2070 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 1909432320 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=4.64gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2026-01-25 19:16:46,747\tINFO worker.py:1852 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orders: (99441, 8)\n",
      "order_items: (112650, 7)\n",
      "payments: (103886, 5)\n",
      "customers: (99441, 5)\n"
     ]
    }
   ],
   "source": [
    "#6.1\n",
    "import awswrangler as wr\n",
    "import pandas as pd\n",
    "\n",
    "DB = \"olist_datalake\"\n",
    "\n",
    "orders = wr.athena.read_sql_query(\n",
    "    sql=f\"SELECT * FROM {DB}.olist_orders_dataset\",\n",
    "    database=DB,\n",
    "    ctas_approach=False\n",
    ")\n",
    "\n",
    "order_items = wr.athena.read_sql_query(\n",
    "    sql=f\"SELECT * FROM {DB}.olist_order_items_dataset\",\n",
    "    database=DB,\n",
    "    ctas_approach=False\n",
    ")\n",
    "\n",
    "payments = wr.athena.read_sql_query(\n",
    "    sql=f\"SELECT * FROM {DB}.olist_order_payments_dataset\",\n",
    "    database=DB,\n",
    "    ctas_approach=False\n",
    ")\n",
    "\n",
    "customers = wr.athena.read_sql_query(\n",
    "    sql=f\"SELECT * FROM {DB}.olist_customers_dataset\",\n",
    "    database=DB,\n",
    "    ctas_approach=False\n",
    ")\n",
    "\n",
    "print(\"orders:\", orders.shape)\n",
    "print(\"order_items:\", order_items.shape)\n",
    "print(\"payments:\", payments.shape)\n",
    "print(\"customers:\", customers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cfa6170-a17c-4f5c-8cf7-8770b8b46166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orders rows: 99441\n",
      "EDA rows: 99441\n",
      "Row loss: 0\n",
      "Late rate:\n",
      " is_late\n",
      "0    0.92129\n",
      "1    0.07871\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#6.2\n",
    "# Parse timestamps\n",
    "timestamp_cols = [\n",
    "    \"order_purchase_timestamp\",\n",
    "    \"order_approved_at\",\n",
    "    \"order_delivered_carrier_date\",\n",
    "    \"order_delivered_customer_date\",\n",
    "    \"order_estimated_delivery_date\",\n",
    "]\n",
    "for col in timestamp_cols:\n",
    "    orders[col] = pd.to_datetime(orders[col], errors=\"coerce\")\n",
    "\n",
    "# Aggregations\n",
    "items_agg = (\n",
    "    order_items.groupby(\"order_id\")\n",
    "    .agg(\n",
    "        num_items=(\"order_item_id\", \"count\"),\n",
    "        total_price=(\"price\", \"sum\"),\n",
    "        total_freight_value=(\"freight_value\", \"sum\"),\n",
    "        num_sellers=(\"seller_id\", \"nunique\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "payments_agg = (\n",
    "    payments.groupby(\"order_id\")\n",
    "    .agg(\n",
    "        payment_value=(\"payment_value\", \"sum\"),\n",
    "        payment_installments=(\"payment_installments\", \"max\"),\n",
    "        payment_type=(\"payment_type\", lambda x: x.value_counts().index[0]),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "eda_df = (\n",
    "    orders\n",
    "    .merge(items_agg, on=\"order_id\", how=\"left\")\n",
    "    .merge(payments_agg, on=\"order_id\", how=\"left\")\n",
    "    .merge(customers[[\"customer_id\", \"customer_state\"]], on=\"customer_id\", how=\"left\")\n",
    ")\n",
    "\n",
    "# Time features\n",
    "eda_df[\"purchase_dow\"] = eda_df[\"order_purchase_timestamp\"].dt.dayofweek\n",
    "eda_df[\"purchase_hour\"] = eda_df[\"order_purchase_timestamp\"].dt.hour\n",
    "\n",
    "# Label: late delivery\n",
    "eda_df[\"is_late\"] = (\n",
    "    (eda_df[\"order_delivered_customer_date\"].notna()) &\n",
    "    (eda_df[\"order_estimated_delivery_date\"].notna()) &\n",
    "    (eda_df[\"order_delivered_customer_date\"] > eda_df[\"order_estimated_delivery_date\"])\n",
    ").astype(int)\n",
    "\n",
    "print(\"Orders rows:\", len(orders))\n",
    "print(\"EDA rows:\", len(eda_df))\n",
    "print(\"Row loss:\", len(orders) - len(eda_df))\n",
    "print(\"Late rate:\\n\", eda_df[\"is_late\"].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6c44e50-a27a-4052-b192-82f344558f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ feat shape: (99441, 14)\n",
      "✅ feat_fs shape: (99441, 13)\n",
      "event_time sample: ['2017-10-02T10:56:33Z', '2018-07-24T20:41:37Z', '2018-08-08T08:38:49Z', '2017-11-18T19:28:06Z', '2018-02-13T21:18:39Z']\n"
     ]
    }
   ],
   "source": [
    "#7.0 + 7B\n",
    "# Canonical features (with purchase timestamp)\n",
    "feat = eda_df[[\n",
    "    \"order_id\",\n",
    "    \"order_purchase_timestamp\",\n",
    "    \"customer_state\",\n",
    "    \"num_items\",\n",
    "    \"total_price\",\n",
    "    \"total_freight_value\",\n",
    "    \"num_sellers\",\n",
    "    \"payment_value\",\n",
    "    \"payment_installments\",\n",
    "    \"payment_type\",\n",
    "    \"purchase_dow\",\n",
    "    \"purchase_hour\",\n",
    "    \"is_late\"\n",
    "]].copy()\n",
    "\n",
    "feat[\"customer_state\"] = feat[\"customer_state\"].fillna(\"unknown\").astype(str)\n",
    "feat[\"payment_type\"] = feat[\"payment_type\"].fillna(\"unknown\").astype(str)\n",
    "\n",
    "for c in [\"num_items\", \"num_sellers\", \"payment_installments\", \"purchase_dow\", \"purchase_hour\", \"is_late\"]:\n",
    "    feat[c] = feat[c].fillna(0).astype(int)\n",
    "\n",
    "for c in [\"total_price\", \"total_freight_value\", \"payment_value\"]:\n",
    "    feat[c] = feat[c].fillna(0.0).astype(float)\n",
    "\n",
    "# Create strict ISO-8601 event time WITH timezone \"Z\"\n",
    "feat[\"event_time\"] = (\n",
    "    pd.to_datetime(feat[\"order_purchase_timestamp\"], errors=\"coerce\")\n",
    "    .dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    ")\n",
    "\n",
    "feat = feat.dropna(subset=[\"order_id\", \"event_time\"]).reset_index(drop=True)\n",
    "\n",
    "# FeatureStore-safe version (remove datetime64 column)\n",
    "feat_fs = feat.drop(columns=[\"order_purchase_timestamp\"]).copy()\n",
    "feat_fs[\"event_time\"] = feat_fs[\"event_time\"].astype(str)\n",
    "\n",
    "print(\"✅ feat shape:\", feat.shape)\n",
    "print(\"✅ feat_fs shape:\", feat_fs.shape)\n",
    "print(\"event_time sample:\", feat_fs[\"event_time\"].head().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f15a681-f32e-4103-be35-fab9b0646be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "Region: us-east-1\n",
      "Role: arn:aws:iam::758289042916:role/LabRole\n",
      "Offline store URI: s3://aai540-olist-mlops-chris-7f3k2p/feature-store/olist-order-features-v1/\n"
     ]
    }
   ],
   "source": [
    "#7.1\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "\n",
    "bucket = \"aai540-olist-mlops-chris-7f3k2p\"\n",
    "region = boto3.session.Session().region_name\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "feature_group_name = \"olist-order-features-v1\"\n",
    "offline_store_s3_uri = f\"s3://{bucket}/feature-store/{feature_group_name}/\"\n",
    "\n",
    "fg = FeatureGroup(name=feature_group_name, sagemaker_session=sess)\n",
    "\n",
    "print(\"Region:\", region)\n",
    "print(\"Role:\", role)\n",
    "print(\"Offline store URI:\", offline_store_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b652859d-e72d-4a6b-8af1-72af458ed97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Feature Group already exists: olist-order-features-v1\n",
      "Status=Created, OfflineStoreStatus=UNKNOWN\n",
      "✅ Feature Group ready\n"
     ]
    }
   ],
   "source": [
    "#7.2\n",
    "import time\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "sm = boto3.client(\"sagemaker\", region_name=region)\n",
    "\n",
    "def feature_group_exists(name: str) -> bool:\n",
    "    try:\n",
    "        sm.describe_feature_group(FeatureGroupName=name)\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        if \"ResourceNotFound\" in str(e):\n",
    "            return False\n",
    "        raise\n",
    "\n",
    "def wait_for_fg_created(name: str, timeout_sec: int = 600, poll_sec: int = 10):\n",
    "    start = time.time()\n",
    "    while True:\n",
    "        desc = sm.describe_feature_group(FeatureGroupName=name)\n",
    "        status = desc.get(\"FeatureGroupStatus\")\n",
    "        offline_status = desc.get(\"OfflineStoreStatus\", {}).get(\"Status\", \"UNKNOWN\")\n",
    "        print(f\"Status={status}, OfflineStoreStatus={offline_status}\")\n",
    "        if status == \"Created\" and offline_status in (\"Active\", \"UNKNOWN\"):\n",
    "            return desc\n",
    "        if status in (\"CreateFailed\", \"DeleteFailed\"):\n",
    "            raise RuntimeError(f\"Feature Group failed with status={status}. Details: {desc}\")\n",
    "        if time.time() - start > timeout_sec:\n",
    "            raise TimeoutError(f\"Timed out waiting for Feature Group to be Created: {name}\")\n",
    "        time.sleep(poll_sec)\n",
    "\n",
    "if feature_group_exists(feature_group_name):\n",
    "    print(f\"✅ Feature Group already exists: {feature_group_name}\")\n",
    "else:\n",
    "    fg.load_feature_definitions(data_frame=feat_fs)\n",
    "    fg.create(\n",
    "        s3_uri=offline_store_s3_uri,\n",
    "        record_identifier_name=\"order_id\",\n",
    "        event_time_feature_name=\"event_time\",\n",
    "        role_arn=role,\n",
    "        enable_online_store=False,\n",
    "    )\n",
    "    print(\"⏳ Create request submitted\")\n",
    "\n",
    "wait_for_fg_created(feature_group_name)\n",
    "print(\"✅ Feature Group ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2460fd33-5601-4e33-a636-da6a6ce54fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ingest complete\n"
     ]
    }
   ],
   "source": [
    "#7.3\n",
    "ingest_response = fg.ingest(data_frame=feat_fs, max_workers=2, wait=True)\n",
    "print(\"✅ Ingest complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1afae01-1c16-48a2-ab19-d0115ae85d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureGroupStatus: Created\n",
      "OfflineStoreStatus: Active\n",
      "S3 Offline Store Uri: s3://aai540-olist-mlops-chris-7f3k2p/feature-store/olist-order-features-v1/\n"
     ]
    }
   ],
   "source": [
    "#7.4\n",
    "import boto3\n",
    "\n",
    "sm = boto3.client(\"sagemaker\", region_name=region)\n",
    "desc = sm.describe_feature_group(FeatureGroupName=feature_group_name)\n",
    "\n",
    "print(\"FeatureGroupStatus:\", desc[\"FeatureGroupStatus\"])\n",
    "print(\"OfflineStoreStatus:\", desc[\"OfflineStoreStatus\"][\"Status\"])\n",
    "print(\"S3 Offline Store Uri:\", desc[\"OfflineStoreConfig\"][\"S3StorageConfig\"][\"S3Uri\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68adb030-ec45-4e9a-bd5a-abbbce5622ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Split sizes\n",
      "train: 39776\n",
      "val:   9944\n",
      "test:  9944\n",
      "prod:  39777\n",
      "✅ Wrote splits to: s3://aai540-olist-mlops-chris-7f3k2p/splits/olist/features/version=v1/\n"
     ]
    }
   ],
   "source": [
    "#8.0\n",
    "import awswrangler as wr\n",
    "\n",
    "bucket = \"aai540-olist-mlops-chris-7f3k2p\"\n",
    "\n",
    "# Use feat_fs for splits (Feature Store compatible)\n",
    "feat_sorted = feat_fs.sort_values(\"event_time\").reset_index(drop=True)\n",
    "n = len(feat_sorted)\n",
    "\n",
    "train_end = int(n * 0.40)\n",
    "val_end   = int(n * 0.50)\n",
    "test_end  = int(n * 0.60)\n",
    "\n",
    "train_df = feat_sorted.iloc[:train_end]\n",
    "val_df   = feat_sorted.iloc[train_end:val_end]\n",
    "test_df  = feat_sorted.iloc[val_end:test_end]\n",
    "prod_df  = feat_sorted.iloc[test_end:]\n",
    "\n",
    "print(\"✅ Split sizes\")\n",
    "print(\"train:\", len(train_df))\n",
    "print(\"val:  \", len(val_df))\n",
    "print(\"test: \", len(test_df))\n",
    "print(\"prod: \", len(prod_df))\n",
    "\n",
    "split_base = f\"s3://{bucket}/splits/olist/features/version=v1/\"\n",
    "wr.s3.to_parquet(train_df, f\"{split_base}train/\", dataset=True, mode=\"overwrite\")\n",
    "wr.s3.to_parquet(val_df,   f\"{split_base}val/\",   dataset=True, mode=\"overwrite\")\n",
    "wr.s3.to_parquet(test_df,  f\"{split_base}test/\",  dataset=True, mode=\"overwrite\")\n",
    "wr.s3.to_parquet(prod_df,  f\"{split_base}prod/\",  dataset=True, mode=\"overwrite\")\n",
    "\n",
    "print(\"✅ Wrote splits to:\", split_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8ab440e-a4ea-477e-bad5-fc372615b00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5822e971-6f0e-4ddb-acb7-41898a947a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objects found: 4\n",
      "splits/olist/features/version=v1/prod/a8cdd289b48b495ba34f072d8dfa9932.snappy.parquet\n",
      "splits/olist/features/version=v1/test/ba691db52493437ab3c663c8b98fe955.snappy.parquet\n",
      "splits/olist/features/version=v1/train/33879ef6e41a44e384d5255caa4ffa7f.snappy.parquet\n",
      "splits/olist/features/version=v1/val/6fb7a4263ec04acb8f57f8ad0050cfb0.snappy.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1769369955.066884   28608 chttp2_transport.cc:1182] ipv4:169.255.255.2:57431: Got goaway [2] err=UNAVAILABLE:GOAWAY received; Error code: 2; Debug Text: Cancelling all calls {grpc_status:14, http2_error:2, created_time:\"2026-01-25T19:39:15.064991581+00:00\"}\n",
      "*** SIGTERM received at time=1769369957 on cpu 0 ***\n",
      "PC: @     0x7f13accaae9e  (unknown)  epoll_wait\n",
      "    @     0x7f1357624b0d         64  absl::lts_20240722::AbslFailureSignalHandler()\n",
      "    @     0x7f13acbc7520  (unknown)  (unknown)\n",
      "[2026-01-25 19:39:17,560 E 28266 28266] logging.cc:497: *** SIGTERM received at time=1769369957 on cpu 0 ***\n",
      "[2026-01-25 19:39:17,560 E 28266 28266] logging.cc:497: PC: @     0x7f13accaae9e  (unknown)  epoll_wait\n",
      "[2026-01-25 19:39:17,561 E 28266 28266] logging.cc:497:     @     0x7f1357624b39         64  absl::lts_20240722::AbslFailureSignalHandler()\n",
      "[2026-01-25 19:39:17,561 E 28266 28266] logging.cc:497:     @     0x7f13acbc7520  (unknown)  (unknown)\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "prefix = \"splits/olist/features/version=v1/\"\n",
    "\n",
    "resp = s3.list_objects_v2(Bucket=bucket, Prefix=prefix, MaxKeys=50)\n",
    "print(\"Objects found:\", resp.get(\"KeyCount\", 0))\n",
    "for obj in resp.get(\"Contents\", [])[:20]:\n",
    "    print(obj[\"Key\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bab04fb0-989f-4744-bbda-b33cb55b73ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Kernel is fresh — variables not loaded\n"
     ]
    }
   ],
   "source": [
    "#M4 Start\n",
    "try:\n",
    "    print(len(train))\n",
    "except NameError:\n",
    "    print(\"❌ Kernel is fresh — variables not loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0034099-aa6f-4045-a5b2-4375191fdacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import awswrangler as wr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "bucket = \"aai540-olist-mlops-chris-7f3k2p\"\n",
    "split_base = f\"s3://{bucket}/splits/olist/features/version=v1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae9faaf5-2b66-4cc1-b3e1-7d0e8e541b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-31 19:18:39,008\tWARNING services.py:2070 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 1938792448 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=4.61gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2026-01-31 19:18:40,252\tINFO worker.py:1852 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39776, 13) (9944, 13) (9944, 13)\n"
     ]
    }
   ],
   "source": [
    "train = wr.s3.read_parquet(f\"{split_base}train/\")\n",
    "val   = wr.s3.read_parquet(f\"{split_base}val/\")\n",
    "test  = wr.s3.read_parquet(f\"{split_base}test/\")\n",
    "\n",
    "print(train.shape, val.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d4945ad-cf8d-4724-a7a2-9a62e20ad344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8621279163314561, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#M4-1 Benchmark Model Baseline\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Ground truth\n",
    "y_test = test[\"is_late\"].astype(int)\n",
    "\n",
    "# Baseline prediction: always predict NOT late\n",
    "y_pred_baseline = pd.Series(0, index=test.index)\n",
    "\n",
    "baseline_metrics = {\n",
    "    \"accuracy\": accuracy_score(y_test, y_pred_baseline),\n",
    "    \"precision\": precision_score(y_test, y_pred_baseline, zero_division=0),\n",
    "    \"recall\": recall_score(y_test, y_pred_baseline, zero_division=0),\n",
    "    \"f1\": f1_score(y_test, y_pred_baseline, zero_division=0),\n",
    "}\n",
    "\n",
    "baseline_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30e4bb0b-3070-47b1-9036-21f7f01bbbb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_late</th>\n",
       "      <th>num_items</th>\n",
       "      <th>total_price</th>\n",
       "      <th>total_freight_value</th>\n",
       "      <th>num_sellers</th>\n",
       "      <th>payment_value</th>\n",
       "      <th>payment_installments</th>\n",
       "      <th>purchase_dow</th>\n",
       "      <th>purchase_hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>72.89</td>\n",
       "      <td>63.34</td>\n",
       "      <td>1</td>\n",
       "      <td>136.23</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>59.50</td>\n",
       "      <td>15.56</td>\n",
       "      <td>1</td>\n",
       "      <td>75.06</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>40.95</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>134.97</td>\n",
       "      <td>8.49</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100.00</td>\n",
       "      <td>9.34</td>\n",
       "      <td>1</td>\n",
       "      <td>109.34</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_late  num_items  total_price  total_freight_value  num_sellers  \\\n",
       "0        0          2        72.89                63.34            1   \n",
       "1        0          1        59.50                15.56            1   \n",
       "2        0          0         0.00                 0.00            0   \n",
       "3        1          3       134.97                 8.49            1   \n",
       "4        0          1       100.00                 9.34            1   \n",
       "\n",
       "   payment_value  payment_installments  purchase_dow  purchase_hour  \n",
       "0         136.23                     1             6             21  \n",
       "1          75.06                     3             0              0  \n",
       "2          40.95                     2             1             15  \n",
       "3           0.00                     0             3             12  \n",
       "4         109.34                     1             6             22  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#M4-2 1st Model Sage Maker\n",
    "\n",
    "FEATURES = [\n",
    "    \"num_items\",\n",
    "    \"total_price\",\n",
    "    \"total_freight_value\",\n",
    "    \"num_sellers\",\n",
    "    \"payment_value\",\n",
    "    \"payment_installments\",\n",
    "    \"purchase_dow\",\n",
    "    \"purchase_hour\",\n",
    "]\n",
    "\n",
    "def to_xgb_matrix(df):\n",
    "    out = df[[\"is_late\"] + FEATURES].copy()\n",
    "    for c in FEATURES:\n",
    "        out[c] = pd.to_numeric(out[c], errors=\"coerce\").fillna(0.0)\n",
    "    out[\"is_late\"] = out[\"is_late\"].astype(int)\n",
    "    return out\n",
    "\n",
    "train_xgb = to_xgb_matrix(train)\n",
    "val_xgb   = to_xgb_matrix(val)\n",
    "test_xgb  = to_xgb_matrix(test)\n",
    "\n",
    "train_xgb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a07fbf40-aeb6-4aa5-ab61-cdb2068dba2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: s3://aai540-olist-mlops-chris-7f3k2p/modeling/xgb_v1/train/train.csv\n",
      "Val:   s3://aai540-olist-mlops-chris-7f3k2p/modeling/xgb_v1/val/val.csv\n",
      "Test:  s3://aai540-olist-mlops-chris-7f3k2p/modeling/xgb_v1/test/test.csv\n"
     ]
    }
   ],
   "source": [
    "#M4-2-2\n",
    "## NOTE: CSVs already written prior to training — do not rerun\n",
    "xgb_prefix = f\"s3://{bucket}/modeling/xgb_v1/\"\n",
    "\n",
    "train_csv_uri = f\"{xgb_prefix}train/train.csv\"\n",
    "val_csv_uri   = f\"{xgb_prefix}val/val.csv\"\n",
    "test_csv_uri  = f\"{xgb_prefix}test/test.csv\"\n",
    "\n",
    "wr.s3.to_csv(train_xgb, train_csv_uri, index=False, header=False)\n",
    "wr.s3.to_csv(val_xgb,   val_csv_uri,   index=False, header=False)\n",
    "wr.s3.to_csv(test_xgb,  test_csv_uri,  index=False, header=False)\n",
    "\n",
    "print(\"Train:\", train_csv_uri)\n",
    "print(\"Val:  \", val_csv_uri)\n",
    "print(\"Test: \", test_csv_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db400ea4-a70a-457a-9441-2c4bb5b1e071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2026-01-31-18-00-53-460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-31 18:00:53 Starting - Starting the training job...\n",
      "2026-01-31 18:01:14 Starting - Preparing the instances for training...\n",
      "2026-01-31 18:01:38 Downloading - Downloading input data...\n",
      "2026-01-31 18:02:23 Downloading - Downloading the training image........\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m[2026-01-31 18:03:35.611 ip-10-2-90-203.ec2.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2026-01-31 18:03:35.683 ip-10-2-90-203.ec2.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2026-01-31:18:03:36:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2026-01-31:18:03:36:INFO] Failed to parse hyperparameter eval_metric value auc to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2026-01-31:18:03:36:INFO] Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2026-01-31:18:03:36:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-01-31:18:03:36:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34m[2026-01-31:18:03:36:INFO] Determined 0 GPU(s) available on the instance.\u001b[0m\n",
      "\u001b[34m[2026-01-31:18:03:36:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2026-01-31:18:03:36:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2026-01-31:18:03:36:INFO] File path /opt/ml/input/data/train of input files\u001b[0m\n",
      "\u001b[34m[2026-01-31:18:03:36:INFO] Making smlinks from folder /opt/ml/input/data/train to folder /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2026-01-31:18:03:36:INFO] creating symlink between Path /opt/ml/input/data/train/train.csv and destination /tmp/sagemaker_xgboost_input_data/train.csv-4925750691625951217\u001b[0m\n",
      "\u001b[34m[2026-01-31:18:03:36:INFO] files path: /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2026-01-31:18:03:36:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2026-01-31:18:03:36:INFO] File path /opt/ml/input/data/validation of input files\u001b[0m\n",
      "\u001b[34m[2026-01-31:18:03:36:INFO] Making smlinks from folder /opt/ml/input/data/validation to folder /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2026-01-31:18:03:36:INFO] creating symlink between Path /opt/ml/input/data/validation/val.csv and destination /tmp/sagemaker_xgboost_input_data/val.csv-493361961238117506\u001b[0m\n",
      "\u001b[34m[2026-01-31:18:03:36:INFO] files path: /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2026-01-31:18:03:36:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2026-01-31:18:03:36:INFO] Single node training.\u001b[0m\n",
      "\u001b[34m[2026-01-31:18:03:36:INFO] Train matrix has 39776 rows and 8 columns\u001b[0m\n",
      "\u001b[34m[2026-01-31:18:03:36:INFO] Validation matrix has 9944 rows\u001b[0m\n",
      "\u001b[34m[2026-01-31 18:03:36.084 ip-10-2-90-203.ec2.internal:7 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2026-01-31 18:03:36.085 ip-10-2-90-203.ec2.internal:7 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2026-01-31 18:03:36.085 ip-10-2-90-203.ec2.internal:7 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2026-01-31 18:03:36.086 ip-10-2-90-203.ec2.internal:7 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2026-01-31:18:03:36:INFO] Debug hook created from config\u001b[0m\n",
      "\u001b[34m[0]#011train-auc:0.57718#011validation-auc:0.60987\u001b[0m\n",
      "\u001b[34m[2026-01-31 18:03:36.133 ip-10-2-90-203.ec2.internal:7 INFO hook.py:427] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[34m[2026-01-31 18:03:36.138 ip-10-2-90-203.ec2.internal:7 INFO hook.py:491] Hook is writing from the hook with pid: 7\u001b[0m\n",
      "\u001b[34m[1]#011train-auc:0.58678#011validation-auc:0.61750\u001b[0m\n",
      "\u001b[34m[2]#011train-auc:0.58967#011validation-auc:0.61922\u001b[0m\n",
      "\u001b[34m[3]#011train-auc:0.59464#011validation-auc:0.60855\u001b[0m\n",
      "\u001b[34m[4]#011train-auc:0.60033#011validation-auc:0.61503\u001b[0m\n",
      "\u001b[34m[5]#011train-auc:0.60274#011validation-auc:0.61439\u001b[0m\n",
      "\u001b[34m[6]#011train-auc:0.60120#011validation-auc:0.61226\u001b[0m\n",
      "\u001b[34m[7]#011train-auc:0.60385#011validation-auc:0.61266\u001b[0m\n",
      "\u001b[34m[8]#011train-auc:0.60486#011validation-auc:0.61274\u001b[0m\n",
      "\u001b[34m[9]#011train-auc:0.60711#011validation-auc:0.61181\u001b[0m\n",
      "\u001b[34m[10]#011train-auc:0.60697#011validation-auc:0.61046\u001b[0m\n",
      "\u001b[34m[11]#011train-auc:0.60916#011validation-auc:0.60854\u001b[0m\n",
      "\u001b[34m[12]#011train-auc:0.61248#011validation-auc:0.60882\u001b[0m\n",
      "\u001b[34m[13]#011train-auc:0.61380#011validation-auc:0.60906\u001b[0m\n",
      "\u001b[34m[14]#011train-auc:0.61496#011validation-auc:0.60803\u001b[0m\n",
      "\u001b[34m[15]#011train-auc:0.61577#011validation-auc:0.60883\u001b[0m\n",
      "\u001b[34m[16]#011train-auc:0.61938#011validation-auc:0.60737\u001b[0m\n",
      "\u001b[34m[17]#011train-auc:0.62391#011validation-auc:0.60716\u001b[0m\n",
      "\u001b[34m[18]#011train-auc:0.62429#011validation-auc:0.60665\u001b[0m\n",
      "\u001b[34m[19]#011train-auc:0.62490#011validation-auc:0.60728\u001b[0m\n",
      "\u001b[34m[20]#011train-auc:0.62650#011validation-auc:0.60556\u001b[0m\n",
      "\u001b[34m[21]#011train-auc:0.62888#011validation-auc:0.60332\u001b[0m\n",
      "\u001b[34m[22]#011train-auc:0.63095#011validation-auc:0.60233\u001b[0m\n",
      "\u001b[34m[23]#011train-auc:0.63570#011validation-auc:0.59728\u001b[0m\n",
      "\u001b[34m[24]#011train-auc:0.63690#011validation-auc:0.59524\u001b[0m\n",
      "\u001b[34m[25]#011train-auc:0.63829#011validation-auc:0.59603\u001b[0m\n",
      "\u001b[34m[26]#011train-auc:0.64073#011validation-auc:0.59587\u001b[0m\n",
      "\u001b[34m[27]#011train-auc:0.64565#011validation-auc:0.59651\u001b[0m\n",
      "\u001b[34m[28]#011train-auc:0.64638#011validation-auc:0.59769\u001b[0m\n",
      "\u001b[34m[29]#011train-auc:0.64903#011validation-auc:0.59423\u001b[0m\n",
      "\u001b[34m[30]#011train-auc:0.65247#011validation-auc:0.59341\u001b[0m\n",
      "\u001b[34m[31]#011train-auc:0.65328#011validation-auc:0.59380\u001b[0m\n",
      "\u001b[34m[32]#011train-auc:0.65542#011validation-auc:0.59545\u001b[0m\n",
      "\u001b[34m[33]#011train-auc:0.65942#011validation-auc:0.59481\u001b[0m\n",
      "\u001b[34m[34]#011train-auc:0.66074#011validation-auc:0.59615\u001b[0m\n",
      "\u001b[34m[35]#011train-auc:0.66255#011validation-auc:0.59685\u001b[0m\n",
      "\u001b[34m[36]#011train-auc:0.66308#011validation-auc:0.59537\u001b[0m\n",
      "\u001b[34m[37]#011train-auc:0.66411#011validation-auc:0.59450\u001b[0m\n",
      "\u001b[34m[38]#011train-auc:0.66587#011validation-auc:0.59220\u001b[0m\n",
      "\u001b[34m[39]#011train-auc:0.66659#011validation-auc:0.59020\u001b[0m\n",
      "\u001b[34m[40]#011train-auc:0.67067#011validation-auc:0.58837\u001b[0m\n",
      "\u001b[34m[41]#011train-auc:0.67208#011validation-auc:0.59180\u001b[0m\n",
      "\u001b[34m[42]#011train-auc:0.67470#011validation-auc:0.59018\u001b[0m\n",
      "\u001b[34m[43]#011train-auc:0.67599#011validation-auc:0.58973\u001b[0m\n",
      "\u001b[34m[44]#011train-auc:0.67859#011validation-auc:0.59031\u001b[0m\n",
      "\u001b[34m[45]#011train-auc:0.68025#011validation-auc:0.59029\u001b[0m\n",
      "\u001b[34m[46]#011train-auc:0.68055#011validation-auc:0.58914\u001b[0m\n",
      "\u001b[34m[47]#011train-auc:0.68338#011validation-auc:0.58682\u001b[0m\n",
      "\u001b[34m[48]#011train-auc:0.68535#011validation-auc:0.58531\u001b[0m\n",
      "\u001b[34m[49]#011train-auc:0.68663#011validation-auc:0.58506\u001b[0m\n",
      "\u001b[34m[50]#011train-auc:0.68960#011validation-auc:0.58067\u001b[0m\n",
      "\u001b[34m[51]#011train-auc:0.69082#011validation-auc:0.57944\u001b[0m\n",
      "\u001b[34m[52]#011train-auc:0.69350#011validation-auc:0.57876\u001b[0m\n",
      "\u001b[34m[53]#011train-auc:0.69431#011validation-auc:0.57790\u001b[0m\n",
      "\u001b[34m[54]#011train-auc:0.69508#011validation-auc:0.57814\u001b[0m\n",
      "\u001b[34m[55]#011train-auc:0.69788#011validation-auc:0.57586\u001b[0m\n",
      "\u001b[34m[56]#011train-auc:0.69958#011validation-auc:0.57689\u001b[0m\n",
      "\u001b[34m[57]#011train-auc:0.69967#011validation-auc:0.57617\u001b[0m\n",
      "\u001b[34m[58]#011train-auc:0.70106#011validation-auc:0.57703\u001b[0m\n",
      "\u001b[34m[59]#011train-auc:0.70124#011validation-auc:0.57684\u001b[0m\n",
      "\u001b[34m[60]#011train-auc:0.70351#011validation-auc:0.57662\u001b[0m\n",
      "\u001b[34m[61]#011train-auc:0.70420#011validation-auc:0.57699\u001b[0m\n",
      "\u001b[34m[62]#011train-auc:0.70539#011validation-auc:0.57893\u001b[0m\n",
      "\u001b[34m[63]#011train-auc:0.70704#011validation-auc:0.57891\u001b[0m\n",
      "\u001b[34m[64]#011train-auc:0.70791#011validation-auc:0.57796\u001b[0m\n",
      "\u001b[34m[65]#011train-auc:0.71070#011validation-auc:0.57732\u001b[0m\n",
      "\u001b[34m[66]#011train-auc:0.71187#011validation-auc:0.57656\u001b[0m\n",
      "\u001b[34m[67]#011train-auc:0.71214#011validation-auc:0.57604\u001b[0m\n",
      "\u001b[34m[68]#011train-auc:0.71341#011validation-auc:0.57686\u001b[0m\n",
      "\u001b[34m[69]#011train-auc:0.71415#011validation-auc:0.57558\u001b[0m\n",
      "\u001b[34m[70]#011train-auc:0.71535#011validation-auc:0.57584\u001b[0m\n",
      "\u001b[34m[71]#011train-auc:0.71621#011validation-auc:0.57537\u001b[0m\n",
      "\u001b[34m[72]#011train-auc:0.71661#011validation-auc:0.57624\u001b[0m\n",
      "\u001b[34m[73]#011train-auc:0.71805#011validation-auc:0.57630\u001b[0m\n",
      "\u001b[34m[74]#011train-auc:0.71962#011validation-auc:0.57667\u001b[0m\n",
      "\u001b[34m[75]#011train-auc:0.72031#011validation-auc:0.57737\u001b[0m\n",
      "\u001b[34m[76]#011train-auc:0.72202#011validation-auc:0.57646\u001b[0m\n",
      "\u001b[34m[77]#011train-auc:0.72265#011validation-auc:0.57596\u001b[0m\n",
      "\u001b[34m[78]#011train-auc:0.72382#011validation-auc:0.57490\u001b[0m\n",
      "\u001b[34m[79]#011train-auc:0.72416#011validation-auc:0.57438\u001b[0m\n",
      "\u001b[34m[80]#011train-auc:0.72454#011validation-auc:0.57428\u001b[0m\n",
      "\u001b[34m[81]#011train-auc:0.72604#011validation-auc:0.57419\u001b[0m\n",
      "\u001b[34m[82]#011train-auc:0.72744#011validation-auc:0.57596\u001b[0m\n",
      "\u001b[34m[83]#011train-auc:0.72923#011validation-auc:0.57486\u001b[0m\n",
      "\u001b[34m[84]#011train-auc:0.73094#011validation-auc:0.57390\u001b[0m\n",
      "\u001b[34m[85]#011train-auc:0.73201#011validation-auc:0.57408\u001b[0m\n",
      "\u001b[34m[86]#011train-auc:0.73278#011validation-auc:0.57454\u001b[0m\n",
      "\u001b[34m[87]#011train-auc:0.73306#011validation-auc:0.57505\u001b[0m\n",
      "\u001b[34m[88]#011train-auc:0.73337#011validation-auc:0.57517\u001b[0m\n",
      "\u001b[34m[89]#011train-auc:0.73389#011validation-auc:0.57457\u001b[0m\n",
      "\u001b[34m[90]#011train-auc:0.73583#011validation-auc:0.57328\u001b[0m\n",
      "\u001b[34m[91]#011train-auc:0.73729#011validation-auc:0.57133\u001b[0m\n",
      "\u001b[34m[92]#011train-auc:0.73771#011validation-auc:0.57227\u001b[0m\n",
      "\u001b[34m[93]#011train-auc:0.73876#011validation-auc:0.57303\u001b[0m\n",
      "\u001b[34m[94]#011train-auc:0.74023#011validation-auc:0.57281\u001b[0m\n",
      "\u001b[34m[95]#011train-auc:0.74183#011validation-auc:0.57206\u001b[0m\n",
      "\u001b[34m[96]#011train-auc:0.74366#011validation-auc:0.57112\u001b[0m\n",
      "\u001b[34m[97]#011train-auc:0.74468#011validation-auc:0.57151\u001b[0m\n",
      "\u001b[34m[98]#011train-auc:0.74505#011validation-auc:0.57078\u001b[0m\n",
      "\u001b[34m[99]#011train-auc:0.74611#011validation-auc:0.57011\u001b[0m\n",
      "\n",
      "2026-01-31 18:03:42 Training - Training image download completed. Training in progress.\n",
      "2026-01-31 18:03:42 Uploading - Uploading generated training model\n",
      "2026-01-31 18:04:00 Completed - Training job completed\n",
      "Training seconds: 143\n",
      "Billable seconds: 143\n"
     ]
    }
   ],
   "source": [
    "#M4-2-3 Train Model\n",
    "# TRAINING CELL (DO NOT RERUN)\n",
    "# This cell was executed once to train the initial XGBoost model.\n",
    "# Re-running this cell will retrain the model and incur additional cost.\n",
    "# The trained model is reused below via attachment for evaluation and deployment.\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "# Built-in XGBoost container\n",
    "xgb_image = retrieve(\n",
    "    framework=\"xgboost\",\n",
    "    region=region,\n",
    "    version=\"1.7-1\"\n",
    ")\n",
    "\n",
    "output_path = f\"s3://{bucket}/modeling/xgb_v1/output/\"\n",
    "\n",
    "xgb = Estimator(\n",
    "    image_uri=xgb_image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",  # budget-friendly\n",
    "    output_path=output_path,\n",
    "    sagemaker_session=sess,\n",
    ")\n",
    "\n",
    "# Simple, reasonable first-pass hyperparameters\n",
    "xgb.set_hyperparameters(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\",\n",
    "    num_round=100,\n",
    "    max_depth=4,\n",
    "    eta=0.2,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    ")\n",
    "\n",
    "train_input = TrainingInput(train_csv_uri, content_type=\"text/csv\")\n",
    "val_input   = TrainingInput(val_csv_uri, content_type=\"text/csv\")\n",
    "\n",
    "#xgb.fit({\n",
    "#    \"train\": train_input,\n",
    "#    \"validation\": val_input\n",
    "#})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0151caee-82d9-4259-819d-bf0696a22481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "✅ Using training job: sagemaker-xgboost-2026-01-31-18-00-53-460\n",
      "✅ Attached. Ready for Batch Transform.\n"
     ]
    }
   ],
   "source": [
    "# M4-2-3b Attach to Existing Trained Model (No Retraining)\n",
    "import boto3, sagemaker\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.session.Session().region_name\n",
    "sm = boto3.client(\"sagemaker\", region_name=region)\n",
    "\n",
    "# Pick the most recent completed training job\n",
    "jobs = sm.list_training_jobs(SortBy=\"CreationTime\", SortOrder=\"Descending\", MaxResults=20)[\"TrainingJobSummaries\"]\n",
    "training_job_name = next(j[\"TrainingJobName\"] for j in jobs if j[\"TrainingJobStatus\"] == \"Completed\")\n",
    "print(\"✅ Using training job:\", training_job_name)\n",
    "\n",
    "# Recreate estimator and attach (no retraining)\n",
    "xgb_image = retrieve(framework=\"xgboost\", region=region, version=\"1.7-1\")\n",
    "xgb = Estimator(\n",
    "    image_uri=xgb_image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    output_path=f\"s3://{bucket}/modeling/xgb_v1/output/\",\n",
    "    sagemaker_session=sess,\n",
    ")\n",
    "xgb._current_job_name = training_job_name\n",
    "print(\"✅ Attached. Ready for Batch Transform.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "574dce64-f186-495e-a51a-7c63553268cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: s3://aai540-olist-mlops-chris-7f3k2p/modeling/xgb_v1/train/train.csv\n",
      "Val:   s3://aai540-olist-mlops-chris-7f3k2p/modeling/xgb_v1/val/val.csv\n",
      "Test:  s3://aai540-olist-mlops-chris-7f3k2p/modeling/xgb_v1/test/test.csv\n"
     ]
    }
   ],
   "source": [
    "# M4-2-2b Recreate CSV URIs\n",
    "\n",
    "xgb_prefix = f\"s3://{bucket}/modeling/xgb_v1/\"\n",
    "\n",
    "train_csv_uri = f\"{xgb_prefix}train/train.csv\"\n",
    "val_csv_uri   = f\"{xgb_prefix}val/val.csv\"\n",
    "test_csv_uri  = f\"{xgb_prefix}test/test.csv\"\n",
    "\n",
    "print(\"Train:\", train_csv_uri)\n",
    "print(\"Val:  \", val_csv_uri)\n",
    "print(\"Test: \", test_csv_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7bf01c7-3661-40d1-8cb6-9298677f125d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote inference CSV: s3://aai540-olist-mlops-chris-7f3k2p/modeling/xgb_v1/test/test_infer.csv\n",
      "Shape (should be 9944 x 8): (9944, 8)\n"
     ]
    }
   ],
   "source": [
    "# M4-3.0 Create inference-only TEST input (features only)\n",
    "# Code developed using ChatGPT (ChatGPT, 2024) as a paired programmer.\n",
    "\n",
    "# test_xgb currently has: is_late + 8 features\n",
    "test_infer = test_xgb.drop(columns=[\"is_late\"]).copy()\n",
    "\n",
    "test_infer_csv_uri = f\"{xgb_prefix}test/test_infer.csv\"\n",
    "\n",
    "# IMPORTANT: no header, no index\n",
    "wr.s3.to_csv(test_infer, test_infer_csv_uri, index=False, header=False)\n",
    "\n",
    "print(\"✅ Wrote inference CSV:\", test_infer_csv_uri)\n",
    "print(\"Shape (should be 9944 x 8):\", test_infer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93fd6908-b378-4c81-924d-062f65c4215a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating transform job with name: sagemaker-xgboost-2026-01-31-19-39-44-779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Starting batch transform (features-only input)...\n",
      ".................................\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:17:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:17:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:17:INFO] nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2026-01-31 19:45:17 +0000] [13] [INFO] Starting gunicorn 23.0.0\u001b[0m\n",
      "\u001b[34m[2026-01-31 19:45:17 +0000] [13] [INFO] Listening at: unix:/tmp/gunicorn.sock (13)\u001b[0m\n",
      "\u001b[34m[2026-01-31 19:45:17 +0000] [13] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2026-01-31 19:45:17 +0000] [16] [INFO] Booting worker with pid: 16\u001b[0m\n",
      "\u001b[34m[2026-01-31 19:45:17 +0000] [17] [INFO] Booting worker with pid: 17\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:19:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:19:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:19:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:19:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:19:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:19:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:26:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [31/Jan/2026:19:45:26 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:26:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [31/Jan/2026:19:45:26 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:26:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [31/Jan/2026:19:45:26 +0000] \"POST /invocations HTTP/1.1\" 200 200088 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2026-01-31:19:45:26:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [31/Jan/2026:19:45:26 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2026-01-31:19:45:26:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [31/Jan/2026:19:45:26 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2026-01-31:19:45:26:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [31/Jan/2026:19:45:26 +0000] \"POST /invocations HTTP/1.1\" 200 200088 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2026-01-31T19:45:26.181:[sagemaker logs]: MaxConcurrentTransforms=2, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:17:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:17:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:17:INFO] nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2026-01-31 19:45:17 +0000] [13] [INFO] Starting gunicorn 23.0.0\u001b[0m\n",
      "\u001b[34m[2026-01-31 19:45:17 +0000] [13] [INFO] Listening at: unix:/tmp/gunicorn.sock (13)\u001b[0m\n",
      "\u001b[34m[2026-01-31 19:45:17 +0000] [13] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2026-01-31 19:45:17 +0000] [16] [INFO] Booting worker with pid: 16\u001b[0m\n",
      "\u001b[35m[2026-01-31:19:45:17:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2026-01-31:19:45:17:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2026-01-31:19:45:17:INFO] nginx config: \u001b[0m\n",
      "\u001b[35mworker_processes auto;\u001b[0m\n",
      "\u001b[35mdaemon off;\u001b[0m\n",
      "\u001b[35mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[35merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[35mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[35mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35m[2026-01-31 19:45:17 +0000] [13] [INFO] Starting gunicorn 23.0.0\u001b[0m\n",
      "\u001b[35m[2026-01-31 19:45:17 +0000] [13] [INFO] Listening at: unix:/tmp/gunicorn.sock (13)\u001b[0m\n",
      "\u001b[35m[2026-01-31 19:45:17 +0000] [13] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[35m[2026-01-31 19:45:17 +0000] [16] [INFO] Booting worker with pid: 16\u001b[0m\n",
      "\u001b[34m[2026-01-31 19:45:17 +0000] [17] [INFO] Booting worker with pid: 17\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:19:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:19:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:19:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:19:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:19:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:19:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[35m[2026-01-31 19:45:17 +0000] [17] [INFO] Booting worker with pid: 17\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[35m[2026-01-31:19:45:19:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2026-01-31:19:45:19:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[35m[2026-01-31:19:45:19:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[35m[2026-01-31:19:45:19:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2026-01-31:19:45:19:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[35m[2026-01-31:19:45:19:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:26:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [31/Jan/2026:19:45:26 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:26:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [31/Jan/2026:19:45:26 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2026-01-31:19:45:26:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [31/Jan/2026:19:45:26 +0000] \"POST /invocations HTTP/1.1\" 200 200088 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2026-01-31:19:45:26:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [31/Jan/2026:19:45:26 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2026-01-31:19:45:26:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [31/Jan/2026:19:45:26 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2026-01-31:19:45:26:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [31/Jan/2026:19:45:26 +0000] \"POST /invocations HTTP/1.1\" 200 200088 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2026-01-31T19:45:26.181:[sagemaker logs]: MaxConcurrentTransforms=2, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "✅ Batch transform finished: s3://aai540-olist-mlops-chris-7f3k2p/modeling/xgb_v1/batch-out/test_v2/\n"
     ]
    }
   ],
   "source": [
    "# M4-3.1 Batch Transform on TEST (Evaluation) - v2 (features-only)\n",
    "from sagemaker.transformer import Transformer\n",
    "\n",
    "test_transform_output_v2 = f\"s3://{bucket}/modeling/xgb_v1/batch-out/test_v2/\"\n",
    "\n",
    "transformer = Transformer(\n",
    "    model_name=model_name,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    output_path=test_transform_output_v2,\n",
    "    assemble_with=\"Line\",\n",
    "    accept=\"text/csv\"\n",
    ")\n",
    "\n",
    "print(\"⏳ Starting batch transform (features-only input)...\")\n",
    "transformer.transform(\n",
    "    data=test_infer_csv_uri,\n",
    "    content_type=\"text/csv\",\n",
    "    split_type=\"Line\"\n",
    ")\n",
    "\n",
    "transformer.wait()\n",
    "print(\"✅ Batch transform finished:\", test_transform_output_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c23b185-e888-4cb8-a1bf-1c2d89f33e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading predictions from: s3://aai540-olist-mlops-chris-7f3k2p/modeling/xgb_v1/batch-out/test_v2/test_infer.csv.out\n",
      "✅ Model metrics: {'auc': 0.5635208855035949, 'accuracy': 0.8621279163314561, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline_always_on_time</th>\n",
       "      <td>0.862128</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb_v1</th>\n",
       "      <td>0.862128</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.563521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         accuracy  precision  recall   f1       auc\n",
       "baseline_always_on_time  0.862128        0.0     0.0  0.0       NaN\n",
       "xgb_v1                   0.862128        0.0     0.0  0.0  0.563521"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# M4-3.2 Load predictions and evaluate\n",
    "import boto3\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "resp = s3.list_objects_v2(Bucket=bucket, Prefix=\"modeling/xgb_v1/batch-out/test_v2/\")\n",
    "keys = [o[\"Key\"] for o in resp.get(\"Contents\", [])]\n",
    "out_files = [k for k in keys if k.endswith(\".out\")]\n",
    "\n",
    "if not out_files:\n",
    "    raise RuntimeError(f\"No .out files found in test_v2 output. Keys seen: {keys[:10]}\")\n",
    "\n",
    "out_key = sorted(out_files)[-1]\n",
    "out_uri = f\"s3://{bucket}/{out_key}\"\n",
    "print(\"Reading predictions from:\", out_uri)\n",
    "\n",
    "pred_df = wr.s3.read_csv(out_uri, header=None)\n",
    "y_prob = pred_df[0].astype(float).reset_index(drop=True)\n",
    "\n",
    "y_true = test_xgb[\"is_late\"].astype(int).reset_index(drop=True)\n",
    "y_hat = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "model_metrics = {\n",
    "    \"auc\": roc_auc_score(y_true, y_prob),\n",
    "    \"accuracy\": accuracy_score(y_true, y_hat),\n",
    "    \"precision\": precision_score(y_true, y_hat, zero_division=0),\n",
    "    \"recall\": recall_score(y_true, y_hat, zero_division=0),\n",
    "    \"f1\": f1_score(y_true, y_hat, zero_division=0),\n",
    "}\n",
    "\n",
    "comparison = pd.DataFrame(\n",
    "    [baseline_metrics, model_metrics],\n",
    "    index=[\"baseline_always_on_time\", \"xgb_v1\"]\n",
    ")\n",
    "\n",
    "print(\"✅ Model metrics:\", model_metrics)\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "907c66f6-bf25-49d8-ac61-ca93e7100c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Deleted model: xgb-v1-1769887733\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "sm = boto3.client(\"sagemaker\", region_name=region)\n",
    "sm.delete_model(ModelName=model_name)\n",
    "print(\"✅ Deleted model:\", model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80622011-15db-4d64-8218-fcea39c9552e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "#Impement:\n",
    "# Implement model monitors on your ML system.\n",
    "# Implement data monitors on your ML system.\n",
    "# Implement infrastructure monitors on your ML system.\n",
    "# Create a monitoring dashboard for your ML endpoint/job on CloudWatch.\n",
    "# Generate model and data reports on SageMaker.\n",
    "\n",
    "#0\n",
    "import time, json\n",
    "import boto3\n",
    "import sagemaker\n",
    "import awswrangler as wr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker.model import Model\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.session.Session().region_name\n",
    "sm = boto3.client(\"sagemaker\", region_name=region)\n",
    "cw = boto3.client(\"cloudwatch\", region_name=region)\n",
    "\n",
    "bucket = \"aai540-olist-mlops-chris-7f3k2p\"\n",
    "xgb_prefix = f\"s3://{bucket}/modeling/xgb_v1/\"\n",
    "\n",
    "# Training job from earlier\n",
    "training_job_name = \"sagemaker-xgboost-2026-01-31-18-00-53-460\"\n",
    "\n",
    "# Your feature list (must match training)\n",
    "FEATURES = [\n",
    "    \"num_items\",\"total_price\",\"total_freight_value\",\"num_sellers\",\n",
    "    \"payment_value\",\"payment_installments\",\"purchase_dow\",\"purchase_hour\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bf618d6-efc6-4b9c-be82-dbf034a77870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model artifact: s3://aai540-olist-mlops-chris-7f3k2p/modeling/xgb_v1/output/sagemaker-xgboost-2026-01-31-18-00-53-460/output/model.tar.gz\n",
      "✅ Created model: xgb-v1-monitor-1770585486\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "tj = sm.describe_training_job(TrainingJobName=training_job_name)\n",
    "model_data = tj[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "print(\"Model artifact:\", model_data)\n",
    "\n",
    "xgb_image = retrieve(framework=\"xgboost\", region=region, version=\"1.7-1\")\n",
    "\n",
    "model_name = f\"xgb-v1-monitor-{int(time.time())}\"\n",
    "model = Model(\n",
    "    image_uri=xgb_image,\n",
    "    model_data=model_data,\n",
    "    role=role,\n",
    "    sagemaker_session=sess,\n",
    "    name=model_name,\n",
    ")\n",
    "\n",
    "model.create(instance_type=\"ml.m5.large\")\n",
    "print(\"✅ Created model:\", model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6098aa9d-7856-463d-a2aa-a1ce178d0c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker sdk version: 2.245.0\n",
      "Transformer.transform signature:\n",
      " (self, data: Union[str, sagemaker.workflow.entities.PipelineVariable], data_type: Union[str, sagemaker.workflow.entities.PipelineVariable] = 'S3Prefix', content_type: Union[str, sagemaker.workflow.entities.PipelineVariable, NoneType] = None, compression_type: Union[str, sagemaker.workflow.entities.PipelineVariable, NoneType] = None, split_type: Union[str, sagemaker.workflow.entities.PipelineVariable, NoneType] = None, job_name: Optional[str] = None, input_filter: Union[str, sagemaker.workflow.entities.PipelineVariable, NoneType] = None, output_filter: Union[str, sagemaker.workflow.entities.PipelineVariable, NoneType] = None, join_source: Union[str, sagemaker.workflow.entities.PipelineVariable, NoneType] = None, experiment_config: Optional[Dict[str, str]] = None, model_client_config: Optional[Dict[str, Union[str, sagemaker.workflow.entities.PipelineVariable]]] = None, batch_data_capture_config: sagemaker.inputs.BatchDataCaptureConfig = None, wait: bool = True, logs: bool = True)\n"
     ]
    }
   ],
   "source": [
    "#2A\n",
    "import sagemaker, inspect\n",
    "from sagemaker.transformer import Transformer\n",
    "\n",
    "print(\"sagemaker sdk version:\", sagemaker.__version__)\n",
    "print(\"Transformer.transform signature:\\n\", inspect.signature(Transformer.transform))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "866e0ea7-413c-40ac-a676-1ea404585110",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating transform job with name: sagemaker-xgboost-2026-02-08-21-21-26-827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Starting batch transform WITH batch data capture...\n",
      ".................................\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m[2026-02-08:21:26:55:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-02-08:21:26:55:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-02-08:21:26:55:INFO] nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2026-02-08 21:26:55 +0000] [13] [INFO] Starting gunicorn 23.0.0\u001b[0m\n",
      "\u001b[34m[2026-02-08 21:26:55 +0000] [13] [INFO] Listening at: unix:/tmp/gunicorn.sock (13)\u001b[0m\n",
      "\u001b[34m[2026-02-08 21:26:55 +0000] [13] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2026-02-08 21:26:55 +0000] [16] [INFO] Booting worker with pid: 16\u001b[0m\n",
      "\u001b[34m[2026-02-08 21:26:55 +0000] [17] [INFO] Booting worker with pid: 17\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m[2026-02-08:21:26:58:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-02-08:21:26:58:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2026-02-08:21:26:58:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[34m[2026-02-08:21:26:58:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-02-08:21:26:58:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2026-02-08:21:26:58:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[34m[2026-02-08:21:27:05:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [08/Feb/2026:21:27:05 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2026-02-08:21:27:05:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [08/Feb/2026:21:27:05 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2026-02-08:21:27:05:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [08/Feb/2026:21:27:05 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2026-02-08:21:27:05:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [08/Feb/2026:21:27:05 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2026-02-08:21:27:05:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [08/Feb/2026:21:27:06 +0000] \"POST /invocations HTTP/1.1\" 200 798984 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2026-02-08:21:27:05:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [08/Feb/2026:21:27:06 +0000] \"POST /invocations HTTP/1.1\" 200 798984 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2026-02-08T21:27:05.496:[sagemaker logs]: MaxConcurrentTransforms=2, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\n",
      "✅ Transform complete\n",
      "Transform output: s3://aai540-olist-mlops-chris-7f3k2p/monitoring/batch-transform/output/1770585686/\n",
      "Captured data: s3://aai540-olist-mlops-chris-7f3k2p/monitoring/batch-transform/capture/1770585686/\n"
     ]
    }
   ],
   "source": [
    "# M5-2B Batch Transform with Batch Data Capture (SDK 2.245.0)\n",
    "\n",
    "import time\n",
    "from sagemaker.transformer import Transformer\n",
    "from sagemaker.inputs import BatchDataCaptureConfig\n",
    "\n",
    "transform_output = f\"s3://{bucket}/monitoring/batch-transform/output/{int(time.time())}/\"\n",
    "capture_uri = f\"s3://{bucket}/monitoring/batch-transform/capture/{int(time.time())}/\"\n",
    "\n",
    "batch_capture = BatchDataCaptureConfig(\n",
    "    destination_s3_uri=capture_uri,\n",
    "    generate_inference_id=True,\n",
    ")\n",
    "\n",
    "transformer = Transformer(\n",
    "    model_name=model_name,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    output_path=transform_output,\n",
    "    assemble_with=\"Line\",\n",
    "    accept=\"text/csv\",\n",
    ")\n",
    "\n",
    "print(\"⏳ Starting batch transform WITH batch data capture...\")\n",
    "transformer.transform(\n",
    "    data=prod_infer_uri,\n",
    "    content_type=\"text/csv\",\n",
    "    split_type=\"Line\",\n",
    "    batch_data_capture_config=batch_capture,  # <-- correct name for your SDK\n",
    "    wait=True,\n",
    "    logs=True,\n",
    ")\n",
    "\n",
    "print(\"✅ Transform complete\")\n",
    "print(\"Transform output:\", transform_output)\n",
    "print(\"Captured data:\", capture_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ea95b7f-6293-46da-bbcc-5cbd96769a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Baseline CSV: s3://aai540-olist-mlops-chris-7f3k2p/monitoring/baselines/data_quality/train_baseline.csv shape: (39776, 8)\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "# M5-3.0 Data quality baseline dataset (TRAIN features)\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "split_base = f\"s3://{bucket}/splits/olist/features/version=v1/\"\n",
    "train = wr.s3.read_parquet(f\"{split_base}train/\")\n",
    "\n",
    "train_baseline = train[FEATURES].copy()\n",
    "for c in FEATURES:\n",
    "    train_baseline[c] = pd.to_numeric(train_baseline[c], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "baseline_uri = f\"s3://{bucket}/monitoring/baselines/data_quality/train_baseline.csv\"\n",
    "wr.s3.to_csv(train_baseline, baseline_uri, index=False, header=False)\n",
    "\n",
    "print(\"✅ Baseline CSV:\", baseline_uri, \"shape:\", train_baseline.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "098a20c9-9cb5-4254-a550-1740a46889bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating processing-job with name baseline-suggestion-job-2026-02-08-21-32-41-870\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......................\u001b[34m2026-02-08 21:36:23.809849: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:23.809892: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:25.321307: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:25.321342: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:25.321368: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-2-105-159.ec2.internal): /proc/driver/nvidia/version does not exist\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:25.321660: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\u001b[0m\n",
      "\u001b[34mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:26,856 - __main__ - INFO - All params:{'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:758289042916:processing-job/baseline-suggestion-job-2026-02-08-21-32-41-870', 'ProcessingJobName': 'baseline-suggestion-job-2026-02-08-21-32-41-870', 'Environment': {'dataset_format': '{\"csv\": {\"header\": false, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}, 'AppSpecification': {'ImageUri': '156813124566.dkr.ecr.us-east-1.amazonaws.com/sagemaker-model-monitor-analyzer', 'ContainerEntrypoint': None, 'ContainerArguments': None}, 'ProcessingInputs': [{'InputName': 'baseline_dataset_input', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3Uri': 's3://aai540-olist-mlops-chris-7f3k2p/monitoring/baselines/data_quality/train_baseline.csv', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinitionInput': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'monitoring_output', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/output', 'S3Uri': 's3://aai540-olist-mlops-chris-7f3k2p/monitoring/baselines/data_quality/results/', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.m5.large', 'VolumeSizeInGB': 20, 'VolumeKmsKeyId': None}}, 'NetworkConfig': {'VpcConfig': None, 'EnableNetworkIsolation': False, 'EnableInterContainerTrafficEncryption': False}, 'RoleArn': 'arn:aws:iam::758289042916:role/LabRole', 'StoppingCondition': {'MaxRuntimeInSeconds': 3600}}\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:26,856 - __main__ - INFO - Current Environment:{'dataset_format': '{\"csv\": {\"header\": false, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:26,856 - __main__ - INFO - categorical_drift_method:None\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:26,856 - DefaultDataAnalyzer - INFO - Performing analysis with input: {\"dataset_source\": \"/opt/ml/processing/input/baseline_dataset_input\", \"dataset_format\": {\"csv\": {\"header\": false, \"output_columns_position\": \"START\"}}, \"output_path\": \"/opt/ml/processing/output\", \"monitoring_input_type\": null, \"analysis_type\": null, \"problem_type\": null, \"inference_attribute\": null, \"probability_attribute\": null, \"ground_truth_attribute\": null, \"probability_threshold_attribute\": null, \"positive_label\": null, \"exclude_features_attribute\": null, \"record_preprocessor_script\": null, \"post_analytics_processor_script\": null, \"baseline_constraints\": null, \"baseline_statistics\": null, \"data_quality_monitoring_config\": {\"evaluate_constraints\": \"Enabled\", \"emit_metrics\": \"Enabled\", \"datatype_check_threshold\": 1.0, \"domain_content_threshold\": 1.0, \"distribution_constraints\": {\"perform_comparison\": \"Enabled\", \"comparison_threshold\": 0.1, \"comparison_method\": \"Robust\", \"categorical_comparison_threshold\": 0.1, \"categorical_drift_method\": \"LInfinity\"}}, \"start_time\": null, \"end_time\": null, \"metric_time\": null, \"cloudwatch_metrics_directory\": \"/opt/ml/output/metrics/cloudwatch\", \"publish_cloudwatch_metrics\": \"Disabled\", \"sagemaker_endpoint_name\": null, \"sagemaker_monitoring_schedule_name\": null, \"output_message_file\": \"/opt/ml/output/message\", \"detect_outliers\": null, \"detect_drift\": null, \"image_data\": null, \"report_enabled\": false, \"auto_ml_job_detail\": null}\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:26,857 - DefaultDataAnalyzer - INFO - Bootstrapping yarn\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:26,857 - bootstrap - INFO - Copy aws jars\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:27,137 - bootstrap - INFO - Copy cluster config\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:27,138 - bootstrap - INFO - Write runtime cluster config\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:27,138 - bootstrap - INFO - Resource Config is: {'current_host': 'algo-1', 'current_instance_type': 'ml.m5.large', 'current_group_name': 'homogeneousCluster', 'hosts': ['algo-1'], 'instance_groups': [{'instance_group_name': 'homogeneousCluster', 'instance_type': 'ml.m5.large', 'hosts': ['algo-1']}], 'network_interface_name': 'eth0', 'topology': None}\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:27,180 - bootstrap - INFO - Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:27,180 - bootstrap - INFO - Starting spark process for master node algo-1\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:27,180 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs namenode -format -force\u001b[0m\n",
      "\u001b[34mWARNING: /usr/hadoop-3.0.0/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:28,599 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.2.105.159\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.0.0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.0.0/etc/hadoop:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/junit-4.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.199.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-aws-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-kms-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/yarn:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-2.2.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/servlet-api-2.5-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-api-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/htrace-core-3.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-el-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-common-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-annotations-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/joni-2.1.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jcodings-1.0.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/disruptor-3.3.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop2-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-runtime-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-protocol-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/findbugs-annotations-1.3.9-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-server-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jamon-runtime-2.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-procedure-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-csv-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-client-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-prefix-tree-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-compiler-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-registry-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-se\u001b[0m\n",
      "\u001b[34mrver-router-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-api-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.0.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533; compiled by 'andrew' on 2017-12-08T19:16Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_462\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:28,616 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:28,621 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-7e73c957-772b-4f35-a870-1ed6d9c21150\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,542 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,565 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,566 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,570 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,586 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,586 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,586 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,587 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,647 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,668 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,668 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,671 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,674 INFO blockmanagement.BlockManager: The block deletion will start around 2026 Feb 08 21:36:29\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,676 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,676 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,679 INFO util.GSet: 2.0% max memory 1.4 GB = 28.3 MB\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,679 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,733 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,736 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,736 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,737 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,737 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,737 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,737 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,737 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,737 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,737 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,737 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,737 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,776 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,776 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,777 INFO util.GSet: 1.0% max memory 1.4 GB = 14.2 MB\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,777 INFO util.GSet: capacity      = 2^21 = 2097152 entries\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,778 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,778 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,778 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,778 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,783 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,787 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,787 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,788 INFO util.GSet: 0.25% max memory 1.4 GB = 3.5 MB\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,788 INFO util.GSet: capacity      = 2^19 = 524288 entries\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,796 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,796 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,796 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,799 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,800 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,802 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,802 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,803 INFO util.GSet: 0.029999999329447746% max memory 1.4 GB = 435.3 KB\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,803 INFO util.GSet: capacity      = 2^16 = 65536 entries\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,833 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1472862815-10.2.105.159-1770586589823\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,853 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,869 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,976 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 389 bytes saved in 0 seconds.\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:29,996 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:30,002 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.2.105.159\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:30,012 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:32,096 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode, return code 1\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:32,096 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:34,253 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode, return code 1\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:34,253 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:36,619 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager, return code 1\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:36,620 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:39,096 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager, return code 1\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:39,097 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:41,524 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver, return code 1\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:41,525 - DefaultDataAnalyzer - INFO - Total number of hosts in the cluster: 1\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:51,533 - DefaultDataAnalyzer - INFO - Running command: bin/spark-submit --master yarn --deploy-mode client --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.serializer=org.apache.spark.serializer.KryoSerializer /opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:54,430 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:55,089 INFO Main: Start analyzing with args: --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:55,131 INFO Main: Analytics input path: DataAnalyzerParams(/tmp/spark_job_config.json,yarn)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:55,150 INFO FileUtil: Read file from path /tmp/spark_job_config.json.\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:55,887 INFO spark.SparkContext: Running Spark version 3.3.0\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:55,916 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:55,916 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:55,917 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:55,917 INFO spark.SparkContext: Submitted application: SageMakerDataAnalyzer\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:55,960 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 5673, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:55,974 INFO resource.ResourceProfile: Limiting resource is cpus at 1 tasks per executor\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:55,976 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:56,063 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:56,063 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:56,064 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:56,064 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:56,064 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:56,629 INFO util.Utils: Successfully started service 'sparkDriver' on port 41271.\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:56,693 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:56,763 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:56,795 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:56,796 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:56,849 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:56,901 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-d02cd5fc-3c73-48f0-9932-f1bde97807ee\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:56,926 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MiB\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:56,990 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:57,042 INFO spark.SparkContext: Added JAR file:/opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar at spark://10.2.105.159:41271/jars/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar with timestamp 1770586615881\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:57,755 INFO client.RMProxy: Connecting to ResourceManager at /10.2.105.159:8032\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:58,532 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:58,533 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:58,540 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (7736 MB per container)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:58,540 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:58,541 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:58,541 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:58,551 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2026-02-08 21:36:58,643 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:00,806 INFO yarn.Client: Uploading resource file:/tmp/spark-b664905d-0fa7-4149-a030-7adbf5ecd929/__spark_libs__7616539710974437573.zip -> hdfs://10.2.105.159/user/root/.sparkStaging/application_1770586597676_0001/__spark_libs__7616539710974437573.zip\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:03,050 INFO yarn.Client: Uploading resource file:/tmp/spark-b664905d-0fa7-4149-a030-7adbf5ecd929/__spark_conf__2915181524241141727.zip -> hdfs://10.2.105.159/user/root/.sparkStaging/application_1770586597676_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:03,103 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:03,103 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:03,103 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:03,104 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:03,104 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:03,132 INFO yarn.Client: Submitting application application_1770586597676_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:03,525 INFO impl.YarnClientImpl: Submitted application application_1770586597676_0001\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:04,534 INFO yarn.Client: Application report for application_1770586597676_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:04,542 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: [Sun Feb 08 21:37:03 +0000 2026] Scheduler has assigned a container for AM, waiting for AM container to be launched\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1770586623229\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1770586597676_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:05,548 INFO yarn.Client: Application report for application_1770586597676_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:06,552 INFO yarn.Client: Application report for application_1770586597676_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:07,563 INFO yarn.Client: Application report for application_1770586597676_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:08,568 INFO yarn.Client: Application report for application_1770586597676_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:09,574 INFO yarn.Client: Application report for application_1770586597676_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:10,577 INFO yarn.Client: Application report for application_1770586597676_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:10,578 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.2.105.159\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1770586623229\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1770586597676_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:10,580 INFO cluster.YarnClientSchedulerBackend: Application application_1770586597676_0001 has started running.\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:10,623 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38475.\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:10,624 INFO netty.NettyBlockTransferService: Server created on 10.2.105.159:38475\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:10,626 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:10,634 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.2.105.159, 38475, None)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:10,656 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.2.105.159:38475 with 1458.6 MiB RAM, BlockManagerId(driver, 10.2.105.159, 38475, None)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:10,659 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.2.105.159, 38475, None)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:10,662 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.2.105.159, 38475, None)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:10,901 INFO util.log: Logging initialized @19138ms to org.sparkproject.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:11,271 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1770586597676_0001), /proxy/application_1770586597676_0001\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:13,111 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:18,160 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.2.105.159:41044) with ID 1,  ResourceProfileId 0\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:18,581 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:42739 with 2.8 GiB RAM, BlockManagerId(1, algo-1, 42739, None)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:27,608 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:27,981 WARN spark.SparkContext: Spark is not running in local mode, therefore the checkpoint directory must not be on the local filesystem. Directory '/tmp' appears to be on the local filesystem.\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:28,081 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:28,091 INFO internal.SharedState: Warehouse path is 'file:/usr/spark-3.3.0/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:29,874 INFO datasources.InMemoryFileIndex: It took 83 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:30,207 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 416.9 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:30,710 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 39.2 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:30,714 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.2.105.159:38475 (size: 39.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:30,731 INFO spark.SparkContext: Created broadcast 0 from csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:31,328 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:31,331 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:31,340 INFO input.CombineFileInputFormat: DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 1154874\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:31,435 INFO spark.SparkContext: Starting job: csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:31,455 INFO scheduler.DAGScheduler: Got job 0 (csv at DatasetReader.scala:99) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:31,456 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (csv at DatasetReader.scala:99)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:31,457 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:31,459 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:31,467 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:31,567 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.3 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:31,570 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:31,571 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.2.105.159:38475 (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:31,572 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:31,595 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:31,597 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:31,656 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4627 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:32,011 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-1:42739 (size: 4.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:33,200 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-1:42739 (size: 39.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:33,596 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1956 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:33,598 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:33,604 INFO scheduler.DAGScheduler: ResultStage 0 (csv at DatasetReader.scala:99) finished in 2.080 s\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:33,773 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:33,774 INFO cluster.YarnScheduler: Killing all running tasks in stage 0: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:33,781 INFO scheduler.DAGScheduler: Job 0 finished: csv at DatasetReader.scala:99, took 2.346015 s\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:36,817 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:36,819 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:36,822 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: string, _c1: string, _c2: string, _c3: string, _c4: string ... 6 more fields>\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:37,061 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 416.5 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:37,082 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 39.1 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:37,094 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.2.105.159:38475 (size: 39.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:37,097 INFO spark.SparkContext: Created broadcast 2 from head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:37,116 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:37,165 INFO spark.SparkContext: Starting job: head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:37,167 INFO scheduler.DAGScheduler: Got job 1 (head at DataAnalyzer.scala:124) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:37,167 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (head at DataAnalyzer.scala:124)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:37,168 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:37,171 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:37,178 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:37,269 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 16.1 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:37,275 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:37,276 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.2.105.159:38475 (size: 7.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:37,278 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:37,279 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:37,279 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:37,283 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4955 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:37,376 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-1:42739 (size: 7.7 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:38,598 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-1:42739 (size: 39.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:39,542 INFO storage.BlockManagerInfo: Added rdd_7_0 in memory on algo-1:42739 (size: 829.5 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:39,763 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2482 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:39,764 INFO scheduler.DAGScheduler: ResultStage 1 (head at DataAnalyzer.scala:124) finished in 2.577 s\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:39,765 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:39,766 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:39,766 INFO cluster.YarnScheduler: Killing all running tasks in stage 1: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:39,767 INFO scheduler.DAGScheduler: Job 1 finished: head at DataAnalyzer.scala:124, took 2.601495 s\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:40,125 INFO codegen.CodeGenerator: Code generated in 241.570744 ms\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:40,728 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:40,910 INFO scheduler.DAGScheduler: Registering RDD 16 (collect at AnalysisRunner.scala:326) as input to shuffle 0\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:40,916 INFO scheduler.DAGScheduler: Got map stage job 2 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:40,916 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:40,917 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:40,921 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:40,927 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:40,959 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 113.1 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:40,963 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:40,965 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.2.105.159:38475 (size: 34.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:40,966 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:40,973 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:40,973 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:40,982 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4944 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:41,025 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-1:42739 (size: 34.7 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:43,833 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 2853 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:43,836 INFO scheduler.DAGScheduler: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326) finished in 2.904 s\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:43,841 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:43,845 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:43,846 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:43,847 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:43,846 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:43,933 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:43,936 INFO scheduler.DAGScheduler: Got job 3 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:43,936 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:43,936 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:43,936 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:43,937 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:43,966 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 165.6 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:43,972 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 45.7 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:43,973 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.2.105.159:38475 (size: 45.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:43,974 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:43,977 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:43,977 INFO cluster.YarnScheduler: Adding task set 4.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:43,980 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:44,014 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-1:42739 (size: 45.7 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:44,076 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.2.105.159:41044\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:44,463 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 484 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:44,465 INFO scheduler.DAGScheduler: ResultStage 4 (collect at AnalysisRunner.scala:326) finished in 0.514 s\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:44,465 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:44,466 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:44,466 INFO cluster.YarnScheduler: Killing all running tasks in stage 4: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:44,466 INFO scheduler.DAGScheduler: Job 3 finished: collect at AnalysisRunner.scala:326, took 0.533038 s\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:44,519 INFO codegen.CodeGenerator: Code generated in 42.673163 ms\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:44,943 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.2.105.159:38475 in memory (size: 34.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:44,948 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-1:42739 in memory (size: 34.7 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:44,984 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.2.105.159:38475 in memory (size: 45.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:44,989 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-1:42739 in memory (size: 45.7 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:45,028 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-1:42739 in memory (size: 7.7 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:45,030 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.2.105.159:38475 in memory (size: 7.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:45,039 INFO codegen.CodeGenerator: Code generated in 144.883031 ms\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:45,137 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:45,139 INFO scheduler.DAGScheduler: Got job 4 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:45,141 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:45,141 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:45,142 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:45,144 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:45,169 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 36.9 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:45,176 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 16.1 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:45,177 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.2.105.159:38475 (size: 16.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:45,179 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:45,180 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:45,182 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:45,184 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4955 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:45,204 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-1:42739 (size: 16.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:47,406 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 2222 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:47,407 INFO scheduler.DAGScheduler: ResultStage 5 (treeReduce at KLLRunner.scala:107) finished in 2.261 s\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:47,408 INFO scheduler.DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:47,409 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:47,409 INFO cluster.YarnScheduler: Killing all running tasks in stage 5: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:47,410 INFO scheduler.DAGScheduler: Job 4 finished: treeReduce at KLLRunner.scala:107, took 2.272103 s\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:48,175 INFO codegen.CodeGenerator: Code generated in 130.11189 ms\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:48,184 INFO scheduler.DAGScheduler: Registering RDD 34 (collect at AnalysisRunner.scala:326) as input to shuffle 1\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:48,184 INFO scheduler.DAGScheduler: Got map stage job 5 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:48,184 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:48,184 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:48,185 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:48,186 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:48,195 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 73.3 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:48,197 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 23.7 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:48,198 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.2.105.159:38475 (size: 23.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:48,199 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:48,200 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:48,200 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:48,201 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4944 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:48,216 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-1:42739 (size: 23.7 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:48,561 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 360 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:48,562 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326) finished in 0.373 s\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:48,563 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:48,564 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:48,564 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:48,564 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:48,564 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:48,815 INFO codegen.CodeGenerator: Code generated in 153.145394 ms\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:48,828 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:48,829 INFO scheduler.DAGScheduler: Got job 6 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:48,829 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:48,829 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:48,829 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:48,830 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:48,833 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 66.8 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:48,835 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 19.6 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:48,836 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.2.105.159:38475 (size: 19.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:48,836 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:48,837 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:48,837 INFO cluster.YarnScheduler: Adding task set 8.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:48,838 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:48,857 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-1:42739 (size: 19.6 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:48,876 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.2.105.159:41044\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:49,006 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 168 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:49,012 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:49,012 INFO scheduler.DAGScheduler: ResultStage 8 (collect at AnalysisRunner.scala:326) finished in 0.181 s\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:49,013 INFO scheduler.DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:49,013 INFO cluster.YarnScheduler: Killing all running tasks in stage 8: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:49,013 INFO scheduler.DAGScheduler: Job 6 finished: collect at AnalysisRunner.scala:326, took 0.185220 s\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:49,160 INFO codegen.CodeGenerator: Code generated in 111.686518 ms\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:49,303 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:49,313 INFO scheduler.DAGScheduler: Registering RDD 45 (countByKey at ColumnProfiler.scala:592) as input to shuffle 2\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:49,314 INFO scheduler.DAGScheduler: Got job 7 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:49,314 INFO scheduler.DAGScheduler: Final stage: ResultStage 10 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:49,314 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:49,314 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:49,320 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:49,344 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 29.3 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:49,348 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 13.6 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:49,352 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.2.105.159:38475 (size: 13.6 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:49,352 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:49,353 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:49,353 INFO cluster.YarnScheduler: Adding task set 9.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:49,354 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4944 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:49,373 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-1:42739 (size: 13.6 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:51,668 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 2314 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:51,668 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:51,669 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (countByKey at ColumnProfiler.scala:592) finished in 2.347 s\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:51,671 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:51,671 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:51,672 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 10)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:51,672 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:51,672 INFO scheduler.DAGScheduler: Submitting ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:51,675 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 5.1 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:51,678 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:51,682 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.2.105.159:38475 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:51,682 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:51,687 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:51,688 INFO cluster.YarnScheduler: Adding task set 10.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:51,690 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 8) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:51,708 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-1:42739 (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:51,719 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.2.105.159:41044\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:51,758 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 8) in 69 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:51,759 INFO cluster.YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:51,760 INFO scheduler.DAGScheduler: ResultStage 10 (countByKey at ColumnProfiler.scala:592) finished in 0.087 s\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:51,763 INFO scheduler.DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:51,764 INFO cluster.YarnScheduler: Killing all running tasks in stage 10: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:51,764 INFO scheduler.DAGScheduler: Job 7 finished: countByKey at ColumnProfiler.scala:592, took 2.460734 s\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,004 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.2.105.159:38475 in memory (size: 23.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,007 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-1:42739 in memory (size: 23.7 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,029 INFO scheduler.DAGScheduler: Registering RDD 51 (collect at AnalysisRunner.scala:326) as input to shuffle 3\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,031 INFO scheduler.DAGScheduler: Got map stage job 8 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,032 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,032 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,033 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,040 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,067 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 61.6 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,071 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 22.0 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,073 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.2.105.159:38475 (size: 22.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,074 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,074 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,076 INFO cluster.YarnScheduler: Adding task set 11.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,077 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 9) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4944 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,105 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-1:42739 (size: 22.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,127 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.2.105.159:38475 in memory (size: 19.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,128 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-1:42739 in memory (size: 19.6 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,178 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.2.105.159:38475 in memory (size: 16.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,197 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-1:42739 in memory (size: 16.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,245 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.2.105.159:38475 in memory (size: 13.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,249 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-1:42739 in memory (size: 13.6 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,311 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.2.105.159:38475 in memory (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,316 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-1:42739 in memory (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,667 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 9) in 589 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,667 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,668 INFO scheduler.DAGScheduler: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326) finished in 0.626 s\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,669 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,669 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,669 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,669 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,748 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,750 INFO scheduler.DAGScheduler: Got job 9 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,751 INFO scheduler.DAGScheduler: Final stage: ResultStage 13 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,751 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,752 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,757 INFO scheduler.DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,762 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 113.9 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,765 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,771 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.2.105.159:38475 (size: 34.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,772 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,773 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,773 INFO cluster.YarnScheduler: Adding task set 13.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,774 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 10) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,786 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-1:42739 (size: 34.6 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,803 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 10.2.105.159:41044\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:52,999 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 10) in 225 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:53,000 INFO cluster.YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:53,001 INFO scheduler.DAGScheduler: ResultStage 13 (collect at AnalysisRunner.scala:326) finished in 0.243 s\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:53,001 INFO scheduler.DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:53,001 INFO cluster.YarnScheduler: Killing all running tasks in stage 13: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:53,002 INFO scheduler.DAGScheduler: Job 9 finished: collect at AnalysisRunner.scala:326, took 0.253824 s\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:53,045 INFO codegen.CodeGenerator: Code generated in 32.621902 ms\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:53,256 INFO codegen.CodeGenerator: Code generated in 31.470579 ms\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:53,302 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:53,304 INFO scheduler.DAGScheduler: Got job 10 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:53,304 INFO scheduler.DAGScheduler: Final stage: ResultStage 14 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:53,304 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:53,305 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:53,307 INFO scheduler.DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:53,315 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 34.5 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:53,320 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 15.3 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:53,323 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.2.105.159:38475 (size: 15.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:53,324 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:53,325 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:53,325 INFO cluster.YarnScheduler: Adding task set 14.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:53,326 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 11) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4955 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:53,339 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-1:42739 (size: 15.3 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:54,366 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 11) in 1040 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:54,377 INFO scheduler.DAGScheduler: ResultStage 14 (treeReduce at KLLRunner.scala:107) finished in 1.069 s\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:54,378 INFO scheduler.DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:54,378 INFO cluster.YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:54,379 INFO cluster.YarnScheduler: Killing all running tasks in stage 14: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:54,379 INFO scheduler.DAGScheduler: Job 10 finished: treeReduce at KLLRunner.scala:107, took 1.076840 s\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:54,758 INFO codegen.CodeGenerator: Code generated in 81.656608 ms\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:54,773 INFO scheduler.DAGScheduler: Registering RDD 69 (collect at AnalysisRunner.scala:326) as input to shuffle 4\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:54,773 INFO scheduler.DAGScheduler: Got map stage job 11 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:54,773 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:54,773 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:54,774 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:54,775 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:54,779 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 53.1 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:54,781 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 18.8 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:54,782 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.2.105.159:38475 (size: 18.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:54,782 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:54,783 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:54,783 INFO cluster.YarnScheduler: Adding task set 15.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:54,786 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 12) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4944 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:54,800 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-1:42739 (size: 18.8 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,032 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 12) in 246 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,033 INFO cluster.YarnScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,034 INFO scheduler.DAGScheduler: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326) finished in 0.258 s\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,035 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,035 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,035 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,036 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,169 INFO codegen.CodeGenerator: Code generated in 60.502004 ms\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,181 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,182 INFO scheduler.DAGScheduler: Got job 12 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,182 INFO scheduler.DAGScheduler: Final stage: ResultStage 17 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,183 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,183 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,184 INFO scheduler.DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,187 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 44.7 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,189 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 14.3 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,190 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.2.105.159:38475 (size: 14.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,190 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,191 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,192 INFO cluster.YarnScheduler: Adding task set 17.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,193 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 13) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,207 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-1:42739 (size: 14.3 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,218 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.2.105.159:41044\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,301 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 13) in 108 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,301 INFO cluster.YarnScheduler: Removed TaskSet 17.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,302 INFO scheduler.DAGScheduler: ResultStage 17 (collect at AnalysisRunner.scala:326) finished in 0.116 s\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,303 INFO scheduler.DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,303 INFO cluster.YarnScheduler: Killing all running tasks in stage 17: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,304 INFO scheduler.DAGScheduler: Job 12 finished: collect at AnalysisRunner.scala:326, took 0.122865 s\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,362 INFO codegen.CodeGenerator: Code generated in 48.381336 ms\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,460 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,462 INFO scheduler.DAGScheduler: Registering RDD 80 (countByKey at ColumnProfiler.scala:592) as input to shuffle 5\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,462 INFO scheduler.DAGScheduler: Got job 13 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,466 INFO scheduler.DAGScheduler: Final stage: ResultStage 19 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,466 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,467 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,469 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,473 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 29.2 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,475 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 13.6 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,476 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.2.105.159:38475 (size: 13.6 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,476 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,477 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,483 INFO cluster.YarnScheduler: Adding task set 18.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,485 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 14) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4944 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,500 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-1:42739 (size: 13.6 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,806 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 14) in 320 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,806 INFO cluster.YarnScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,807 INFO scheduler.DAGScheduler: ShuffleMapStage 18 (countByKey at ColumnProfiler.scala:592) finished in 0.337 s\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,808 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,808 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,808 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 19)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,808 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,808 INFO scheduler.DAGScheduler: Submitting ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,810 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 5.1 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,812 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,812 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.2.105.159:38475 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,813 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,814 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,815 INFO cluster.YarnScheduler: Adding task set 19.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,817 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 19.0 (TID 15) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,837 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-1:42739 (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,841 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 10.2.105.159:41044\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,862 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 19.0 (TID 15) in 46 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,862 INFO cluster.YarnScheduler: Removed TaskSet 19.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,863 INFO scheduler.DAGScheduler: ResultStage 19 (countByKey at ColumnProfiler.scala:592) finished in 0.054 s\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,863 INFO scheduler.DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,863 INFO cluster.YarnScheduler: Killing all running tasks in stage 19: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:55,864 INFO scheduler.DAGScheduler: Job 13 finished: countByKey at ColumnProfiler.scala:592, took 0.402940 s\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,067 INFO FileUtil: Write to file constraints.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,118 INFO codegen.CodeGenerator: Code generated in 8.631002 ms\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,124 INFO scheduler.DAGScheduler: Registering RDD 86 (count at StatsGenerator.scala:66) as input to shuffle 6\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,124 INFO scheduler.DAGScheduler: Got map stage job 14 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,125 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 20 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,125 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,129 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,129 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[86] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,133 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 21.3 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,135 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 10.0 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,136 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.2.105.159:38475 (size: 10.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,137 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,137 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[86] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,137 INFO cluster.YarnScheduler: Adding task set 20.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,139 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 16) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4944 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,151 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-1:42739 (size: 10.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,222 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 16) in 83 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,222 INFO cluster.YarnScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,223 INFO scheduler.DAGScheduler: ShuffleMapStage 20 (count at StatsGenerator.scala:66) finished in 0.092 s\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,224 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,224 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,225 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,225 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,268 INFO codegen.CodeGenerator: Code generated in 25.853469 ms\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,284 INFO spark.SparkContext: Starting job: count at StatsGenerator.scala:66\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,285 INFO scheduler.DAGScheduler: Got job 15 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,286 INFO scheduler.DAGScheduler: Final stage: ResultStage 22 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,287 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,287 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,287 INFO scheduler.DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[89] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,292 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 11.1 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,294 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,296 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.2.105.159:38475 (size: 5.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,297 INFO spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,297 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[89] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,297 INFO cluster.YarnScheduler: Adding task set 22.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,299 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 22.0 (TID 17) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,311 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-1:42739 (size: 5.5 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,318 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.2.105.159:41044\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,337 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 22.0 (TID 17) in 39 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,337 INFO cluster.YarnScheduler: Removed TaskSet 22.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,338 INFO scheduler.DAGScheduler: ResultStage 22 (count at StatsGenerator.scala:66) finished in 0.047 s\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,338 INFO scheduler.DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,339 INFO cluster.YarnScheduler: Killing all running tasks in stage 22: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,339 INFO scheduler.DAGScheduler: Job 15 finished: count at StatsGenerator.scala:66, took 0.054787 s\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,675 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-1:42739 in memory (size: 34.6 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,688 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.2.105.159:38475 in memory (size: 34.6 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,730 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.2.105.159:38475 in memory (size: 18.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,738 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on algo-1:42739 in memory (size: 18.8 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,755 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on 10.2.105.159:38475 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,757 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on algo-1:42739 in memory (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,791 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on algo-1:42739 in memory (size: 13.6 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,792 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.2.105.159:38475 in memory (size: 13.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,842 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.2.105.159:38475 in memory (size: 22.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,844 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-1:42739 in memory (size: 22.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,872 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.2.105.159:38475 in memory (size: 14.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,879 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on algo-1:42739 in memory (size: 14.3 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,902 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on algo-1:42739 in memory (size: 5.5 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,902 INFO FileUtil: Write to file statistics.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,911 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on 10.2.105.159:38475 in memory (size: 5.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,947 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,972 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,974 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:56,987 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:57,015 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:57,065 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:57,066 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:57,070 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:57,088 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:57,116 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:57,116 INFO Main: Completed: Job completed successfully with no violations.\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:57,116 INFO Main: Write to file /opt/ml/output/message.\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:57,143 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:57,144 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-d2124a9f-39c4-404b-8a73-63d36d02e2be\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:57,168 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-b664905d-0fa7-4149-a030-7adbf5ecd929\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:57,335 - DefaultDataAnalyzer - INFO - Completed spark-submit with return code : 0\u001b[0m\n",
      "\u001b[34m2026-02-08 21:37:57,336 - DefaultDataAnalyzer - INFO - Spark job completed.\u001b[0m\n",
      "\n",
      "✅ Baseline results in: s3://aai540-olist-mlops-chris-7f3k2p/monitoring/baselines/data_quality/results/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000\">╭─────────────────────────────── </span><span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">Traceback </span><span style=\"color: #ff7f7f; text-decoration-color: #ff7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> in &lt;module&gt;:21                                                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">18 </span>)                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">19 </span>                                                                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">20 </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"✅ Baseline results in:\"</span>, baseline_results_uri)                                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>21 <span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"Constraints:\"</span>, <span style=\"font-weight: bold; text-decoration: underline\">monitor.baseline_constraints</span>())                                       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">22 </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"Statistics:\"</span>, monitor.baseline_statistics())                                         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">23 </span>                                                                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">AttributeError: </span><span style=\"color: #008700; text-decoration-color: #008700\">'DefaultModelMonitor'</span> object has no attribute <span style=\"color: #008700; text-decoration-color: #008700\">'baseline_constraints'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;0;0m╭─\u001b[0m\u001b[38;2;255;0;0m──────────────────────────────\u001b[0m\u001b[38;2;255;0;0m \u001b[0m\u001b[1;38;2;255;0;0mTraceback \u001b[0m\u001b[1;2;38;2;255;0;0m(most recent call last)\u001b[0m\u001b[38;2;255;0;0m \u001b[0m\u001b[38;2;255;0;0m───────────────────────────────\u001b[0m\u001b[38;2;255;0;0m─╮\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m in <module>:21                                                                                   \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m18 \u001b[0m)                                                                                           \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m19 \u001b[0m                                                                                            \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m20 \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33m✅ Baseline results in:\u001b[0m\u001b[33m\"\u001b[0m, baseline_results_uri)                                      \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m21 \u001b[96mprint\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mConstraints:\u001b[0m\u001b[33m\"\u001b[0m, \u001b[1;4mmonitor.baseline_constraints\u001b[0m())                                       \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m22 \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mStatistics:\u001b[0m\u001b[33m\"\u001b[0m, monitor.baseline_statistics())                                         \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m23 \u001b[0m                                                                                            \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mAttributeError: \u001b[0m\u001b[38;2;0;135;0m'DefaultModelMonitor'\u001b[0m object has no attribute \u001b[38;2;0;135;0m'baseline_constraints'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# M5-3.1 Suggest baseline (stats + constraints)\n",
    "# NOTE (Feb 2026):\n",
    "# This cell follows older SageMaker examples that reference\n",
    "# `monitor.baseline_constraints()` and `monitor.baseline_statistics()`.\n",
    "# These methods are NOT available in SageMaker SDK v2.245.0.\n",
    "# \n",
    "# The baseline IS created successfully, but artifacts must be\n",
    "# retrieved directly from S3 instead.\n",
    "# \n",
    "# See M5-3.1b below for the correct, SDK-safe implementation.\n",
    "monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",  # budget-friendly\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    "    sagemaker_session=sess,\n",
    ")\n",
    "\n",
    "baseline_results_uri = f\"s3://{bucket}/monitoring/baselines/data_quality/results/\"\n",
    "monitor.suggest_baseline(\n",
    "    baseline_dataset=baseline_uri,\n",
    "    dataset_format=DatasetFormat.csv(header=False),\n",
    "    output_s3_uri=baseline_results_uri,\n",
    "    wait=True,\n",
    ")\n",
    "\n",
    "print(\"✅ Baseline results in:\", baseline_results_uri)\n",
    "# print(\"Constraints:\", monitor.baseline_constraints())\n",
    "# print(\"Statistics:\", monitor.baseline_statistics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c7114eb-658c-44aa-9f89-b2f1c06c207b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found objects: 2\n",
      "monitoring/baselines/data_quality/results/constraints.json\n",
      "monitoring/baselines/data_quality/results/statistics.json\n",
      "\n",
      "statistics.json: monitoring/baselines/data_quality/results/statistics.json\n",
      "constraints.json: monitoring/baselines/data_quality/results/constraints.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('s3://aai540-olist-mlops-chris-7f3k2p/monitoring/baselines/data_quality/results/statistics.json',\n",
       " 's3://aai540-olist-mlops-chris-7f3k2p/monitoring/baselines/data_quality/results/constraints.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# M5-3.1b Find baseline statistics + constraints files in S3\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.client(\"s3\", region_name=region)\n",
    "\n",
    "baseline_prefix = baseline_results_uri.replace(f\"s3://{bucket}/\", \"\")\n",
    "resp = s3.list_objects_v2(Bucket=bucket, Prefix=baseline_prefix)\n",
    "\n",
    "keys = [o[\"Key\"] for o in resp.get(\"Contents\", [])]\n",
    "print(\"Found objects:\", len(keys))\n",
    "for k in keys[:50]:\n",
    "    print(k)\n",
    "\n",
    "# Try to auto-detect the files we need\n",
    "stats = [k for k in keys if k.endswith(\"statistics.json\")]\n",
    "constraints = [k for k in keys if k.endswith(\"constraints.json\")]\n",
    "\n",
    "print(\"\\nstatistics.json:\", stats[-1] if stats else \"NOT FOUND\")\n",
    "print(\"constraints.json:\", constraints[-1] if constraints else \"NOT FOUND\")\n",
    "\n",
    "baseline_statistics_uri = f\"s3://{bucket}/{stats[-1]}\" if stats else None\n",
    "baseline_constraints_uri = f\"s3://{bucket}/{constraints[-1]}\" if constraints else None\n",
    "\n",
    "baseline_statistics_uri, baseline_constraints_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eaa9ca3d-52be-4125-a38b-95dd994fa4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Production inference CSV: s3://aai540-olist-mlops-chris-7f3k2p/modeling/xgb_v1/prod/prod_infer.csv\n",
      "✅ prod_infer rows/cols: (39777, 8)\n",
      "Columns: ['num_items', 'total_price', 'total_freight_value', 'num_sellers', 'payment_value', 'payment_installments', 'purchase_dow', 'purchase_hour']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_items</th>\n",
       "      <th>total_price</th>\n",
       "      <th>total_freight_value</th>\n",
       "      <th>num_sellers</th>\n",
       "      <th>payment_value</th>\n",
       "      <th>payment_installments</th>\n",
       "      <th>purchase_dow</th>\n",
       "      <th>purchase_hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>243.90</td>\n",
       "      <td>33.71</td>\n",
       "      <td>2</td>\n",
       "      <td>277.61</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>86.90</td>\n",
       "      <td>13.63</td>\n",
       "      <td>1</td>\n",
       "      <td>100.53</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>159.90</td>\n",
       "      <td>14.14</td>\n",
       "      <td>1</td>\n",
       "      <td>174.04</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>19.90</td>\n",
       "      <td>12.48</td>\n",
       "      <td>1</td>\n",
       "      <td>32.38</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>149.00</td>\n",
       "      <td>45.12</td>\n",
       "      <td>1</td>\n",
       "      <td>194.12</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>35.15</td>\n",
       "      <td>15.10</td>\n",
       "      <td>1</td>\n",
       "      <td>50.25</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>27.90</td>\n",
       "      <td>8.27</td>\n",
       "      <td>1</td>\n",
       "      <td>36.17</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>15.30</td>\n",
       "      <td>11.85</td>\n",
       "      <td>1</td>\n",
       "      <td>27.15</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>125.00</td>\n",
       "      <td>38.42</td>\n",
       "      <td>1</td>\n",
       "      <td>163.42</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>79.90</td>\n",
       "      <td>15.31</td>\n",
       "      <td>1</td>\n",
       "      <td>95.21</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_items  total_price  total_freight_value  num_sellers  payment_value  \\\n",
       "0          2       243.90                33.71            2         277.61   \n",
       "1          1        86.90                13.63            1         100.53   \n",
       "2          1       159.90                14.14            1         174.04   \n",
       "3          1        19.90                12.48            1          32.38   \n",
       "4          1       149.00                45.12            1         194.12   \n",
       "5          1        35.15                15.10            1          50.25   \n",
       "6          1        27.90                 8.27            1          36.17   \n",
       "7          1        15.30                11.85            1          27.15   \n",
       "8          1       125.00                38.42            1         163.42   \n",
       "9          1        79.90                15.31            1          95.21   \n",
       "\n",
       "   payment_installments  purchase_dow  purchase_hour  \n",
       "0                     8             3             21  \n",
       "1                     4             3             21  \n",
       "2                     4             3             21  \n",
       "3                     1             3             21  \n",
       "4                     2             3             21  \n",
       "5                     2             3             21  \n",
       "6                     1             3             21  \n",
       "7                     1             3             21  \n",
       "8                     2             3             22  \n",
       "9                     1             3             22  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# M5-3.2a Verify production inference dataset (CSV)\n",
    "\n",
    "import awswrangler as wr\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Production inference CSV:\", prod_infer_uri)\n",
    "\n",
    "prod_df = wr.s3.read_csv(prod_infer_uri, header=None)\n",
    "prod_df.columns = FEATURES  # we wrote it without headers\n",
    "\n",
    "print(\"✅ prod_infer rows/cols:\", prod_df.shape)\n",
    "print(\"Columns:\", list(prod_df.columns))\n",
    "prod_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fab24b6-c761-4ad9-9cb8-bbced57c933a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Capture objects found: 2\n",
      "monitoring/batch-transform/capture/1770585686/input/2026/02/08/21/f1f0c63f-a181-492b-8765-b29a732c06ca.json\n",
      "monitoring/batch-transform/capture/1770585686/output/2026/02/08/21/a9305a65-6fbb-47aa-b3b7-3a2cc419e728.json\n",
      "\n",
      "First capture object: monitoring/batch-transform/capture/1770585686/input/2026/02/08/21/f1f0c63f-a181-492b-8765-b29a732c06ca.json\n",
      "First line (raw): [{\"prefix\":\"s3://aai540-olist-mlops-chris-7f3k2p/modeling/xgb_v1/prod/prod_infer.csv\"},\"\"] \n",
      "\n",
      "⚠️ Could not parse first line as JSON: 'list' object has no attribute 'keys'\n"
     ]
    }
   ],
   "source": [
    "# M5-3.2b Verify batch capture artifacts (JSONL)\n",
    "import boto3, json\n",
    "\n",
    "s3 = boto3.client(\"s3\", region_name=region)\n",
    "\n",
    "capture_uri = \"s3://aai540-olist-mlops-chris-7f3k2p/monitoring/batch-transform/capture/1770585686/\"\n",
    "prefix = capture_uri.replace(f\"s3://{bucket}/\", \"\")\n",
    "\n",
    "resp = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "keys = [o[\"Key\"] for o in resp.get(\"Contents\", [])]\n",
    "\n",
    "print(\"✅ Capture objects found:\", len(keys))\n",
    "for k in keys[:25]:\n",
    "    print(k)\n",
    "\n",
    "# Read the first non-empty object and show 1 parsed JSON record\n",
    "if not keys:\n",
    "    raise RuntimeError(\"No capture files found under: \" + capture_uri)\n",
    "\n",
    "k0 = sorted(keys)[0]\n",
    "obj = s3.get_object(Bucket=bucket, Key=k0)\n",
    "text = obj[\"Body\"].read().decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "first_line = next((ln for ln in text.splitlines() if ln.strip()), None)\n",
    "print(\"\\nFirst capture object:\", k0)\n",
    "print(\"First line (raw):\", first_line[:300], \"...\" if len(first_line) > 300 else \"\")\n",
    "\n",
    "# Try parsing as JSON\n",
    "try:\n",
    "    rec = json.loads(first_line)\n",
    "    print(\"\\n✅ Parsed JSON keys:\", list(rec.keys())[:25])\n",
    "    # Show a couple common locations\n",
    "    if \"inferenceId\" in rec:\n",
    "        print(\"inferenceId:\", rec[\"inferenceId\"])\n",
    "    if \"eventMetadata\" in rec:\n",
    "        print(\"eventMetadata keys:\", list(rec[\"eventMetadata\"].keys()))\n",
    "except Exception as e:\n",
    "    print(\"\\n⚠️ Could not parse first line as JSON:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3262ee76-cf7f-46af-9e1b-fb9f45037967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys:\n",
      " - monitoring/batch-transform/capture/1770585686/input/2026/02/08/21/f1f0c63f-a181-492b-8765-b29a732c06ca.json\n",
      " - monitoring/batch-transform/capture/1770585686/output/2026/02/08/21/a9305a65-6fbb-47aa-b3b7-3a2cc419e728.json\n",
      "\n",
      "--- INPUT CAPTURE ---\n",
      "Raw line: [{\"prefix\":\"s3://aai540-olist-mlops-chris-7f3k2p/modeling/xgb_v1/prod/prod_infer.csv\"},\"\"] \n",
      "Type: <class 'list'>\n",
      "Parsed: [{'prefix': 's3://aai540-olist-mlops-chris-7f3k2p/modeling/xgb_v1/prod/prod_infer.csv'}, '']\n",
      "\n",
      "--- OUTPUT CAPTURE ---\n",
      "Raw line: [{\"prefix\":\"s3://aai540-olist-mlops-chris-7f3k2p/monitoring/batch-transform/output/1770585686/\"},\"prod_infer.csv.out\"] \n",
      "Type: <class 'list'>\n",
      "Parsed: [{'prefix': 's3://aai540-olist-mlops-chris-7f3k2p/monitoring/batch-transform/output/1770585686/'}, 'prod_infer.csv.out']\n"
     ]
    }
   ],
   "source": [
    "# M5-3.2c Parse capture JSON properly (input + output)\n",
    "import boto3, json\n",
    "\n",
    "s3 = boto3.client(\"s3\", region_name=region)\n",
    "\n",
    "capture_prefix = \"monitoring/batch-transform/capture/1770585686/\"\n",
    "resp = s3.list_objects_v2(Bucket=bucket, Prefix=capture_prefix)\n",
    "keys = sorted([o[\"Key\"] for o in resp.get(\"Contents\", [])])\n",
    "\n",
    "print(\"Keys:\")\n",
    "for k in keys:\n",
    "    print(\" -\", k)\n",
    "\n",
    "def read_first_json_record(key):\n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    text = obj[\"Body\"].read().decode(\"utf-8\", errors=\"replace\")\n",
    "    first_line = next((ln for ln in text.splitlines() if ln.strip()), None)\n",
    "    data = json.loads(first_line)\n",
    "    return first_line, data\n",
    "\n",
    "# Read & show input\n",
    "input_key = [k for k in keys if \"/input/\" in k][0]\n",
    "raw_in, data_in = read_first_json_record(input_key)\n",
    "\n",
    "print(\"\\n--- INPUT CAPTURE ---\")\n",
    "print(\"Raw line:\", raw_in[:300], \"...\" if len(raw_in) > 300 else \"\")\n",
    "print(\"Type:\", type(data_in))\n",
    "print(\"Parsed:\", data_in)\n",
    "\n",
    "# Read & show output\n",
    "output_key = [k for k in keys if \"/output/\" in k][0]\n",
    "raw_out, data_out = read_first_json_record(output_key)\n",
    "\n",
    "print(\"\\n--- OUTPUT CAPTURE ---\")\n",
    "print(\"Raw line:\", raw_out[:300], \"...\" if len(raw_out) > 300 else \"\")\n",
    "print(\"Type:\", type(data_out))\n",
    "print(\"Parsed:\", data_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4be1c514-7437-4b94-a99a-0936ef060eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TrainingJobs': ['sagemaker-xgboost-2026-02-07-17-53-40-249',\n",
       "  'sagemaker-xgboost-2026-02-07-16-46-56-471',\n",
       "  'sagemaker-xgboost-2026-02-07-16-42-34-291',\n",
       "  'sagemaker-xgboost-2026-01-31-18-00-53-460',\n",
       "  'a4-1-xgb-1769829697'],\n",
       " 'TransformJobs': ['sagemaker-xgboost-2026-02-08-21-21-26-827',\n",
       "  'sagemaker-xgboost-2026-01-31-19-39-44-779',\n",
       "  'sagemaker-xgboost-2026-01-31-19-29-14-989',\n",
       "  'a4-1-xgb-batch-1769830620'],\n",
       " 'ProcessingJobs': ['baseline-suggestion-job-2026-02-08-21-32-41-870',\n",
       "  'clarify-bias-1770490533',\n",
       "  'clarify-bias-1770489976',\n",
       "  'Clarify-Bias-2026-02-07-18-30-09-560',\n",
       "  'Clarify-Bias-2026-02-07-18-07-08-683']}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# M5-4.1 Collect SageMaker job names for infrastructure monitoring\n",
    "\n",
    "import boto3\n",
    "\n",
    "sm = boto3.client(\"sagemaker\", region_name=region)\n",
    "\n",
    "jobs = {\n",
    "    \"TrainingJobs\": [],\n",
    "    \"TransformJobs\": [],\n",
    "    \"ProcessingJobs\": []\n",
    "}\n",
    "\n",
    "# Training jobs\n",
    "for j in sm.list_training_jobs(MaxResults=5)[\"TrainingJobSummaries\"]:\n",
    "    jobs[\"TrainingJobs\"].append(j[\"TrainingJobName\"])\n",
    "\n",
    "# Transform jobs\n",
    "for j in sm.list_transform_jobs(MaxResults=5)[\"TransformJobSummaries\"]:\n",
    "    jobs[\"TransformJobs\"].append(j[\"TransformJobName\"])\n",
    "\n",
    "# Processing jobs\n",
    "for j in sm.list_processing_jobs(MaxResults=5)[\"ProcessingJobSummaries\"]:\n",
    "    jobs[\"ProcessingJobs\"].append(j[\"ProcessingJobName\"])\n",
    "\n",
    "jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2f38b9e-ca2e-4951-a8d5-1f31cb3eeb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CloudWatch dashboard created: AAI540-Olist-MLops-Dashboard\n"
     ]
    }
   ],
   "source": [
    "# M5-4.3 Create CloudWatch Dashboard for ML Infrastructure\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "cw = boto3.client(\"cloudwatch\", region_name=region)\n",
    "\n",
    "dashboard_name = \"AAI540-Olist-MLops-Dashboard\"\n",
    "\n",
    "dashboard_body = {\n",
    "    \"widgets\": [\n",
    "        {\n",
    "            \"type\": \"metric\",\n",
    "            \"x\": 0,\n",
    "            \"y\": 0,\n",
    "            \"width\": 12,\n",
    "            \"height\": 6,\n",
    "            \"properties\": {\n",
    "                \"title\": \"Training Job Duration\",\n",
    "                \"metrics\": [\n",
    "                    [\"AWS/SageMaker\", \"TrainingJobDuration\",\n",
    "                     \"TrainingJobName\", \"sagemaker-xgboost-2026-01-31-18-00-53-460\"]\n",
    "                ],\n",
    "                \"stat\": \"Average\",\n",
    "                \"period\": 300,\n",
    "                \"region\": region\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"metric\",\n",
    "            \"x\": 12,\n",
    "            \"y\": 0,\n",
    "            \"width\": 12,\n",
    "            \"height\": 6,\n",
    "            \"properties\": {\n",
    "                \"title\": \"Batch Transform Duration\",\n",
    "                \"metrics\": [\n",
    "                    [\"AWS/SageMaker\", \"TransformJobDuration\",\n",
    "                     \"TransformJobName\", \"sagemaker-xgboost-2026-02-08-21-21-26-827\"]\n",
    "                ],\n",
    "                \"stat\": \"Average\",\n",
    "                \"period\": 300,\n",
    "                \"region\": region\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"metric\",\n",
    "            \"x\": 0,\n",
    "            \"y\": 6,\n",
    "            \"width\": 12,\n",
    "            \"height\": 6,\n",
    "            \"properties\": {\n",
    "                \"title\": \"Processing Job Duration\",\n",
    "                \"metrics\": [\n",
    "                    [\"AWS/SageMaker\", \"ProcessingJobDuration\",\n",
    "                     \"ProcessingJobName\", \"baseline-suggestion-job-2026-02-08-21-32-41-870\"]\n",
    "                ],\n",
    "                \"stat\": \"Average\",\n",
    "                \"period\": 300,\n",
    "                \"region\": region\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"metric\",\n",
    "            \"x\": 12,\n",
    "            \"y\": 6,\n",
    "            \"width\": 12,\n",
    "            \"height\": 6,\n",
    "            \"properties\": {\n",
    "                \"title\": \"Job Failures (All SageMaker Jobs)\",\n",
    "                \"metrics\": [\n",
    "                    [\"AWS/SageMaker\", \"TrainingJobsFailed\"],\n",
    "                    [\"AWS/SageMaker\", \"TransformJobsFailed\"],\n",
    "                    [\"AWS/SageMaker\", \"ProcessingJobsFailed\"]\n",
    "                ],\n",
    "                \"stat\": \"Sum\",\n",
    "                \"period\": 300,\n",
    "                \"region\": region\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "cw.put_dashboard(\n",
    "    DashboardName=dashboard_name,\n",
    "    DashboardBody=json.dumps(dashboard_body)\n",
    ")\n",
    "\n",
    "print(\"✅ CloudWatch dashboard created:\", dashboard_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d02ad7f8-c411-4c51-89e3-8bb16a987654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ s3://aai540-olist-mlops-chris-7f3k2p/monitoring/baselines/data_quality/results/statistics.json -> objects: 1\n",
      "✅ s3://aai540-olist-mlops-chris-7f3k2p/monitoring/baselines/data_quality/results/constraints.json -> objects: 1\n",
      "✅ s3://aai540-olist-mlops-chris-7f3k2p/monitoring/batch-transform/capture/ -> objects: 2\n"
     ]
    }
   ],
   "source": [
    "# M5-5.1 Verify monitoring report artifacts exist\n",
    "\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.client(\"s3\", region_name=region)\n",
    "\n",
    "paths = [\n",
    "    baseline_statistics_uri,\n",
    "    baseline_constraints_uri,\n",
    "    \"s3://aai540-olist-mlops-chris-7f3k2p/monitoring/batch-transform/capture/\"\n",
    "]\n",
    "\n",
    "for p in paths:\n",
    "    bucket_name, key = p.replace(\"s3://\", \"\").split(\"/\", 1)\n",
    "    resp = s3.list_objects_v2(Bucket=bucket_name, Prefix=key)\n",
    "    print(f\"✅ {p} -> objects:\", resp.get(\"KeyCount\", 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9ac68c-609e-474d-9192-6464992802b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
