{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69584688-1464-4b46-be61-06a594249ce9",
   "metadata": {},
   "source": [
    "# MLOps Pipeline Workflow & Team Notes\n",
    "\n",
    "## Project Overview\n",
    "This notebook implements an end-to-end MLOps data pipeline using the **Olist Brazilian E-Commerce dataset**. The goal was to demonstrate a production-style workflow that covers data ingestion, cataloging, exploratory analysis, feature engineering, feature storage, and dataset splitting — all using AWS services in a cost-efficient way.\n",
    "\n",
    "The pipeline was intentionally built step-by-step to mirror MLOps practices rather than a one-off modeling notebook.\n",
    "\n",
    "---\n",
    "\n",
    "## High-Level Workflow\n",
    "\n",
    "### 1. Raw Data Ingestion (S3 Data Lake)\n",
    "- Created an S3 bucket to act as a data lake.\n",
    "- Uploaded all **9 raw CSV files** from the Kaggle Olist dataset.\n",
    "- Organized raw data under: s3:///raw/olist/ingest_date=YYYY-MM-DD/\n",
    "- Each dataset was placed into its own subfolder to support Athena’s directory-based table requirements.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Data Cataloging & Querying (Athena)\n",
    "- Created an Athena database (`olist_datalake`).\n",
    "- Defined **external tables** for each dataset directly from JupyterLab (no Glue crawler required).\n",
    "- Verified schemas and row counts using Athena queries.\n",
    "- This step enabled SQL-based access to the data and served as the cataloging layer for downstream analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Exploratory Data Analysis (SageMaker + Pandas)\n",
    "- Loaded Athena tables into Pandas using `awswrangler`.\n",
    "- Performed sanity checks on row counts and joins.\n",
    "- Built an **order-level analytical view** by aggregating:\n",
    "- order items\n",
    "- payments\n",
    "- customer attributes\n",
    "- Engineered a target variable (`is_late`) based on delivery vs. estimated delivery dates.\n",
    "- Observed class imbalance (~8% late deliveries), motivating careful splitting and evaluation later.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Feature Engineering\n",
    "- Created leakage-safe, order-level features using only information available at purchase time:\n",
    "- pricing, freight, number of items/sellers\n",
    "- payment information\n",
    "- time-based features (day of week, hour of day)\n",
    "- customer state\n",
    "- Maintained a **canonical feature dataset** for analysis and splitting.\n",
    "- Created a **Feature Store–compatible version** with strict data types.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. SageMaker Feature Store (Offline Store)\n",
    "- Created a **SageMaker Feature Group** (offline store only to control cost).\n",
    "- Used `order_id` as the record identifier.\n",
    "- Used a strictly formatted ISO-8601 `event_time` with UTC (`Z`) as the event time feature.\n",
    "- Successfully ingested ~99k feature records into Feature Store.\n",
    "- Offline store data is persisted in S3 for training and future reuse.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Dataset Splitting (Time-Based)\n",
    "- Performed a **time-based split** using `event_time` to avoid temporal leakage:\n",
    "- Train: ~40%\n",
    "- Validation: ~10%\n",
    "- Test: ~10%\n",
    "- Production reserve: ~40%\n",
    "- Persisted each split as Parquet files to: s3:///splits/olist/features/version=v1/\n",
    "- This mirrors a real production setup where recent data is reserved for inference.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Engineering Decisions & Lessons Learned\n",
    "\n",
    "- **Athena LOCATION must point to directories**, not individual files.\n",
    "- **Feature Store requires strict ISO-8601 timestamps with timezone** — missing the `Z` suffix causes ingestion failures.\n",
    "- Maintaining separate:\n",
    "- canonical feature data (analysis-friendly)\n",
    "- Feature Store–safe data (schema-restricted)\n",
    "is a best practice in real MLOps systems.\n",
    "- Time-based splitting is critical to avoid data leakage in temporal datasets.\n",
    "- Offline Feature Store provides the required functionality while minimizing cost.\n",
    "\n",
    "---\n",
    "\n",
    "## Cost Management Notes\n",
    "- SageMaker compute was stopped immediately after completion.\n",
    "- Feature Store **online store was intentionally disabled** to avoid ongoing charges.\n",
    "- S3 storage costs are minimal and safe to keep until final submission.\n",
    "- Cleanup (Feature Group deletion, S3 cleanup) should only be done **after submission**.\n",
    "\n",
    "---\n",
    "\n",
    "## For Teammates\n",
    "If you need to re-run or extend this work:\n",
    "1. Start at the Athena read step (no need to re-upload raw data).\n",
    "2. Do **not** re-run ingestion unless changing the feature schema.\n",
    "3. Always stop SageMaker compute when finished.\n",
    "\n",
    "This notebook represents a complete, MLOps data pipeline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a684ab4-5185-4603-9f30-358b125dcba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found files: 10\n",
      "raw/olist/ingest_date=2026-01-25/\n",
      "raw/olist/ingest_date=2026-01-25/olist_customers_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_geolocation_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_order_items_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_order_payments_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_order_reviews_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_orders_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_products_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_sellers_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/product_category_name_translation.csv\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "bucket = \"aai540-olist-mlops-chris-7f3k2p\"\n",
    "prefix = \"raw/olist/ingest_date=2026-01-25/\"\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "resp = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "\n",
    "keys = [obj[\"Key\"] for obj in resp.get(\"Contents\", [])]\n",
    "print(\"Found files:\", len(keys))\n",
    "for k in keys:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9273a5f2-6a04-48e3-91fc-89dd28b3cd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Athena results prefix exists: s3://aai540-olist-mlops-chris-7f3k2p/athena-results/\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "bucket = \"aai540-olist-mlops-chris-7f3k2p\"\n",
    "results_prefix = \"athena-results/\"\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "resp = s3.list_objects_v2(Bucket=bucket, Prefix=results_prefix, MaxKeys=1)\n",
    "\n",
    "if \"Contents\" in resp:\n",
    "    print(\"✅ Athena results prefix exists:\", f\"s3://{bucket}/{results_prefix}\")\n",
    "else:\n",
    "    # create a zero-byte object so the prefix exists\n",
    "    s3.put_object(Bucket=bucket, Key=results_prefix)\n",
    "    print(\"✅ Created Athena results prefix:\", f\"s3://{bucket}/{results_prefix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ca03c11-54ff-441e-bf5e-fd9c05665d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Database ready: olist_datalake\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import boto3\n",
    "\n",
    "bucket = \"aai540-olist-mlops-chris-7f3k2p\"\n",
    "REGION = boto3.session.Session().region_name\n",
    "athena = boto3.client(\"athena\", region_name=REGION)\n",
    "\n",
    "ATHENA_OUTPUT = f\"s3://{bucket}/athena-results/\"\n",
    "DB = \"olist_datalake\"\n",
    "\n",
    "def run_athena(sql: str, database: str = \"default\"):\n",
    "    res = athena.start_query_execution(\n",
    "        QueryString=sql,\n",
    "        QueryExecutionContext={\"Database\": database},\n",
    "        ResultConfiguration={\"OutputLocation\": ATHENA_OUTPUT},\n",
    "    )\n",
    "    qid = res[\"QueryExecutionId\"]\n",
    "    while True:\n",
    "        q = athena.get_query_execution(QueryExecutionId=qid)\n",
    "        state = q[\"QueryExecution\"][\"Status\"][\"State\"]\n",
    "        if state in (\"SUCCEEDED\", \"FAILED\", \"CANCELLED\"):\n",
    "            break\n",
    "        time.sleep(1)\n",
    "    if state != \"SUCCEEDED\":\n",
    "        reason = q[\"QueryExecution\"][\"Status\"].get(\"StateChangeReason\", \"Unknown\")\n",
    "        raise RuntimeError(f\"Athena query failed: {state} - {reason}\\nSQL:\\n{sql}\")\n",
    "    return qid\n",
    "\n",
    "run_athena(f\"CREATE DATABASE IF NOT EXISTS {DB};\", database=\"default\")\n",
    "print(\"✅ Database ready:\", DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e977a0a3-98b4-4b4a-90b6-d6b93a85f392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Copied to: raw/olist/ingest_date=2026-01-25/olist_customers_dataset/olist_customers_dataset.csv\n",
      "✅ Copied to: raw/olist/ingest_date=2026-01-25/olist_geolocation_dataset/olist_geolocation_dataset.csv\n",
      "✅ Copied to: raw/olist/ingest_date=2026-01-25/olist_order_items_dataset/olist_order_items_dataset.csv\n",
      "✅ Copied to: raw/olist/ingest_date=2026-01-25/olist_order_payments_dataset/olist_order_payments_dataset.csv\n",
      "✅ Copied to: raw/olist/ingest_date=2026-01-25/olist_order_reviews_dataset/olist_order_reviews_dataset.csv\n",
      "✅ Copied to: raw/olist/ingest_date=2026-01-25/olist_orders_dataset/olist_orders_dataset.csv\n",
      "✅ Copied to: raw/olist/ingest_date=2026-01-25/olist_products_dataset/olist_products_dataset.csv\n",
      "✅ Copied to: raw/olist/ingest_date=2026-01-25/olist_sellers_dataset/olist_sellers_dataset.csv\n",
      "✅ Copied to: raw/olist/ingest_date=2026-01-25/product_category_name_translation/product_category_name_translation.csv\n",
      "\n",
      "Done. Next we’ll point Athena tables at these folders.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "bucket = \"aai540-olist-mlops-chris-7f3k2p\"\n",
    "base_prefix = \"raw/olist/ingest_date=2026-01-25/\"\n",
    "\n",
    "files = [\n",
    "    \"olist_customers_dataset.csv\",\n",
    "    \"olist_geolocation_dataset.csv\",\n",
    "    \"olist_order_items_dataset.csv\",\n",
    "    \"olist_order_payments_dataset.csv\",\n",
    "    \"olist_order_reviews_dataset.csv\",\n",
    "    \"olist_orders_dataset.csv\",\n",
    "    \"olist_products_dataset.csv\",\n",
    "    \"olist_sellers_dataset.csv\",\n",
    "    \"product_category_name_translation.csv\",\n",
    "]\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "for f in files:\n",
    "    src_key = base_prefix + f\n",
    "    folder = f.replace(\".csv\", \"\")  # folder name = file name without .csv\n",
    "    dst_key = f\"{base_prefix}{folder}/{f}\"\n",
    "    \n",
    "    # copy\n",
    "    s3.copy_object(\n",
    "        Bucket=bucket,\n",
    "        CopySource={\"Bucket\": bucket, \"Key\": src_key},\n",
    "        Key=dst_key\n",
    "    )\n",
    "    print(\"✅ Copied to:\", dst_key)\n",
    "\n",
    "print(\"\\nDone. Next we’ll point Athena tables at these folders.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "858ee701-16e0-4030-bd4c-c966fe47a717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw/olist/ingest_date=2026-01-25/\n",
      "raw/olist/ingest_date=2026-01-25/olist_customers_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_customers_dataset/olist_customers_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_geolocation_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_geolocation_dataset/olist_geolocation_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_order_items_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_order_items_dataset/olist_order_items_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_order_payments_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_order_payments_dataset/olist_order_payments_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_order_reviews_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_order_reviews_dataset/olist_order_reviews_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_orders_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_orders_dataset/olist_orders_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_products_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_products_dataset/olist_products_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_sellers_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/olist_sellers_dataset/olist_sellers_dataset.csv\n",
      "raw/olist/ingest_date=2026-01-25/product_category_name_translation.csv\n",
      "raw/olist/ingest_date=2026-01-25/product_category_name_translation/product_category_name_translation.csv\n"
     ]
    }
   ],
   "source": [
    "resp = s3.list_objects_v2(Bucket=bucket, Prefix=base_prefix, MaxKeys=50)\n",
    "for obj in resp.get(\"Contents\", []):\n",
    "    print(obj[\"Key\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c29e34fd-478d-486e-ab8e-bf2105b59b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created: olist_datalake.olist_customers_dataset\n",
      "✅ Created: olist_datalake.olist_geolocation_dataset\n",
      "✅ Created: olist_datalake.olist_order_items_dataset\n",
      "✅ Created: olist_datalake.olist_order_payments_dataset\n",
      "✅ Created: olist_datalake.olist_order_reviews_dataset\n",
      "✅ Created: olist_datalake.olist_orders_dataset\n",
      "✅ Created: olist_datalake.olist_products_dataset\n",
      "✅ Created: olist_datalake.olist_sellers_dataset\n",
      "✅ Created: olist_datalake.product_category_name_translation\n"
     ]
    }
   ],
   "source": [
    "RAW_BASE = f\"s3://{bucket}/{base_prefix}\"\n",
    "\n",
    "def create_csv_table(table_name: str, columns_ddl: str, folder_name: str):\n",
    "    sql = f\"\"\"\n",
    "    CREATE EXTERNAL TABLE IF NOT EXISTS {DB}.{table_name} (\n",
    "      {columns_ddl}\n",
    "    )\n",
    "    ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
    "    WITH SERDEPROPERTIES (\n",
    "      'separatorChar' = ',',\n",
    "      'quoteChar'     = '\\\"',\n",
    "      'escapeChar'    = '\\\\\\\\'\n",
    "    )\n",
    "    STORED AS TEXTFILE\n",
    "    LOCATION '{RAW_BASE}{folder_name}/'\n",
    "    TBLPROPERTIES ('skip.header.line.count'='1');\n",
    "    \"\"\"\n",
    "    run_athena(sql, database=DB)\n",
    "    print(f\"✅ Created: {DB}.{table_name}\")\n",
    "\n",
    "create_csv_table(\n",
    "    \"olist_customers_dataset\",\n",
    "    \"\"\"\n",
    "    customer_id string,\n",
    "    customer_unique_id string,\n",
    "    customer_zip_code_prefix int,\n",
    "    customer_city string,\n",
    "    customer_state string\n",
    "    \"\"\",\n",
    "    \"olist_customers_dataset\"\n",
    ")\n",
    "\n",
    "create_csv_table(\n",
    "    \"olist_geolocation_dataset\",\n",
    "    \"\"\"\n",
    "    geolocation_zip_code_prefix int,\n",
    "    geolocation_lat double,\n",
    "    geolocation_lng double,\n",
    "    geolocation_city string,\n",
    "    geolocation_state string\n",
    "    \"\"\",\n",
    "    \"olist_geolocation_dataset\"\n",
    ")\n",
    "\n",
    "create_csv_table(\n",
    "    \"olist_order_items_dataset\",\n",
    "    \"\"\"\n",
    "    order_id string,\n",
    "    order_item_id int,\n",
    "    product_id string,\n",
    "    seller_id string,\n",
    "    shipping_limit_date string,\n",
    "    price double,\n",
    "    freight_value double\n",
    "    \"\"\",\n",
    "    \"olist_order_items_dataset\"\n",
    ")\n",
    "\n",
    "create_csv_table(\n",
    "    \"olist_order_payments_dataset\",\n",
    "    \"\"\"\n",
    "    order_id string,\n",
    "    payment_sequential int,\n",
    "    payment_type string,\n",
    "    payment_installments int,\n",
    "    payment_value double\n",
    "    \"\"\",\n",
    "    \"olist_order_payments_dataset\"\n",
    ")\n",
    "\n",
    "create_csv_table(\n",
    "    \"olist_order_reviews_dataset\",\n",
    "    \"\"\"\n",
    "    review_id string,\n",
    "    order_id string,\n",
    "    review_score int,\n",
    "    review_comment_title string,\n",
    "    review_comment_message string,\n",
    "    review_creation_date string,\n",
    "    review_answer_timestamp string\n",
    "    \"\"\",\n",
    "    \"olist_order_reviews_dataset\"\n",
    ")\n",
    "\n",
    "create_csv_table(\n",
    "    \"olist_orders_dataset\",\n",
    "    \"\"\"\n",
    "    order_id string,\n",
    "    customer_id string,\n",
    "    order_status string,\n",
    "    order_purchase_timestamp string,\n",
    "    order_approved_at string,\n",
    "    order_delivered_carrier_date string,\n",
    "    order_delivered_customer_date string,\n",
    "    order_estimated_delivery_date string\n",
    "    \"\"\",\n",
    "    \"olist_orders_dataset\"\n",
    ")\n",
    "\n",
    "create_csv_table(\n",
    "    \"olist_products_dataset\",\n",
    "    \"\"\"\n",
    "    product_id string,\n",
    "    product_category_name string,\n",
    "    product_name_lenght int,\n",
    "    product_description_lenght int,\n",
    "    product_photos_qty int,\n",
    "    product_weight_g int,\n",
    "    product_length_cm int,\n",
    "    product_height_cm int,\n",
    "    product_width_cm int\n",
    "    \"\"\",\n",
    "    \"olist_products_dataset\"\n",
    ")\n",
    "\n",
    "create_csv_table(\n",
    "    \"olist_sellers_dataset\",\n",
    "    \"\"\"\n",
    "    seller_id string,\n",
    "    seller_zip_code_prefix int,\n",
    "    seller_city string,\n",
    "    seller_state string\n",
    "    \"\"\",\n",
    "    \"olist_sellers_dataset\"\n",
    ")\n",
    "\n",
    "create_csv_table(\n",
    "    \"product_category_name_translation\",\n",
    "    \"\"\"\n",
    "    product_category_name string,\n",
    "    product_category_name_english string\n",
    "    \"\"\",\n",
    "    \"product_category_name_translation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "393bfe94-4366-42c4-b48c-8a556f415750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SHOW TABLES succeeded\n",
      "✅ COUNT orders succeeded\n",
      "✅ GROUP BY order_status succeeded\n"
     ]
    }
   ],
   "source": [
    "run_athena(f\"SHOW TABLES IN {DB};\", database=DB)\n",
    "print(\"✅ SHOW TABLES succeeded\")\n",
    "\n",
    "run_athena(f\"SELECT COUNT(*) FROM {DB}.olist_orders_dataset;\", database=DB)\n",
    "print(\"✅ COUNT orders succeeded\")\n",
    "\n",
    "run_athena(f\"SELECT order_status, COUNT(*) c FROM {DB}.olist_orders_dataset GROUP BY 1 ORDER BY c DESC;\", database=DB)\n",
    "print(\"✅ GROUP BY order_status succeeded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d48e162-12df-4319-ba71-f3cf7f1412a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-25 19:16:46,584\tWARNING services.py:2070 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 1909432320 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=4.64gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2026-01-25 19:16:46,747\tINFO worker.py:1852 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orders: (99441, 8)\n",
      "order_items: (112650, 7)\n",
      "payments: (103886, 5)\n",
      "customers: (99441, 5)\n"
     ]
    }
   ],
   "source": [
    "#6.1\n",
    "import awswrangler as wr\n",
    "import pandas as pd\n",
    "\n",
    "DB = \"olist_datalake\"\n",
    "\n",
    "orders = wr.athena.read_sql_query(\n",
    "    sql=f\"SELECT * FROM {DB}.olist_orders_dataset\",\n",
    "    database=DB,\n",
    "    ctas_approach=False\n",
    ")\n",
    "\n",
    "order_items = wr.athena.read_sql_query(\n",
    "    sql=f\"SELECT * FROM {DB}.olist_order_items_dataset\",\n",
    "    database=DB,\n",
    "    ctas_approach=False\n",
    ")\n",
    "\n",
    "payments = wr.athena.read_sql_query(\n",
    "    sql=f\"SELECT * FROM {DB}.olist_order_payments_dataset\",\n",
    "    database=DB,\n",
    "    ctas_approach=False\n",
    ")\n",
    "\n",
    "customers = wr.athena.read_sql_query(\n",
    "    sql=f\"SELECT * FROM {DB}.olist_customers_dataset\",\n",
    "    database=DB,\n",
    "    ctas_approach=False\n",
    ")\n",
    "\n",
    "print(\"orders:\", orders.shape)\n",
    "print(\"order_items:\", order_items.shape)\n",
    "print(\"payments:\", payments.shape)\n",
    "print(\"customers:\", customers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cfa6170-a17c-4f5c-8cf7-8770b8b46166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orders rows: 99441\n",
      "EDA rows: 99441\n",
      "Row loss: 0\n",
      "Late rate:\n",
      " is_late\n",
      "0    0.92129\n",
      "1    0.07871\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#6.2\n",
    "# Parse timestamps\n",
    "timestamp_cols = [\n",
    "    \"order_purchase_timestamp\",\n",
    "    \"order_approved_at\",\n",
    "    \"order_delivered_carrier_date\",\n",
    "    \"order_delivered_customer_date\",\n",
    "    \"order_estimated_delivery_date\",\n",
    "]\n",
    "for col in timestamp_cols:\n",
    "    orders[col] = pd.to_datetime(orders[col], errors=\"coerce\")\n",
    "\n",
    "# Aggregations\n",
    "items_agg = (\n",
    "    order_items.groupby(\"order_id\")\n",
    "    .agg(\n",
    "        num_items=(\"order_item_id\", \"count\"),\n",
    "        total_price=(\"price\", \"sum\"),\n",
    "        total_freight_value=(\"freight_value\", \"sum\"),\n",
    "        num_sellers=(\"seller_id\", \"nunique\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "payments_agg = (\n",
    "    payments.groupby(\"order_id\")\n",
    "    .agg(\n",
    "        payment_value=(\"payment_value\", \"sum\"),\n",
    "        payment_installments=(\"payment_installments\", \"max\"),\n",
    "        payment_type=(\"payment_type\", lambda x: x.value_counts().index[0]),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "eda_df = (\n",
    "    orders\n",
    "    .merge(items_agg, on=\"order_id\", how=\"left\")\n",
    "    .merge(payments_agg, on=\"order_id\", how=\"left\")\n",
    "    .merge(customers[[\"customer_id\", \"customer_state\"]], on=\"customer_id\", how=\"left\")\n",
    ")\n",
    "\n",
    "# Time features\n",
    "eda_df[\"purchase_dow\"] = eda_df[\"order_purchase_timestamp\"].dt.dayofweek\n",
    "eda_df[\"purchase_hour\"] = eda_df[\"order_purchase_timestamp\"].dt.hour\n",
    "\n",
    "# Label: late delivery\n",
    "eda_df[\"is_late\"] = (\n",
    "    (eda_df[\"order_delivered_customer_date\"].notna()) &\n",
    "    (eda_df[\"order_estimated_delivery_date\"].notna()) &\n",
    "    (eda_df[\"order_delivered_customer_date\"] > eda_df[\"order_estimated_delivery_date\"])\n",
    ").astype(int)\n",
    "\n",
    "print(\"Orders rows:\", len(orders))\n",
    "print(\"EDA rows:\", len(eda_df))\n",
    "print(\"Row loss:\", len(orders) - len(eda_df))\n",
    "print(\"Late rate:\\n\", eda_df[\"is_late\"].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6c44e50-a27a-4052-b192-82f344558f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ feat shape: (99441, 14)\n",
      "✅ feat_fs shape: (99441, 13)\n",
      "event_time sample: ['2017-10-02T10:56:33Z', '2018-07-24T20:41:37Z', '2018-08-08T08:38:49Z', '2017-11-18T19:28:06Z', '2018-02-13T21:18:39Z']\n"
     ]
    }
   ],
   "source": [
    "#7.0 + 7B\n",
    "# Canonical features (with purchase timestamp)\n",
    "feat = eda_df[[\n",
    "    \"order_id\",\n",
    "    \"order_purchase_timestamp\",\n",
    "    \"customer_state\",\n",
    "    \"num_items\",\n",
    "    \"total_price\",\n",
    "    \"total_freight_value\",\n",
    "    \"num_sellers\",\n",
    "    \"payment_value\",\n",
    "    \"payment_installments\",\n",
    "    \"payment_type\",\n",
    "    \"purchase_dow\",\n",
    "    \"purchase_hour\",\n",
    "    \"is_late\"\n",
    "]].copy()\n",
    "\n",
    "feat[\"customer_state\"] = feat[\"customer_state\"].fillna(\"unknown\").astype(str)\n",
    "feat[\"payment_type\"] = feat[\"payment_type\"].fillna(\"unknown\").astype(str)\n",
    "\n",
    "for c in [\"num_items\", \"num_sellers\", \"payment_installments\", \"purchase_dow\", \"purchase_hour\", \"is_late\"]:\n",
    "    feat[c] = feat[c].fillna(0).astype(int)\n",
    "\n",
    "for c in [\"total_price\", \"total_freight_value\", \"payment_value\"]:\n",
    "    feat[c] = feat[c].fillna(0.0).astype(float)\n",
    "\n",
    "# Create strict ISO-8601 event time WITH timezone \"Z\"\n",
    "feat[\"event_time\"] = (\n",
    "    pd.to_datetime(feat[\"order_purchase_timestamp\"], errors=\"coerce\")\n",
    "    .dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    ")\n",
    "\n",
    "feat = feat.dropna(subset=[\"order_id\", \"event_time\"]).reset_index(drop=True)\n",
    "\n",
    "# FeatureStore-safe version (remove datetime64 column)\n",
    "feat_fs = feat.drop(columns=[\"order_purchase_timestamp\"]).copy()\n",
    "feat_fs[\"event_time\"] = feat_fs[\"event_time\"].astype(str)\n",
    "\n",
    "print(\"✅ feat shape:\", feat.shape)\n",
    "print(\"✅ feat_fs shape:\", feat_fs.shape)\n",
    "print(\"event_time sample:\", feat_fs[\"event_time\"].head().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f15a681-f32e-4103-be35-fab9b0646be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "Region: us-east-1\n",
      "Role: arn:aws:iam::758289042916:role/LabRole\n",
      "Offline store URI: s3://aai540-olist-mlops-chris-7f3k2p/feature-store/olist-order-features-v1/\n"
     ]
    }
   ],
   "source": [
    "#7.1\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "\n",
    "bucket = \"aai540-olist-mlops-chris-7f3k2p\"\n",
    "region = boto3.session.Session().region_name\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "feature_group_name = \"olist-order-features-v1\"\n",
    "offline_store_s3_uri = f\"s3://{bucket}/feature-store/{feature_group_name}/\"\n",
    "\n",
    "fg = FeatureGroup(name=feature_group_name, sagemaker_session=sess)\n",
    "\n",
    "print(\"Region:\", region)\n",
    "print(\"Role:\", role)\n",
    "print(\"Offline store URI:\", offline_store_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b652859d-e72d-4a6b-8af1-72af458ed97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Feature Group already exists: olist-order-features-v1\n",
      "Status=Created, OfflineStoreStatus=UNKNOWN\n",
      "✅ Feature Group ready\n"
     ]
    }
   ],
   "source": [
    "#7.2\n",
    "import time\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "sm = boto3.client(\"sagemaker\", region_name=region)\n",
    "\n",
    "def feature_group_exists(name: str) -> bool:\n",
    "    try:\n",
    "        sm.describe_feature_group(FeatureGroupName=name)\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        if \"ResourceNotFound\" in str(e):\n",
    "            return False\n",
    "        raise\n",
    "\n",
    "def wait_for_fg_created(name: str, timeout_sec: int = 600, poll_sec: int = 10):\n",
    "    start = time.time()\n",
    "    while True:\n",
    "        desc = sm.describe_feature_group(FeatureGroupName=name)\n",
    "        status = desc.get(\"FeatureGroupStatus\")\n",
    "        offline_status = desc.get(\"OfflineStoreStatus\", {}).get(\"Status\", \"UNKNOWN\")\n",
    "        print(f\"Status={status}, OfflineStoreStatus={offline_status}\")\n",
    "        if status == \"Created\" and offline_status in (\"Active\", \"UNKNOWN\"):\n",
    "            return desc\n",
    "        if status in (\"CreateFailed\", \"DeleteFailed\"):\n",
    "            raise RuntimeError(f\"Feature Group failed with status={status}. Details: {desc}\")\n",
    "        if time.time() - start > timeout_sec:\n",
    "            raise TimeoutError(f\"Timed out waiting for Feature Group to be Created: {name}\")\n",
    "        time.sleep(poll_sec)\n",
    "\n",
    "if feature_group_exists(feature_group_name):\n",
    "    print(f\"✅ Feature Group already exists: {feature_group_name}\")\n",
    "else:\n",
    "    fg.load_feature_definitions(data_frame=feat_fs)\n",
    "    fg.create(\n",
    "        s3_uri=offline_store_s3_uri,\n",
    "        record_identifier_name=\"order_id\",\n",
    "        event_time_feature_name=\"event_time\",\n",
    "        role_arn=role,\n",
    "        enable_online_store=False,\n",
    "    )\n",
    "    print(\"⏳ Create request submitted\")\n",
    "\n",
    "wait_for_fg_created(feature_group_name)\n",
    "print(\"✅ Feature Group ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2460fd33-5601-4e33-a636-da6a6ce54fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ingest complete\n"
     ]
    }
   ],
   "source": [
    "#7.3\n",
    "ingest_response = fg.ingest(data_frame=feat_fs, max_workers=2, wait=True)\n",
    "print(\"✅ Ingest complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1afae01-1c16-48a2-ab19-d0115ae85d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureGroupStatus: Created\n",
      "OfflineStoreStatus: Active\n",
      "S3 Offline Store Uri: s3://aai540-olist-mlops-chris-7f3k2p/feature-store/olist-order-features-v1/\n"
     ]
    }
   ],
   "source": [
    "#7.4\n",
    "import boto3\n",
    "\n",
    "sm = boto3.client(\"sagemaker\", region_name=region)\n",
    "desc = sm.describe_feature_group(FeatureGroupName=feature_group_name)\n",
    "\n",
    "print(\"FeatureGroupStatus:\", desc[\"FeatureGroupStatus\"])\n",
    "print(\"OfflineStoreStatus:\", desc[\"OfflineStoreStatus\"][\"Status\"])\n",
    "print(\"S3 Offline Store Uri:\", desc[\"OfflineStoreConfig\"][\"S3StorageConfig\"][\"S3Uri\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68adb030-ec45-4e9a-bd5a-abbbce5622ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Split sizes\n",
      "train: 39776\n",
      "val:   9944\n",
      "test:  9944\n",
      "prod:  39777\n",
      "✅ Wrote splits to: s3://aai540-olist-mlops-chris-7f3k2p/splits/olist/features/version=v1/\n"
     ]
    }
   ],
   "source": [
    "#8.0\n",
    "import awswrangler as wr\n",
    "\n",
    "bucket = \"aai540-olist-mlops-chris-7f3k2p\"\n",
    "\n",
    "# Use feat_fs for splits (Feature Store compatible)\n",
    "feat_sorted = feat_fs.sort_values(\"event_time\").reset_index(drop=True)\n",
    "n = len(feat_sorted)\n",
    "\n",
    "train_end = int(n * 0.40)\n",
    "val_end   = int(n * 0.50)\n",
    "test_end  = int(n * 0.60)\n",
    "\n",
    "train_df = feat_sorted.iloc[:train_end]\n",
    "val_df   = feat_sorted.iloc[train_end:val_end]\n",
    "test_df  = feat_sorted.iloc[val_end:test_end]\n",
    "prod_df  = feat_sorted.iloc[test_end:]\n",
    "\n",
    "print(\"✅ Split sizes\")\n",
    "print(\"train:\", len(train_df))\n",
    "print(\"val:  \", len(val_df))\n",
    "print(\"test: \", len(test_df))\n",
    "print(\"prod: \", len(prod_df))\n",
    "\n",
    "split_base = f\"s3://{bucket}/splits/olist/features/version=v1/\"\n",
    "wr.s3.to_parquet(train_df, f\"{split_base}train/\", dataset=True, mode=\"overwrite\")\n",
    "wr.s3.to_parquet(val_df,   f\"{split_base}val/\",   dataset=True, mode=\"overwrite\")\n",
    "wr.s3.to_parquet(test_df,  f\"{split_base}test/\",  dataset=True, mode=\"overwrite\")\n",
    "wr.s3.to_parquet(prod_df,  f\"{split_base}prod/\",  dataset=True, mode=\"overwrite\")\n",
    "\n",
    "print(\"✅ Wrote splits to:\", split_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8ab440e-a4ea-477e-bad5-fc372615b00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5822e971-6f0e-4ddb-acb7-41898a947a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objects found: 4\n",
      "splits/olist/features/version=v1/prod/a8cdd289b48b495ba34f072d8dfa9932.snappy.parquet\n",
      "splits/olist/features/version=v1/test/ba691db52493437ab3c663c8b98fe955.snappy.parquet\n",
      "splits/olist/features/version=v1/train/33879ef6e41a44e384d5255caa4ffa7f.snappy.parquet\n",
      "splits/olist/features/version=v1/val/6fb7a4263ec04acb8f57f8ad0050cfb0.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "prefix = \"splits/olist/features/version=v1/\"\n",
    "\n",
    "resp = s3.list_objects_v2(Bucket=bucket, Prefix=prefix, MaxKeys=50)\n",
    "print(\"Objects found:\", resp.get(\"KeyCount\", 0))\n",
    "for obj in resp.get(\"Contents\", [])[:20]:\n",
    "    print(obj[\"Key\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab04fb0-989f-4744-bbda-b33cb55b73ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
